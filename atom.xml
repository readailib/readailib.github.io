<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShenH.&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/543fc8a83d9b480e5f69c3842db96518</icon>
  <subtitle>Learn Anything, Anytime, Anywhere~</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://readailib.com/"/>
  <updated>2019-03-07T07:13:29.046Z</updated>
  <id>https://readailib.com/</id>
  
  <author>
    <name>ShenHengheng</name>
    <email>shenhengheng17g@ict.ac.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在树莓派上编译安装Go版本的Tensorflow</title>
    <link href="https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/"/>
    <id>https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/</id>
    <published>2019-03-06T16:14:16.000Z</published>
    <updated>2019-03-07T07:13:29.046Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-Used-Hardwares-and-Softwares"><a href="#0-Used-Hardwares-and-Softwares" class="headerlink" title="0. Used Hardwares and Softwares"></a>0. Used Hardwares and Softwares</h1><p>All steps were taken on my <strong>Raspberry Pi 3 B</strong> model with:</p><ul><li>Minimum GPU memory allocated (16MB)</li><li><strong>1GB</strong> of swap memory</li><li>External USB HDD (as root partition)</li></ul><a id="more"></a><p>and software versions were:</p><ul><li>Raspbian (Stretch) / gcc 6.3.0</li><li>Tensorflow 1.3.0</li><li>Protobuf 3.1.0</li><li>Bazel 0.5.1</li></ul><p>Before the beginning, I had to install dependencies:</p><h3 id="for-protobuf"><a href="#for-protobuf" class="headerlink" title="for protobuf"></a>for protobuf</h3><pre><code class="bash">$ sudo apt-get install autoconf automake libtool</code></pre><h3 id="for-bazel"><a href="#for-bazel" class="headerlink" title="for bazel"></a>for bazel</h3><pre><code class="bash">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip oracle-java8-jdk</code></pre><hr><h1 id="1-Install-Protobuf"><a href="#1-Install-Protobuf" class="headerlink" title="1. Install Protobuf"></a>1. Install Protobuf</h1><p>I cloned the protobuf’s repository:</p><pre><code class="bash">$ git clone https://github.com/google/protobuf.git</code></pre><p>and started building:</p><pre><code class="bash">$ cd protobuf$ git checkout v3.1.0$ ./autogen.sh$ ./configure$ make -j 4$ sudo make install$ sudo ldconfig</code></pre><p>It took less than an hour to finish.</p><p>I could see the version of installed protobuf with:</p><pre><code class="bash">$ protoc --versionlibprotoc 3.1.0</code></pre><h1 id="2-Install-Bazel"><a href="#2-Install-Bazel" class="headerlink" title="2. Install Bazel"></a>2. Install Bazel</h1><h2 id="a-download"><a href="#a-download" class="headerlink" title="a. download"></a>a. download</h2><p>I got a zip file of bazel from <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">here</a> and unzipped it:</p><pre><code class="bash">$ wget https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip$ unzip -d bazel bazel-0.5.1-dist.zip</code></pre><h2 id="b-edit-bootstrap-files"><a href="#b-edit-bootstrap-files" class="headerlink" title="b. edit bootstrap files"></a>b. edit bootstrap files</h2><p>In the unzipped directory, I opened the <code>scripts/bootstrap/compile.sh</code> file:</p><pre><code class="bash">$ cd bazel$ vi scripts/bootstrap/compile.sh</code></pre><p>searched for lines that looked like following:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot;</code></pre><p>and appended <code>-J-Xmx500M</code> to the last line so that the whole lines would look like:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot; -J-Xmx500M</code></pre><p>It was for enlarging the max heap size of Java.</p><h2 id="c-compile"><a href="#c-compile" class="headerlink" title="c. compile"></a>c. compile</h2><p>After that, started building with:</p><pre><code class="bash">$ chmod u+w ./* -R$ ./compile.sh</code></pre><p>It also took about an hour.</p><h2 id="d-install"><a href="#d-install" class="headerlink" title="d. install"></a>d. install</h2><p>After the compilation had finished, I could find the compiled binary in <code>output</code> directory.</p><p>Copied it into <code>/usr/local/bin</code> directory:</p><pre><code class="bash">$ sudo cp output/bazel /usr/local/bin/</code></pre><h1 id="3-Build-libtensorflow-so"><a href="#3-Build-libtensorflow-so" class="headerlink" title="3. Build libtensorflow.so"></a>3. Build libtensorflow.so</h1><p>(I referenced <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md" target="_blank" rel="noopener">this document</a> for following processes)</p><h2 id="a-download-1"><a href="#a-download-1" class="headerlink" title="a. download"></a>a. download</h2><p>Got the tensorflow go code with:</p><pre><code class="bash">$ go get -d github.com/tensorflow/tensorflow/tensorflow/go</code></pre><h2 id="b-edit-files"><a href="#b-edit-files" class="headerlink" title="b. edit files"></a>b. edit files</h2><p>In the downloaded directory, I checked out the latest tag and replaced <code>lib64</code> to <code>lib</code> in the files with:</p><pre><code class="bash">$ cd ${GOPATH}/src/github.com/tensorflow/tensorflow$ git fetch --all --tags --prune$ git checkout tags/v1.3.0$ grep -Rl &#39;lib64&#39; | xargs sed -i &#39;s/lib64/lib/g&#39;</code></pre><p>Raspberry Pi still runs on 32bit OS, so they had to be changed like this.</p><p>After that, I commented <code>#define IS_MOBILE_PLATFORM</code> out in <code>tensorflow/core/platform/platform.h</code>:</p><pre><code class="c">// Since there&#39;s no macro for the Raspberry Pi, assume we&#39;re on a mobile// platform if we&#39;re compiling for the ARM CPU.//#define IS_MOBILE_PLATFORM    // &lt;= commented this line</code></pre><p>If it is not commented out, bazel will build for mobile platforms like <code>iOS</code> or <code>Android</code>, not Raspberry Pi.</p><p>To do this easily, just run:</p><pre><code class="bash">$ sed -i &quot;s|#define IS_MOBILE_PLATFORM|//#define IS_MOBILE_PLATFORM|g&quot; tensorflow/core/platform/platform.h</code></pre><p>Finally, it was time to configure and build tensorflow.</p><h2 id="c-configure-and-build"><a href="#c-configure-and-build" class="headerlink" title="c. configure and build"></a>c. configure and build</h2><pre><code class="bash">$ ./configure</code></pre><p>I had to answer to some questions here.</p><p>Then I started building <code>libtensorflow.so</code> with:</p><pre><code class="bash">$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so</code></pre><p>My Pi became unresponsive many times during this process, but I kept it going on.</p><h2 id="d-install-1"><a href="#d-install-1" class="headerlink" title="d. install"></a>d. install</h2><p>After a long time of struggle, (it took nearly 7 hours for me!)</p><p>I finally got <code>libtensorflow.so</code> compiled in <code>bazel-bin/tensorflow/</code>.</p><p>So I copied it into <code>/usr/local/lib/</code>:</p><pre><code class="bash">$ sudo cp ./bazel-bin/tensorflow/libtensorflow.so /usr/local/lib/$ sudo chmod 644 /usr/local/lib/libtensorflow.so$ sudo ldconfig</code></pre><p>All done. Time to test!</p><h1 id="4-Go-Test"><a href="#4-Go-Test" class="headerlink" title="4. Go Test"></a>4. Go Test</h1><p>I ran a test for validating the installation:</p><pre><code class="bash">$ go test github.com/tensorflow/tensorflow/tensorflow/go</code></pre><p>then I could see:</p><pre><code class="bash">ok      github.com/tensorflow/tensorflow/tensorflow/go  0.350s</code></pre><p>Ok, it works!</p><p><strong>Edit</strong>: As <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go#generate-wrapper-functions-for-ops" target="_blank" rel="noopener">this instruction</a> says, I had to regenerate operations before the test:</p><pre><code class="bash">$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op</code></pre><h1 id="5-Further-Test"><a href="#5-Further-Test" class="headerlink" title="5. Further Test"></a>5. Further Test</h1><p>I wanted to see a simple go program running, so I wrote this code:</p><pre><code class="go">// sample.gopackage mainimport (    &quot;fmt&quot;    tf &quot;github.com/tensorflow/tensorflow/tensorflow/go&quot;)// Sorry - I don&#39;t have a good example yet :-Pfunc main() {    tensor, _ := tf.NewTensor(int64(42))    if v, ok := tensor.Value().(int64); ok {        fmt.Printf(&quot;The answer to the life, universe, and everything: %v\n&quot;, v)    }}</code></pre><p>and ran it with <code>go run sample.go</code>:</p><pre><code>The answer to the life, universe, and everything: 42</code></pre><p>See the result?</p><p>From now on, I can write tensorflow applications in go, on Raspberry Pi! :-)</p><hr><h1 id="98-Trouble-shooting"><a href="#98-Trouble-shooting" class="headerlink" title="98. Trouble shooting"></a>98. Trouble shooting</h1><h2 id="Build-failure-due-to-a-problem-with-Eigen"><a href="#Build-failure-due-to-a-problem-with-Eigen" class="headerlink" title="Build failure due to a problem with Eigen"></a>Build failure due to a problem with Eigen</h2><p>Back in the day with <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.2.0" target="_blank" rel="noopener">Tensorflow 1.2.0</a>, I encountered <a href="https://github.com/tensorflow/tensorflow/issues/9697" target="_blank" rel="noopener">this issue</a> while building, but it’s still not fixed yet in <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0" target="_blank" rel="noopener">1.3.0</a>.</p><p>So I had to work around this problem again by editing <code>tensorflow/workspace.bzl</code>from:</p><pre><code class="bzl">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,    ],    sha256 = &quot;ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4&quot;,    strip_prefix = &quot;eigen-eigen-f3a22f35b044&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>to:</p><pre><code class="bash">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,    ],    sha256 = &quot;a34b208da6ec18fa8da963369e166e4a368612c14d956dd2f9d7072904675d9b&quot;,    strip_prefix = &quot;eigen-eigen-d781c1de9834&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>and starting again from the beginning:</p><pre><code class="bash">$ bazel clean$ ./configure$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so...</code></pre><p>Then I could build it without further problems.</p><p>I hope it would be fixed on future releases.</p><hr><h1 id="99-Wrap-up"><a href="#99-Wrap-up" class="headerlink" title="99. Wrap-up"></a>99. Wrap-up</h1><p>Installing TensorFlow on Raspberry Pi is not easy yet. (There’s <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi" target="_blank" rel="noopener">a kind project</a> which makes it super easy though!)</p><p>Installing <code>libtensorflow.so</code> is a lot more difficult, because it takes too much time to build it.</p><p>But it is worth trying; managing TensorFlow graphs in golang will be handy for people who don’t love python - just like me.</p><hr><h1 id="999-If-you-need-one"><a href="#999-If-you-need-one" class="headerlink" title="999. If you need one,"></a>999. If you need one,</h1><p>You don’t have time to build it yourself, but still need the compiled file?</p><p>Good, take it <a href="https://github.com/meinside/libtensorflow.so-raspberrypi/releases" target="_blank" rel="noopener">here</a>.</p><p>I cannot promise, but will try keeping it up-to-date whenever a newer version of tensorflow comes out.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-Used-Hardwares-and-Softwares&quot;&gt;&lt;a href=&quot;#0-Used-Hardwares-and-Softwares&quot; class=&quot;headerlink&quot; title=&quot;0. Used Hardwares and Softwares&quot;&gt;&lt;/a&gt;0. Used Hardwares and Softwares&lt;/h1&gt;&lt;p&gt;All steps were taken on my &lt;strong&gt;Raspberry Pi 3 B&lt;/strong&gt; model with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimum GPU memory allocated (16MB)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1GB&lt;/strong&gt; of swap memory&lt;/li&gt;
&lt;li&gt;External USB HDD (as root partition)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="Tensorflow" scheme="https://readailib.com/tags/Tensorflow/"/>
    
      <category term="ARM" scheme="https://readailib.com/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>在树莓派kubernetes集群部署gRPC框架编写的微服务</title>
    <link href="https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/"/>
    <id>https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/</id>
    <published>2019-03-04T16:14:16.000Z</published>
    <updated>2019-03-06T00:50:11.802Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.</p><a id="more"></a><p>本次教程需要准备:</p><ul><li>一个启动好的树莓派kubernetes集群(参考: <a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a>)</li><li>protobuf工具(参考 <a href="https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可" target="_blank" rel="noopener">https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可</a>)</li><li>Docker (参考: <a href="https://docs.docker.com/engine/installation/" target="_blank" rel="noopener">https://docs.docker.com/engine/installation/</a>)</li><li>kubectl工具 (参考: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>)</li></ul><p>本文章参考: </p><ul><li><a href="https://outcrawl.com/getting-started-microservices-go-grpc-kubernetes" target="_blank" rel="noopener">Getting Started with Microservices using Go, gRPC and Kubernetes</a></li></ul><p>本文章的实例代码:</p><ul><li><a href="https://github.com/rh01/grpc-microservice-k8s" target="_blank" rel="noopener">https://github.com/rh01/grpc-microservice-k8s</a></li></ul><h2 id="定义我们的protobuf文件"><a href="#定义我们的protobuf文件" class="headerlink" title="定义我们的protobuf文件"></a>定义我们的protobuf文件</h2><p>protobuf是google的一个序列化结构化数据工具,它可以让人们定义好相关的结构,使用protoc工具自动生成对应的代码.类似的结构化工具还有thrift.</p><blockquote><p>微服务:这里我主要实现一个最大公约数的功能,输入两个数值,返回这两个数的最大公约数.</p></blockquote><p>这里既然使用gRPC来做,那么主要使用rpc来实现服务调用,因为rpc实现的是服务之间的同步调用，即客户端调用服务并等待响应。gRPC是提供RPC功能的框架之一。此时我们需要使用Protocol Buffer的接口定义语言中编写消息类型和服务的代码并进行编译。</p><p>下面就是我们使用protobuf语言定义的消息类型和服务.(具体参考: <a href="https://grpc.io/docs/quickstart/go.html)" target="_blank" rel="noopener">https://grpc.io/docs/quickstart/go.html)</a></p><pre><code class="bash">$ mkdir -pv ~/go/src/github.com/rh01/mini-deploy-app/pb$ cd ~/go/src/github.com/rh01/mini-deploy-app/pb$ vim pb.proto</code></pre><pre><code class="yaml">syntax = &quot;proto3&quot;; // protobuf 版本package pb;        // 代码生成的package名字// 定义的请求消息体message GCDRequest {    uint64 a = 1;    uint64 b = 2;}// 定义的响应消息体message GCDResponse {    uint64 result = 1;}// 调用的远程服务,这是client请求server端的远程计算服务service GCDService {    rpc Compute (GCDRequest) returns (GCDResponse) {}}</code></pre><p>接下来我们需要使用 protoc 生成对应的服务代码</p><pre><code>$ protoc -I . --go_out=plugins=grpc:. ./*.proto</code></pre><blockquote class="colorquote warning"><p><strong>提前须知</strong>:</p><p>1). 执行上面的指令需要使用安装 grpc 和 proto-gen-go 工具,使用下面的命令:</p><pre><code class="bash">$ go get -u google.golang.org/grpc$ go get -u github.com/golang/protobuf/protoc-gen-go</code></pre><p>2). 将 $GOPATH/bin 目录添加到PATH环境变量中</p><pre><code class="bash">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$GOPATH</span>/bin</code></pre></blockquote><p>这时应该生成了 gcd.pb.go 程序.</p><h2 id="最大公约数服务"><a href="#最大公约数服务" class="headerlink" title="最大公约数服务"></a>最大公约数服务</h2><h3 id="定义服务端"><a href="#定义服务端" class="headerlink" title="定义服务端"></a>定义服务端</h3><p>gcd 服务将会使用上一步生成的代码进行实现gcd计算服务.</p><pre><code class="bash">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p gcd$ vim main.go</code></pre><pre><code class="go">package mainimport (    &quot;log&quot;    &quot;net&quot;    context &quot;golang.org/x/net/context&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)</code></pre><p>在main函数中主要定义server结构,并将其注册为server端,用于处理 gcd 计算的请求.然后启动grpc服务.</p><pre><code class="go">type server struct {}func main() {    lis, err := net.Listen(&quot;tcp&quot;, &quot;:3000&quot;)    if err != nil {        log.Fatalf(&quot;Failed to listen: %v&quot;, err)    }    s := grpc.NewServer()    pb.RegisterGCDServiceServer(s, &amp;server{})    reflection.Register(s)    if err := s.Serve(lis); err != nil {        log.Fatalf(&quot;Failed to serve: %v&quot;, err)    }}</code></pre><p>实现GCDServiceServer接口的 <code>Compute</code> 方法, server 结构对象的指针作为方法接受者.</p><pre><code class="Go">// gcd.pb.go// GCDServiceServer is the server API for GCDService service.type GCDServiceServer interface {    Compute(context.Context, *GCDRequest) (*GCDResponse, error)}func (s *server) Compute(ctx context.Context, r *pb.GCDRequest) (*pb.GCDResponse, error) {    a, b := r.A, r.B    for b != 0 {        a, b = b, a%b    }    return &amp;pb.GCDResponse{Result: a}, nil}</code></pre><h2 id="定义RESTFul客户端"><a href="#定义RESTFul客户端" class="headerlink" title="定义RESTFul客户端"></a>定义RESTFul客户端</h2><p>前端使用 <a href="https://github.com/gin-gonic/gin" target="_blank" rel="noopener">gin</a> 框架,主要是提供一个REST风格的访问方式和调用我们定义的gcd服务端执行实际的计算任务.</p><pre><code class="go">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p api$ vim main.go</code></pre><pre><code>package mainimport (    fmt &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;strconv&quot;    &quot;github.com/gin-gonic/gin&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;)func main() {    conn, err := grpc.Dial(&quot;gcd-service:3000&quot;, grpc.WithInsecure())    if err != nil {        log.Fatalf(&quot;Dial failed: %v&quot;, err)    }    gcdClient := pb.NewGCDServiceClient(conn)}</code></pre><p>上面的代码主要使用rpc的方式访问我们定义的服务端,此时的 gcd-service:3000 就是我们gcd服务端的 endpoint,这个就是服务地址,需要在kubernetes中定义.</p><pre><code class="go">    r := gin.Default()    r.GET(&quot;/gcd/:a/:b&quot;, func(c *gin.Context) {        // Parse parameters        a, err := strconv.ParseUint(c.Param(&quot;a&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter A&quot;})            return        }        b, err := strconv.ParseUint(c.Param(&quot;b&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter B&quot;})            return        }        // Call GCD service        req := &amp;pb.GCDRequest{A: a, B: b}        if res, err := gcdClient.Compute(c, req); err == nil {            c.JSON(http.StatusOK, gin.H{                &quot;result&quot;: fmt.Sprint(res.Result),            })        } else {            c.JSON(http.StatusInternalServerError, gin.H{&quot;error&quot;: err.Error()})        }    })</code></pre><p>接下来处理 /gcd/:a/:b 请求,读取参数 A 和 B,然后调用GCD服务.</p><p>最后运行我们的REST API端.启动一个API server.</p><pre><code class="go">    // Run HTTP server    if err := r.Run(&quot;:3000&quot;); err != nil {        log.Fatalf(&quot;Failed to run server: %v&quot;, err)    }</code></pre><h2 id="构建Docker镜像"><a href="#构建Docker镜像" class="headerlink" title="构建Docker镜像"></a>构建Docker镜像</h2><p>下面是我定义的Dockerfile文件,因为有两个服务,所以这里分成两个Docker镜像,一个是gcd服务,另外一个是提供RESTAPI访问的客户端api.</p><blockquote><p>有关Docker的多阶段构建参考: <a href="https://www.41sh.cn/?id=61" target="_blank" rel="noopener">[实战] 将golang编写的微服务部署在树莓派搭建的kubernetes集群</a></p></blockquote><pre><code class="dockerfile"># Dockerfile.gcdFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/gcdCOPY gcd .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/gcd-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/gcd-server /usr/local/bin/gcdEXPOSE 3000CMD [ &quot;gcd&quot; ]</code></pre><pre><code class="dockerfile"># Dockerfile.apiFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/apiCOPY api .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/api-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/api-server /usr/local/bin/apiEXPOSE 3000CMD [ &quot;api&quot; ]</code></pre><p>然后构建</p><pre><code class="bash">$ docker build -t rh02/apiserver:v1.0.0 -f Dockerfile.api . $ docker build -t rh02/gcdserver:v1.0.0 -f Dockerfile.gcd .</code></pre><h2 id="部署到kubernetes"><a href="#部署到kubernetes" class="headerlink" title="部署到kubernetes"></a>部署到kubernetes</h2><p>定义两个Deployment和对应的两个Service,并且将gcd服务的名字写成我们api服务调用的名字:gcd-service.</p><p>下面是gcd的deployment和service的定义: gcd.yaml</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: gcd-deployment  labels:    app: gcdspec:  selector:    matchLabels:      app: gcd  replicas: 3  template:    metadata:      labels:        app: gcd    spec:      containers:      - name: gcd        image: rh02/gcdserver:v1.0.0        imagePullPolicy: Always      ports:      - name: gcd-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: gcd-servicespec:  selector:    app: gcd  ports:  - port: 3000    targetPort: gcd-service</code></pre><p>创建api.yaml, service类型设置为NodePort,从而可以在集群外部也可以访问,对于GCD服务,类型设置为ClusterIP即可,只需要在集群内部访问就可以.</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: api-deployment  labels:    app: apispec:  selector:    matchLabels:      app: api  replicas: 1  template:    metadata:      labels:        app: api    spec:      containers:      - name: api        image: rh02/apiserver:v1.0.0        imagePullPolicy: Always      ports:      - name: api-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: api-servicespec:  type: NodePort  selector:    app: api  ports:  - port: 3000    targetPort: api-service</code></pre><p>使用kubectl创建两个资源:</p><pre><code class="bash">$ kubectl create -f api.yaml$ kubectl create -f gcd.yaml</code></pre><p>检查所有的Pod是否正在运行, 可以指定 <code>-w</code> 标记,查看启动的过程.</p><pre><code class="bash">$ kubectl get pods -wNAME                             READY     STATUS    RESTARTS   AGEapi-deployment-778049682-3vd0z   1/1       Running   0          3sgcd-deployment-544390878-0zgc8   1/1       Running   0          2sgcd-deployment-544390878-p78g0   1/1       Running   0          2sgcd-deployment-544390878-r26nx   1/1       Running   0          2s</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771763579640.png" alt="截图_2019-03-05_15-42-18.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771802295607.png" alt="截图_2019-03-05_15-43-02.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.&lt;/p&gt;
    
    </summary>
    
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="kubernetes" scheme="https://readailib.com/tags/kubernetes/"/>
    
      <category term="Edge computing" scheme="https://readailib.com/tags/Edge-computing/"/>
    
      <category term="gRPC" scheme="https://readailib.com/tags/gRPC/"/>
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="REST" scheme="https://readailib.com/tags/REST/"/>
    
  </entry>
  
  <entry>
    <title>部署微服务到树莓派的kubernetes集群</title>
    <link href="https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/"/>
    <id>https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/</id>
    <published>2019-02-28T17:02:07.000Z</published>
    <updated>2019-03-07T08:18:18.768Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。</p><p>本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>那么下面就开始吧，本次的环境主要分为两台机器，分别我的本地计算机（这里使用的Mac），另外就是我需要部署的环境（有三个树莓派节点组成的kubernetes集群）。</p><p>这里你需要：</p><ul><li>熟悉go语言</li><li>熟悉微服务</li><li>熟悉Docker</li><li>熟悉kubernetes的部署与基本管理任务</li><li>熟悉Linux</li></ul><p>本章主要参考了下面的文章：</p><ul><li><a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a></li><li><a href="https://github.com/rh01/deploy-golang-applicaton-on-kubernetes" target="_blank" rel="noopener">https://github.com/rh01/deploy-golang-applicaton-on-kubernetes</a></li><li><a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></li><li><a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></li></ul><h2 id="编写golang的应用"><a href="#编写golang的应用" class="headerlink" title="编写golang的应用"></a>编写golang的应用</h2><p><strong>本小节全部在我们本地的计算机中做的！</strong></p><p>在本地创建项目文件夹goappk8s，并且将该文件夹设置成临时的 GOPATH环境变量的Value值。</p><pre><code class="bash">$ mkdir -pv k8sapp/src$ export GOPATH=/User/rh01/k8sapp/</code></pre><p>然后在<strong>src</strong>目录下面添加 golang 代码：</p><pre><code class="bash">$ mkdir -pv github.com/rh01/goappk8s &amp;&amp; cd github.com/rh01/goappk8s$ cat &lt;&lt; EOF &gt;&gt; main.gopackage mainimport (    &quot;github.com/gin-gonic/gin&quot;    &quot;net/http&quot;)func main() {    router := gin.Default()    router.GET(&quot;/ping&quot;, func(c *gin.Context) {        c.String(http.StatusOK, &quot;PONG&quot;)    })    router.Run(&quot;:8080&quot;)}EOF</code></pre><blockquote><p>注：上面的代码取自 <a href="https://github.com/cnych/goappk8s" target="_blank" rel="noopener">https://github.com/cnych/goappk8s</a></p><p>另外上面的代码主要是创建了一个微服务，主要实现PING的功能</p></blockquote><p>可以看到上面的代码中，我们导入了第三方包 github.com/gin-gonic/gin，这时可以手动将该依赖包下载下来放置到<strong>GOPATH</strong>下面，这里使用了 <strong>govendor</strong> 来进行管理，当然你可以使用其他的包管理工具，比如：dep、glid或者利用go mod 等等。</p><p>在 <strong>github.com/rh01/goappk8s</strong> 目录下面执行下面的操作：</p><blockquote class="colorquote info"><p><strong>如何安装 govendor：</strong></p><p>在Mac中可以使用brew工具或者使用下面的命令</p><pre><code class="bash">$ go get -u github.com/kardianos/govendor</code></pre></blockquote><p>下面我们使用govendor来将依赖包缓存到本地项目中，方便移植和项目管理。</p><pre><code class="bash"># 初始化本地项目文件夹为使用vendor管理包$ govendor init# 将依赖包缓存到本地$ govendor fetch github.com/gin-gonic/gin</code></pre><blockquote><p>上面 fetch 需要设置代理才能通过，还是老办法：</p><pre><code class="bash">$ export http_proxy=&quot;http://127.0.0.1:12333&quot;$ export https_proxy=&quot;http://127.0.0.1:12333&quot;</code></pre></blockquote><p>这样一个非常小的微服务应用就做完了，这时需要测试一下，看看是否正常运行。这时切换到 <strong>GOPARH</strong> 文件夹。执行下面的语句</p><pre><code class="bash">$ go install github.com/rh01/goappk8s &amp;&amp; ./bin/goappk8s[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.[GIN-debug] [WARNING] Running in &quot;debug&quot; mode. Switch to &quot;release&quot; mode in production. - using env:   export GIN_MODE=release - using code:  gin.SetMode(gin.ReleaseMode)[GIN-debug] GET    /ping                     --&gt; main.main.func1 (3 handlers)[GIN-debug] Listening and serving HTTP on :8080</code></pre><p>这时会打印出如上面所示的日志信息，这时我们可以通过 curl 来访问。</p><pre><code class="bash">$ curl localhost:8080/pingPONG</code></pre><p>这时我们的服务是已经正常可以运行的了。</p><h2 id="打包成Docker镜像（使用ARM）"><a href="#打包成Docker镜像（使用ARM）" class="headerlink" title="打包成Docker镜像（使用ARM）"></a>打包成Docker镜像（使用ARM）</h2><p>这里主要使用了 Docker 的多阶段构建来打造一个非常小的镜像，这对我们的边缘端来讲，是非常必要的，因为他们的资源是非常有限的。</p><blockquote><p>有关 Docker镜像的多阶段构建以及Docker镜像优化问题，请详见我之前的一篇文章：</p><ul><li><a href="https://www.41sh.cn/?id=25" target="_blank" rel="noopener">https://www.41sh.cn/?id=25</a></li></ul></blockquote><p>下面我们看一下Dockerfile文件吧。</p><pre><code class="dockerfile">FROM golang AS build-envADD . /go/src/appWORKDIR /go/src/appENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-serverFROM armhf/alpine:latestRUN apk add -U tzdataRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai  /etc/localtimeCOPY --from=build-env /go/src/app/app-server /usr/local/bin/app-serverEXPOSE 8080CMD [ &quot;app-server&quot; ]</code></pre><blockquote class="colorquote info"><p>这里也参考了知乎的这片文章：<a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></p><p>主要修改如下：</p><p>(1).  增加了</p><pre><code class="dockerfile"><span class="keyword">ENV</span> http_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span><span class="keyword">ENV</span> https_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span></code></pre><p>这是因为国内的网络环境，我们需要添加代理，才能拉取相关的依赖包</p><p>(2).  修改了</p><pre><code class="dockerfile"><span class="keyword">RUN</span><span class="bash"> GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-server</span></code></pre><p> 这是因为我们需要在树莓派 arm架构下去编译和运行golang程序，因此需要交叉编译。</p><p>(3). 修改了</p><pre><code class="dockerfile"><span class="keyword">FROM</span> armhf/alpine:latest</code></pre><p>这个也是因为硬件架构的不同进行相应的修改</p></blockquote><p>然后构建<code>Docker</code>镜像：</p><pre><code class="bash">$ docker build -t rh02/goappk8s:v1.0.0 ........(省略了)Successfully built 00751f94d8a9Successfully tagged cnych/goappk8s:v1.0.0$ docker push rh02/goappk8s:v1.0.0</code></pre><p>上面的操作可以将我们本地的镜像<code>rh02/goappk8s:v1.0.0</code>推送到公共的<code>dockerhub</code>上面去（前提是你得先注册了dockerhub）。</p><h2 id="将服务部署在kubernetes"><a href="#将服务部署在kubernetes" class="headerlink" title="将服务部署在kubernetes"></a>将服务部署在kubernetes</h2><p>如果要将微服务部署在kubernetes上，只需要写一个yaml文件，定义好你需要的资源对象即可，下面先给出我们的部署的yaml文件内容。</p><pre><code class="yaml">---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: goapp-deploy  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  replicas: 2  revisionHistoryLimit: 10  minReadySeconds: 5  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 1  template:    metadata:      labels:        k8s-app: goappk8s    spec:      containers:      - image: rh02/goappk8s:v1.1.0        imagePullPolicy: Always        name: goappk8s        ports:        - containerPort: 8080          protocol: TCP        resources:          limits:            cpu: 100m            memory: 100Mi          requests:            cpu: 50m            memory: 50Mi        livenessProbe:          tcpSocket:            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 3        readinessProbe:          httpGet:            path: /ping            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 2---apiVersion: v1kind: Servicemetadata:  name: goapp-svc  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  ports:    - name: api      port: 8080      protocol: TCP      targetPort: 8080  selector:    k8s-app: goappk8s---kind: IngressapiVersion: extensions/v1beta1metadata:  name: goapp-ingressspec:  rules:  - host: k8sapp1.41sh.cn    http:      paths:      - path: /        backend:          serviceName: goapp-svc          servicePort: api</code></pre><blockquote><p>这里的k8sapp1.41sh.cn 需要解析为ingress的node。详细可以参考 <a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></p></blockquote><p>因为我们编写了一个无状态的应用，因此上面主要创建了三个资源对象，分别为deployment（部署），service（服务）和ingress（主要负责负载均衡和提供一种访问的方式）对象。</p><p>使用kubectl来创建这三个资源对象。</p><pre><code class="bash">$ kubectl apply -f deployment.yamldeployment &quot;goapp-deploy&quot; createdservice &quot;goapp-svc&quot; createdingress &quot;goapp-ingress&quot; created</code></pre><p>这时我们需要创建一个traefik的ingress应用，用来处理ingress的请求。</p><pre><code class="bash">$ kubectl label node edge-node2 ingress-controller=traefik $ kubectl apply -f traefik.yaml</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>这时，我们都已经做完了，这时我们可以打开我们的dashboard看看怎么样。并且可以通过在浏览器中访问 <a href="http://k8sapp1.41sh.cn/ping" target="_blank" rel="noopener">http://k8sapp1.41sh.cn/ping</a> 来访问我们的微服务。</p><p>下面是成果截图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423865486307.png" alt="屏幕快照 2019-03-01 下午3.03.48.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423899753911.png" alt="屏幕快照 2019-03-01 下午3.04.39.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。&lt;/p&gt;
&lt;p&gt;本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。&lt;/p&gt;
    
    </summary>
    
      <category term="RPI kubernetes" scheme="https://readailib.com/categories/RPI-kubernetes/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/categories/RPI-kubernetes/Kubernetes/"/>
    
      <category term="Microservice" scheme="https://readailib.com/categories/RPI-kubernetes/Kubernetes/Microservice/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="Edge Computing" scheme="https://readailib.com/tags/Edge-Computing/"/>
    
      <category term="Gin" scheme="https://readailib.com/tags/Gin/"/>
    
  </entry>
  
  <entry>
    <title>使用minikube快速安装istio集群</title>
    <link href="https://readailib.com/2019/02/22/kubernetes/istio-minikube/"/>
    <id>https://readailib.com/2019/02/22/kubernetes/istio-minikube/</id>
    <published>2019-02-22T05:48:07.000Z</published>
    <updated>2019-03-07T07:33:38.744Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。</p><p>本节课不教：</p><ul><li>使用minikube部署多节点的kubernetes集群，详细教程请看：<a href="https://www.41sh.cn/?id=53" target="_blank" rel="noopener">https://www.41sh.cn/?id=53</a></li></ul><p>本节课的目标是：</p><ul><li><p>使用Helm或者手动方式来构建istio集群</p></li><li><p>使用istio框架来部微服务</p></li><li><p>服务治理与金丝雀发布等等</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820119606413.png" alt="23534644.png"></p></li></ul><h2 id="启动minikube集群"><a href="#启动minikube集群" class="headerlink" title="启动minikube集群"></a>启动minikube集群</h2><p>使用下面的指令在本地启动两个节点的k8s集群，分别为master和node节点，并使node节点加入到集群中。</p><pre><code class="bash"># 启动master节点，名字为k8s-m1$ minikube --profile k8s-m1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24# 以同样的方式启动node节点，名字为k8s-n1$ minikube --profile k8s-n1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24</code></pre><p>确认两个节点已经启动，但是现在node节点并没有加入到集群中，你可能使用下面指令会看到NotReady的标示，也有可能列表中不会出现k8s-n1的条目。</p><pre><code class="bash"># 切换到master节点的配置中，这样才可以使用kubectl来查询资源信息$ kubectl config --use-context k8s-m1# 查看当前的节点列表$ kubectl get no</code></pre><p>这时候需要将node节点加入集群，使用下面指令获得TOKEN，并且使用kubeadm join指令，使得node节点加入集群。</p><pre><code class="bash"># 获取master节点的ip地址$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;# 获取当前的TOKEN列表$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 执行下面指令进入 k8s-n1$ minikube --profile k8s-n1 ssh# 下面为进入 k8s-n1 VM 內执行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下结果后，即可以在 k8s-m1 context 来操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><p>这时候我们回到我们本地的机器（非VM），通过执行下面命令确认k8s-n1已经准备完成。</p><pre><code class="bash"># 查看当前的节点列表$ kubectl get no</code></pre><h2 id="使用helm安装istio"><a href="#使用helm安装istio" class="headerlink" title="使用helm安装istio"></a>使用helm安装istio</h2><p>为了方便起见，这里我使用helm chart来安装istio，这里参考了下面的文档：</p><ul><li><a href="https://istio.io/docs/setup/kubernetes/helm-install/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/helm-install/</a></li><li><a href="https://istio.io/docs/setup/kubernetes/download-release/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/download-release/</a></li></ul><p><em>1). 使用下面的命令安装helm</em>（mac系统）</p><pre><code class="bash">$ brew install kubernetes-helm</code></pre><blockquote><p>其他系统的安装方式请参考：<br><a href="https://helm.sh/docs/using_helm/#installing-helm" target="_blank" rel="noopener">https://helm.sh/docs/using_helm/#installing-helm</a></p></blockquote><p>这里是自动安装的，这里你安装的只是一个helm client，你需要又一个helm后端来支持helm自动化部署的功能，你也可以通过下面的命令查看当前的helm的安装情况以及版本。</p><pre><code class="bash">$ helm version</code></pre><blockquote><p>如果出现Server端没有起来的情况，使用helm init –service-account tiller 就自动将Tiller端安装到kubernetes集群中  。</p></blockquote><p><em>2). 下载istio并准备安装</em></p><p>使用下面的指令自动下载istio的最新发行版本，并解压到当前的文件夹下，这里有一个istio的客户端二进制文件，这时你需要手动的将二进制文件的目录添加到PATH环境变量中。</p><pre><code class="bash">$ curl -L https://git.io/getLatestIstio | sh -$ export PATH=&quot;$PATH:/Users/rh01/istio-1.0.6/bin&quot; #这是会话环境变量的设置，临时使用，如果想永久生效，请添加到 ~/.bashrc 或者 /etc/profile中</code></pre><p><em>3). 安装istioz</em></p><p>切换到istio的文件夹下，并使用helm 安装 install 文件夹下面的yaml 来创建istio。下面的命令是我在本地测试过的：</p><pre><code class="bash">$ cd istio-1.0.6$ helm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; $HOME/istio-1.0.6/istio.yaml$ kubectl create namespace istio-system                              # 创建istio-system命名空间，之后管理istio的所有资源$ kubectl apply -f install/kubernetes/helm/helm-service-account.yaml # 创建helm服务账号，使得helm能够有权限对istio-system命名空间进行操作$ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml # 创建crd$ kubectl apply -f $HOME/istio-1.0.6/istio.yaml                      # 创建istio所有资源对象$ kubectl get po -n istio-systemNAME                                      READY     STATUS      RESTARTS   AGEistio-citadel-6f444d9999-7l2hx            1/1       Running     0          1mistio-cleanup-secrets-qjk7c               0/1       Completed   0          1mistio-egressgateway-6d79447874-v6xds      1/1       Running     0          1mistio-galley-685bb48846-pxcs2             1/1       Running     0          1mistio-ingressgateway-5b64fffc9f-4jnr9     1/1       Running     0          1mistio-pilot-8645f5655b-s6nbq              0/2       Pending     0          1mistio-policy-547d64b8d7-vtxqh             2/2       Running     0          1mistio-security-post-install-r27sv         0/1       Completed   0          1mistio-sidecar-injector-5d8dd9448d-bwcvg   1/1       Running     0          1mistio-telemetry-c5488fc49-qr7sg           2/2       Running     0          1mprometheus-76b7745b64-pm68q               1/1       Running     0          1m</code></pre><blockquote class="colorquote warning"><p><strong>遇到的坑：</strong>等等我遇到了一个istio-pilot内存不足的情况，因为我是两个节点，但是因为是在本地创建的两个虚拟机，所有内存和CPU资源都比较小，因此当出现资源不足的时候，就Pending了,这时候需要手动修改一下请求的内存资源的大小就可以了。</p><p><em>使用到的命令</em>：</p><pre><code class="bash">$ kubectl edit istio-pilot-xxx -n istio-system<span class="comment"># 修改resource的request的memory为100Mi即可</span></code></pre></blockquote><h2 id="部署一个应用"><a href="#部署一个应用" class="headerlink" title="部署一个应用"></a>部署一个应用</h2><p>这个是官方给出的一个例子 bookinfo，这个应用由四个微服务组成，下面是应用的架构图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903061551882998396723.png" alt="20190222153800_46088.png"></p><p>下面的所有命令以及说明部分来自下面的文档：</p><ul><li><a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a></li><li><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></li></ul><p>下面的指令将会在kubernetes上部署一个bookinfo应用，并由istio提供微服务的一些服务调用和路由等等功能，具体的istio特性，后期见。</p><pre><code class="bash">$ kubectl label namespace default istio-injection=enablednamespace &quot;default&quot; labeled$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlservice &quot;details&quot; createddeployment.extensions &quot;details-v1&quot; createdservice &quot;ratings&quot; createddeployment.extensions &quot;ratings-v1&quot; createdservice &quot;reviews&quot; createddeployment.extensions &quot;reviews-v1&quot; createddeployment.extensions &quot;reviews-v2&quot; createddeployment.extensions &quot;reviews-v3&quot; createdservice &quot;productpage&quot; createddeployment.extensions &quot;productpage-v1&quot; created$ kubectl get servicesNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGEdetails       ClusterIP   10.97.137.200    &lt;none&gt;        9080/TCP   7skubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    56mproductpage   ClusterIP   10.106.43.117    &lt;none&gt;        9080/TCP   6sratings       ClusterIP   10.108.175.114   &lt;none&gt;        9080/TCP   7sreviews       ClusterIP   10.96.73.150     &lt;none&gt;        9080/TCP   6s$ kubectl get pods$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yamlgateway.networking.istio.io &quot;bookinfo-gateway&quot; createdvirtualservice.networking.istio.io &quot;bookinfo&quot; created$  kubectl get gatewayNAME               AGEbookinfo-gateway   9s$ kubectl get svc istio-ingressgateway -n istio-systemNAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                   AGEistio-ingressgateway   LoadBalancer   10.101.64.224   &lt;pending&gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31643/TCP,8060:30348/TCP,853:30934/TCP,15030:32334/TCP,15031:32681/TCP   17m</code></pre><p>这时候大家会看到istio-ingressgateway服务的EXTERNAL_IP是pending状态，这是因为我们没有指定外置的负载均衡器的ip地址，这里有三种处理方式：</p><ul><li>如果处于云服务厂商的环境，并且有负载均衡器，这时候就填写负载均衡器的ip地址</li><li>如果没有，可以采取将服务的类型改为NodePort类型，或者直接使用提供的NodePort，这样就可以使用http://&lt;Ingress所在Node的IP&gt;:NodePort/productpage的方式访问</li><li>另外你也可以将ingress对象处于的Node的ip作为ExternalIP，也是可以访问的。这样就可以直接使用<a href="http://ExternalIP:80/productpage进行访问。" target="_blank" rel="noopener">http://ExternalIP:80/productpage进行访问。</a></li></ul><blockquote><p><em>具体的如何获取ingress和配置ingress，详见下面的文档</em>：<br><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></p></blockquote><h2 id="大功告成"><a href="#大功告成" class="headerlink" title="大功告成"></a>大功告成</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820787188445.png" alt="屏幕快照 2019-02-22 下午3.15.31.png"></p><p>如果继续刷新，会发现有三个版本的应用出现，就是红色的星星，黑色的星星，没有星星这三个版本，会随机出现，我们可以使用istio来管理这些不同版本的应用，通过金丝雀发布，灰度发布等一些高级特性实现切流量的功能，后面的文章我会详细介绍有关istio的高级特性。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821049797553.png" alt="屏幕快照 2019-02-22 下午3.37.12.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821077460003.png" alt="屏幕快照 2019-02-22 下午3.37.39.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。&lt;/p&gt;
&lt;p&gt;本节课不教：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用minikube部署多节点的kubernetes集群，详细教程请看：&lt;a href=&quot;https://www.41sh.c
      
    
    </summary>
    
      <category term="istio" scheme="https://readailib.com/categories/istio/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="istio" scheme="https://readailib.com/tags/istio/"/>
    
  </entry>
  
  <entry>
    <title>使用 Minikube 来部署本地 kubernetes 多节点集群</title>
    <link href="https://readailib.com/2019/02/18/kubernetes/multi-nodes-kubernetes-using-minikube/"/>
    <id>https://readailib.com/2019/02/18/kubernetes/multi-nodes-kubernetes-using-minikube/</id>
    <published>2019-02-18T07:57:21.000Z</published>
    <updated>2019-03-07T08:19:23.559Z</updated>
    
    <content type="html"><![CDATA[<p>一般来讲，使用minikube的目的主要用于作为本地单机测试集群，也只能构建单节点的kubernetes集群，本文章参看<a href="https://k2r2bai.com/" target="_blank" rel="noopener">凯仁兄</a>的方法使得minikube能够借助vitual box软件来实现多节点的部署，其中包括Master/Worker节点的部署与安装，下面主要针对kubernetes的最新版本kubernetes 1.13.2, 网络插件为Calico，主要为了测试Network Policy的功能。</p><a id="more"></a><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/20190218164923_24529.jpg" alt="img"></p><h2 id="系统准备"><a href="#系统准备" class="headerlink" title="系统准备"></a>系统准备</h2><p>下面一定要确保以下的步骤都已经执行，所有要安装的软件包已经安转。</p><p>第一步肯定是准备minikube的执行文件，下面我列出了不同平台的二进制文件的下载地址：</p><ul><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-linux-amd64" target="_blank" rel="noopener">Linux</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-darwin-amd64" target="_blank" rel="noopener">Mac OS X</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-windows-amd64.exe" target="_blank" rel="noopener">Windows</a></li></ul><blockquote class="colorquote danger"><p><strong>误操作：</strong>  为了能够使 minikube 能够启动多节点的集群，一定使用上面给出的二进制执行文件链接。</p><p><em>提示：</em></p><p>如果你要使用 官方给的 minikube 的二进制文件来启动集群，此时如果你要使用网络插件Calico，请手动输入下面的命令即可生效。</p><pre><code class="bash">$ kubectl apply -f https://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml</code></pre></blockquote><p>第二步，需要将<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">Virtual Box</a>下载下来，然后提供给minikube来创建虚拟机。</p><blockquote><p>IMPORTANT: 测试机器一定要开启 VT-x or AMD-v virtualization.</p><p>虽然建议使用 VBox，但是也可以其他的虚拟化解决方案，比如使用 KVM, xhyve等虚拟机管理软件。</p></blockquote><p>第三步，下载测试机器操作系统适配的 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubeclt</a>。</p><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><p>本节主要介绍如何使用minikube来建立集群，并且相应的创建master节点和worker节点。</p><p>开始之前，确认本测试机器是否已经安装过Minikube，如果有的话，就把上面下载二进制文件放置在任意方便的位置，或者直接替代之前的然后在启动集群之前，删除Home文件夹下的.minikube文件夹。</p><pre><code class="bash">$ rm -rf $HOME/.minikube</code></pre><h3 id="Master-节点"><a href="#Master-节点" class="headerlink" title="Master 节点"></a>Master 节点</h3><p>首先通过 Minikube 执行以下指令来启动 Master节点，并通过 kubectl 检查：</p><pre><code class="bash">$ minikube --profile k8s-m1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24?  minikube v0.34.1 on darwin (amd64)?  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...?  &quot;k8s-m1&quot; IP address is 192.168.99.102?  Configuring Docker as the container runtime ...    ▪ env HTTP_PROXY=http://192.168.99.1:12333    ▪ env HTTPS_PROXY=http://192.168.99.1:12333    ▪ env NO_PROXY=127.0.0.1/24✨  Preparing Kubernetes environment ...?  Pulling images required by Kubernetes v1.13.3 ...?  Launching Kubernetes v1.13.3 using kubeadm ... ?  Configuring cluster permissions ...?  Verifying component health .....?  kubectl is now configured to use &quot;k8s-m1&quot;?  Done! Thank you for using minikube!</code></pre><blockquote><p><code>--vm-driver</code> 可以选择使用其他 VM driver 启动虚拟机，如 xhyve、hyperv、hyperkit 与 kvm2 等等。</p></blockquote><p>完成后，确认 k8s-m1 节点處处于Ready 状态：</p><pre><code class="bash">$ kubectl get noNAME     STATUS   ROLES    AGE    VERSIONk8s-m1   Ready    master   2m8s   v1.13.2</code></pre><p>下面来部署Node节点，通过minikube执行下面指令来启动Node节点。</p><pre><code>$ minikube --profile k8s-n1 start --network-plugin=cni --node...Stopping extra container runtimes...# 接着取得 Master IP 与 Token$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 執行以下指令進入 k8s-n1$ minikube --profile k8s-n1 ssh#  下面是 在k8s-n1 VM 里执行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下结果后，即可以在 k8s-m1 context 操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><blockquote class="colorquote warning"><p>1). 上面的 IP 有可能不同，请手动确认 Master 节点 IP。</p><p>2).  如果上面的TOKEN失效，请使用下面的指令生成一个，或者生成TOKEN时指定tt值为-1</p><pre><code class="bash">$ minikube --profile k8s-m1 ssh <span class="string">"sudo kubeadm token new"</span></code></pre></blockquote><p>接下来，我们可以通过 kuubectl 客户端检查Node是否加入到集群：</p><pre><code>$ kubectl config use-context k8s-m1Switched to context &quot;k8s-m1&quot;.$ kubectl get noNAME     STATUS   ROLES    AGE     VERSIONk8s-m1   Ready    master   3m44s   v1.13.2k8s-n1   Ready    &lt;none&gt;   80s     v1.13.2$ kubectl get csrNAME                                                   AGE    REQUESTOR                 CONDITIONnode-csr-Ut1k5mLXpXVsyZwjn2z2-fpie9HHyTkMU7wnrjDnD3E   118s   system:bootstrap:3qeeeu   Approved,Issued$ kubectl -n kube-system get po -o wideNAME                             READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATEScalico-node-qxkw5                2/2     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;calico-node-srhlk                2/2     Running   0          3m24s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-826nz         1/1     Running   0          3m27s   10.244.0.3       k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-9z7mr         1/1     Running   0          3m27s   10.244.0.2       k8s-m1   &lt;none&gt;           &lt;none&gt;etcd-k8s-m1                      1/1     Running   0          2m40s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-m1        1/1     Running   0          3m48s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-n1        1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-apiserver-k8s-m1            1/1     Running   0          2m36s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-controller-manager-k8s-m1   1/1     Running   0          2m50s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-proxy-768w8                 1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-proxy-b7ndj                 1/1     Running   0          3m27s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-scheduler-k8s-m1            1/1     Running   0          2m46s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;storage-provisioner              1/1     Running   0          2m46s   192.168.99.100   k8s-m1   &lt;none&gt;</code></pre><p>这样一个 Kubernetes 集群就完成了，速度快一点不到 10 分钟就可以建立好了。</p><h2 id="删除虚拟机"><a href="#删除虚拟机" class="headerlink" title="删除虚拟机"></a>删除虚拟机</h2><p><em>清除环境一条指令即可：</em></p><pre><code>$ minikube --profile &lt;node_name&gt; delete</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般来讲，使用minikube的目的主要用于作为本地单机测试集群，也只能构建单节点的kubernetes集群，本文章参看&lt;a href=&quot;https://k2r2bai.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;凯仁兄&lt;/a&gt;的方法使得minikube能够借助vitual box软件来实现多节点的部署，其中包括Master/Worker节点的部署与安装，下面主要针对kubernetes的最新版本kubernetes 1.13.2, 网络插件为Calico，主要为了测试Network Policy的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Minikube Multi-Node Cluster" scheme="https://readailib.com/categories/Minikube-Multi-Node-Cluster/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dooker" scheme="https://readailib.com/tags/Dooker/"/>
    
      <category term="Calico" scheme="https://readailib.com/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>在仪表盘上增加heapster指标</title>
    <link href="https://readailib.com/2019/01/15/kubernetes/add-heapster-to-dashboard/"/>
    <id>https://readailib.com/2019/01/15/kubernetes/add-heapster-to-dashboard/</id>
    <published>2019-01-15T08:41:31.000Z</published>
    <updated>2019-03-07T08:43:22.350Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547542105188485.png" alt="2019-01-15 16-47-57屏幕截图.png"></p><p>Prerequisites: You have a kubernetes cluster with the dashboard plugin installed, see the article from <a href="https://medium.com/@mrjensens" target="_blank" rel="noopener">Martin Jensen</a> entitled Kubernetes dashboard on ARM with RBAC for the instructions on how to do that.</p><a id="more"></a><pre><code class="bash">$ git clone $ cd heapster/</code></pre><p>and edit the heapster.yaml and influxdb.yaml files to change the image architecture from -amd64 to -arm. For example image: k8s.gcr.io/heapster-amd64:v1.4.2 should be changed to image: k8s.gcr.io/heapster-arm:v1.4.2</p><pre><code class="bash">$ kubectl create -f influxdb.yaml$ kubectl create -f heapster.yaml</code></pre><p>to deploy the heapster and influxeb deployment, service and serviceaccounts.</p><p>Now we need to add the roles from <a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">heapster</a>/<a href="https://github.com/kubernetes/heapster/tree/master/deploy" target="_blank" rel="noopener">deploy</a>/<a href="https://github.com/kubernetes/heapster/tree/master/deploy/kube-config" target="_blank" rel="noopener">kube-config</a>/rbac/.</p><pre><code class="bash">$ cd rbac/$ kubectl create -f heapster-rbac.yaml</code></pre><p>If you go back to the kubernetes dashboard, you will not see any metrics. They are being collected but will not appear until we restart the dashboard.</p><pre><code class="bash">$ kubectl delete -n kube-system kubernetes-dashboard-7fcc5cb979–85vt5</code></pre><p>should take care of that.</p><h2 id="Some-wrong"><a href="#Some-wrong" class="headerlink" title="Some wrong!!!"></a>Some wrong!!!</h2><hr><p>Bug solution: <a href="https://brookbach.com/2018/10/29/Heapster-on-Kubernetes-1.11.3.html" target="_blank" rel="noopener">https://brookbach.com/2018/10/29/Heapster-on-Kubernetes-1.11.3.html</a></p><blockquote class="colorquote info"><h1 id="Heapster"><a href="#Heapster" class="headerlink" title="Heapster"></a>Heapster</h1><h2 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h2><p>First, clone the Heapster repository.</p><pre><code class="bash">$ git <span class="built_in">clone</span> https://github.com/kubernetes/heapster/$ <span class="built_in">cd</span> heapster</code></pre><p>Then, set Grafana Service Type to NodePort and downgrade container version from 5.0.4 to 4.4.3 as follows. The reason why I downgrade the version is the dashboard on Grafana is not shown in 5.0.4.</p><pre><code class="bash">$ diff --git a/deploy/kube-config/influxdb/grafana.yaml b/deploy/kube-config/influxdb/grafana.yamlindex 216bd9a..266f47a 100644--- a/deploy/kube-config/influxdb/grafana.yaml+++ b/deploy/kube-config/influxdb/grafana.yaml@@ -13,7 +13,7 @@ spec:     spec:       containers:       - name: grafana-        image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4+        image: k8s.gcr.io/heapster-grafana-amd64:v4.4.3         ports:         - containerPort: 3000           protocol: TCP@@ -64,7 +64,7 @@ spec:   <span class="comment"># or through a public IP.</span>   <span class="comment"># type: LoadBalancer</span>   <span class="comment"># You could also use NodePort to expose the service at a randomly-generated port</span>-  <span class="comment"># type: NodePort</span>+  <span class="built_in">type</span>: NodePort   ports:   - port: 80     targetPort: 3000</code></pre><p>In this point, The pods are launched if I <code>kubectl apply</code> under <code>deploy/kube-config/rbac</code>and <code>deploy/kube-config/influxdb</code> , but the following error logs are generated and not worked correctly.</p><pre><code class="bash">E1028 07:39:05.011439       1 manager.go:101] Error <span class="keyword">in</span> scraping containers from Kubelet:XX.XX.XX.XX:10255: failed to get all container stats from Kubelet URL <span class="string">"http://XX.XX.XX.XX:10255/stats/container/"</span>: Post http://XX.XX.XX.XX:10255/stats/container/: dial tcp XX.XX.XX.XX:10255: getsockopt: connection refused</code></pre><h2 id="Apply-patch"><a href="#Apply-patch" class="headerlink" title="Apply patch"></a>Apply patch</h2><p>After googling the error message, I found <a href="https://github.com/kubernetes/heapster/issues/1936" target="_blank" rel="noopener">this issue</a>.</p><p>It looks the port to access are changed and I need to deal with HTTPS.</p><p>So I edit <code>deploy/kube-config/influxdb/heapster.yaml</code> and <code>deploy/kube-config/rbac/heapster-rbac.yaml</code> as below.</p><pre><code class="bash">diff --git a/deploy/kube-config/influxdb/heapster.yaml b/deploy/kube-config/influxdb/heapster.yamlindex e820ca5..195061a 100644--- a/deploy/kube-config/influxdb/heapster.yaml+++ b/deploy/kube-config/influxdb/heapster.yaml@@ -24,7 +24,7 @@ spec:         imagePullPolicy: IfNotPresent         <span class="built_in">command</span>:         - /heapster-        - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default+        - --<span class="built_in">source</span>=kubernetes.summary_api:<span class="string">''</span>?useServiceAccount=<span class="literal">true</span>&amp;kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250&amp;insecure=<span class="literal">true</span>         - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 --- apiVersion: v1diff --git a/deploy/kube-config/rbac/heapster-rbac.yaml b/deploy/kube-config/rbac/heapster-rbac.yamlindex 6e63803..1f982fb 100644--- a/deploy/kube-config/rbac/heapster-rbac.yaml+++ b/deploy/kube-config/rbac/heapster-rbac.yaml@@ -5,7 +5,7 @@ metadata: roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole-  name: system:heapster+  name: heapster subjects: - kind: ServiceAccount   name: heapster</code></pre><p>Moreover, I create <code>deploy/kube-config/rbac/heapster-role.yaml</code>.</p><pre><code class="yaml"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span><span class="attr">kind:</span> <span class="string">ClusterRole</span><span class="attr">metadata:</span><span class="attr">  name:</span> <span class="string">heapster</span><span class="attr">rules:</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">""</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">pods</span><span class="bullet">  -</span> <span class="string">nodes</span><span class="bullet">  -</span> <span class="string">namespaces</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span><span class="bullet">  -</span> <span class="string">list</span><span class="bullet">  -</span> <span class="string">watch</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">extensions</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">deployments</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span><span class="bullet">  -</span> <span class="string">list</span><span class="bullet">  -</span> <span class="string">update</span><span class="bullet">  -</span> <span class="string">watch</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">""</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">nodes/stats</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span></code></pre><h2 id="Create-pods"><a href="#Create-pods" class="headerlink" title="Create pods"></a>Create pods</h2><p>Finally, I apply these configurations.</p><pre><code class="bash">$ kubectl create -f ./deploy/kube-config/rbac/clusterrolebinding.rbac.authorization.k8s.io/heapster createdclusterrole.rbac.authorization.k8s.io/heapster created$ kubectl create -f ./deploy/kube-config/influxdb/deployment.extensions/monitoring-grafana createdservice/monitoring-grafana createdserviceaccount/heapster createddeployment.extensions/heapster createdservice/heapster createddeployment.extensions/monitoring-influxdb createdservice/monitoring-influxdb created</code></pre><p>After updating, I can configure launching Heapster, Grafana, and Influx DB by <code>kubectl get all -n kube-system</code>. And after waiting for minutes, I can see the metrics by <code>kubectl top node</code>.</p><h1 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h1><p>When I faced to some miss behavior on Kubernetes, I often find a clue of solution by seeing the log by <code>kubectl logs</code>.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://www.41sh.cn/zb_users/upload/2019/01/201901151547542105188485.png&quot; alt=&quot;2019-01-15 16-47-57屏幕截图.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Prerequisites: You have a kubernetes cluster with the dashboard plugin installed, see the article from &lt;a href=&quot;https://medium.com/@mrjensens&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Martin Jensen&lt;/a&gt; entitled Kubernetes dashboard on ARM with RBAC for the instructions on how to do that.&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes DashBoard" scheme="https://readailib.com/categories/Kubernetes-DashBoard/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dashboard" scheme="https://readailib.com/tags/Dashboard/"/>
    
      <category term="Heapster" scheme="https://readailib.com/tags/Heapster/"/>
    
  </entry>
  
  <entry>
    <title>树莓派kuernetes集群中部署dashboard</title>
    <link href="https://readailib.com/2019/01/15/kubernetes/kubernetes-dashboard/"/>
    <id>https://readailib.com/2019/01/15/kubernetes/kubernetes-dashboard/</id>
    <published>2019-01-15T08:32:03.000Z</published>
    <updated>2019-03-07T08:42:28.349Z</updated>
    
    <content type="html"><![CDATA[<p>在最近关于在树莓派集群上使用kubeadm配置安装Kubernetes 1.13.0的教程中，默认情况下启用了RBAC，本文章将介绍如何在启用RBAC的情况下运行Kubernetes仪表板。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul><li>部署和启动Kubernetes集群 (see <a href="https://www.41sh.cn/?id=16" target="_blank" rel="noopener">这篇文章</a>)</li><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a> v. 1.13.0</li></ul><h2 id="配置-kubeconfig"><a href="#配置-kubeconfig" class="headerlink" title="配置 kubeconfig"></a>配置 kubeconfig</h2><p>配置本地树莓派（并非master），以便使用kubectl与上一个教程中配置的集群进行通信。使用scp从master节点上下载配置文件</p><pre><code class="bash">$ scp pi@edge-master:/home/pi/.kube/config ./config</code></pre><p>将配置文件拷贝到本地机器上的 ~/.kube 目录下.</p><p>注意：如果你已经有一个集群的配置，它将会覆盖之前的配置，为了避免这种情况，可以在复制时添加  — kubeconfig.</p><pre><code class="b">$ cp config ~/.kube/config</code></pre><p>测试:</p><pre><code class="bash">$ kubectl get nodesNAME          STATUS   ROLES    AGE     VERSIONedge-master   Ready    master   7d23h   v1.13.1edge-node1    Ready    &lt;none&gt;   7d23h   v1.13.1edge-node2    Ready    &lt;none&gt;   7d23h   v1.13.1edge-node3    Ready    &lt;none&gt;   6d      v1.13.1</code></pre><h2 id="创建仪表盘"><a href="#创建仪表盘" class="headerlink" title="创建仪表盘"></a>创建仪表盘</h2><p><a href="https://github.com/kubernetes/dashboard/blob/master/src/deploy/recommended/kubernetes-dashboard-arm.yaml" target="_blank" rel="noopener">Kubernetes source</a> 在 kube-system 命名空间下将会创建以下几种资源: secret, service account, role, rolebinding, deployment, and service.</p><pre><code class="bash">$ kubectl create -f  https://raw.githubusercontent.com/kubernetes/dashboard/72832429656c74c4c568ad5b7163fa9716c3e0ec/src/deploy/recommended/kubernetes-dashboard-arm.yamlsecret &quot;kubernetes-dashboard-certs&quot; createdserviceaccount &quot;kubernetes-dashboard&quot; createdrole &quot;kubernetes-dashboard-minimal&quot; createdrolebinding &quot;kubernetes-dashboard-minimal&quot; createddeployment &quot;kubernetes-dashboard&quot; createdservice &quot;kubernetes-dashboard&quot; created</code></pre><blockquote class="colorquote warning"><p><strong>坑</strong>：这里会出现一个问题就是Pod总是重启，后来查资料发现dashboard的版本低，这时需要手动修改一下yaml或者在线修改pods的镜像即可。</p><p>这里介绍几个技巧：</p><p><em>查看日志</em>：</p><pre><code class="bash">$ kubectl logs -p POD_NAME</code></pre><p><em>编辑 Pod</em>:</p><pre><code class="bash">$ kubectl edit pods POD_NAME -n NAMESPACE</code></pre><p><em>导出pod或者deployment配置</em>：</p><pre><code class="bash">$ kubectl get pods POD_NAME -n NAMESPACE_NAME -o yaml</code></pre></blockquote><p>之后，您将能够从本地机器启动代理以访问刚刚创建的服务。</p><pre><code class="bash">$ nohup kubectl proxy --address 0.0.0.0 --accept-hosts &#39;.*&#39; &amp;</code></pre><p>Dashboard  可以通过  <a href="https://172.16.3.17:32351" target="_blank" rel="noopener">https://172.16.3.17:32351</a>  访问仪表盘。</p><blockquote class="colorquote warning"><p><strong>注意</strong>：</p><ul><li><p>这里要使用https协议</p></li><li><p>由于kubernetes-dashboard服务使用了NodePort的方式，因此这里的端口号为NodePort，这里的IP为NodeIP，即该POd所在的Node地址。   </p></li></ul></blockquote><p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547536342389405.png" alt="2019-01-15 15-12-04屏幕截图.png"></p><p>这里我们选择跳过，可以出现下面所示的资源管理界面。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547536405708187.png" alt="2019-01-15 15-13-14屏幕截图.png"></p><p>但是，这种打开方式是安全的，但是集群管理是不允许操作的，这就是说这种打开方式是匿名或者游客模式。</p><p>接下来需要创建一个 service account 在 default namespace 并且创建一个 clusterrolebinding 对象允许我们的service account 来使用 dashboard.</p><pre><code class="bash">$ kubectl create serviceaccount dashboard -n defaultserviceaccount “dashboard” created$ kubectl create clusterrolebinding dashboard-admin -n default \  --clusterrole=cluster-admin \  --serviceaccount=default:dashboardclusterrolebinding &quot;dashboard-admin&quot; created</code></pre><blockquote><p><em>如果想了解更多有关集群角色和更细的配置可以参考</em>:<br><a href="https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles" target="_blank" rel="noopener">官方文档</a>. </p></blockquote><p>以下命令可以得到我们创建的 服务帐号的 token（令牌），我们将打印出的token值复制到登录对话框中。</p><pre><code class="bash">$ kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=&quot;{.secrets[0].name}&quot;) -o jsonpath=&quot;{.data.token}&quot; | base64 --decodeeyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC10b2tlbi01bmo5ayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1OGEzNzlmMC0xODk1LTExZTktYjBlMi1iODI3ZWJiMWY4NTgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQifQ.X6evSc9wDPPa6T9CLy40gDj8aUTjVMg1PhA5bQIhSTnvVeB8mG763y9j8c0G3wOv4gCk1egziTIenpFx0w04P2zUTcqrHdse51vnbE5TNETQo8EiY2ELsqUJuaGd-O3Z6nmL8psBk4CmloPCMgYaBXPWiHPeS9dyOgTH-KxFoEEAuCX1i3BWPkYN_faN-sQe7zlrhu27lPJVUey8HGVjPu_6zGxMSWcZu2Wz3Euc1A-Rg6tekDhnxhxH_dcMRF38jSVY_z9r7mfvw6dJmxXTlH-KNzagruulH2l-Pg9obpz7HO7t14JB6c1F6p5Qa4zk2y9vOz4qCPk6IM7_ZfTwCw</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在最近关于在树莓派集群上使用kubeadm配置安装Kubernetes 1.13.0的教程中，默认情况下启用了RBAC，本文章将介绍如何在启用RBAC的情况下运行Kubernetes仪表板。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes DashBoard" scheme="https://readailib.com/categories/Kubernetes-DashBoard/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dashboard" scheme="https://readailib.com/tags/Dashboard/"/>
    
  </entry>
  
  <entry>
    <title>在树莓派上建立kubernetes集群</title>
    <link href="https://readailib.com/2019/01/07/kubernetes/raspberrypi/build-a-kubernetes-cluster/"/>
    <id>https://readailib.com/2019/01/07/kubernetes/raspberrypi/build-a-kubernetes-cluster/</id>
    <published>2019-01-07T07:37:08.000Z</published>
    <updated>2019-03-07T08:42:31.713Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901161547625729333503.jpg" alt="IMG_0468.jpg"></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>最近学了Kubernetes一段时间后，突然想在自己的树莓派上玩玩，搭建一个集群出来，玩过树莓派的同学都知道，树莓派作为一个“卡片电脑式”的嵌入式电脑，它的性能是非常有限的，内存和CPU对于我们传统的机器智能是远远不可及的，因此我在这里PO出我为什么要做树莓派的kubernetes研究：</p><a id="more"></a><p><em>打造一个具有边缘智能的系统，可以在上面部署边缘级别的微服务组件，包括传感器的采集服务、控制和计算单元为核心的服务！</em></p><p>这里的实验环境为 1 个kubernetes master和 3 个 worker节点，共计 4 个树莓派节点。为了方便，这里的主机名以及IP地址如下：</p><ul><li>edge-master, IP:172.16.3.1</li><li>edge-node1, IP:172.16.3.17</li><li>edge-node2, IP:172.16.3.32</li><li>edge-node3, IP:172.16.3.3</li></ul><blockquote><p>这里我没有使用静态IP，因此IP地址是有可能变的，因此为了稳定，建议设置成静态IP。但为了教程的完整性，我也会说明如何设置静态IP。</p></blockquote><p>这里主要参考了下面的教程：</p><ul><li><a href="https://gist.github.com/aaronkjones/d996f1a441bc80875fd4929866ca65ad" target="_blank" rel="noopener">https://gist.github.com/aaronkjones/d996f1a441bc80875fd4929866ca65ad</a></li><li><a href="https://github.com/alexellis/k8s-on-raspbian/blob/master/GUIDE.md" target="_blank" rel="noopener">https://github.com/alexellis/k8s-on-raspbian/blob/master/GUIDE.md</a></li></ul><h2 id="系统准备-TL-DR"><a href="#系统准备-TL-DR" class="headerlink" title="系统准备(TL;DR)"></a>系统准备(TL;DR)</h2><p>这里的准备系统是要准备树莓派操作系统，一般来讲运行kubernetes这么大的系统，最好的系统选择方式为轻量级的操作系统，这里推荐两个操作系统：</p><ul><li>Raspbian Stretch Lite - <a href="https://www.raspberrypi.org/downloads/raspbian/" target="_blank" rel="noopener">https://www.raspberrypi.org/downloads/raspbian/</a></li><li>Hypriot OS - <a href="https://blog.hypriot.com/" target="_blank" rel="noopener">https://blog.hypriot.com/</a></li></ul><p>这里为了方便，我使用了 Raspbian Stretch Lite 系统作为我的边缘设备操作系统，下载地址： <a href="https://www.raspberrypi.org/downloads/raspbian/" target="_blank" rel="noopener">https://www.raspberrypi.org/downloads/raspbian/</a></p><p>树莓派系统安装都是一个样，安装教程可以参考：<a href="https://www.shenhengheng.xyz/files/respberry_doc.pdf" target="_blank" rel="noopener">https://www.shenhengheng.xyz/files/respberry_doc.pdf</a></p><p>安装完成之后，需要初始化下面的操作，包括更改主机名，密码，连接wifi，设置docker网络代理，还有设置静态IP等。</p><h3 id="初始化系统"><a href="#初始化系统" class="headerlink" title="初始化系统"></a>初始化系统</h3><ul><li>更改主机名，修改密码以及连接Wi-Fi等工作都可以通过 raspi-config 命令来完成。</li></ul><h2 id="Master节点设置"><a href="#Master节点设置" class="headerlink" title="Master节点设置"></a>Master节点设置</h2><h3 id="设置静态IP"><a href="#设置静态IP" class="headerlink" title="设置静态IP"></a>设置静态IP</h3><pre><code class="bash">$ cat &lt;&lt; EOF &gt;&gt; /etc/dhcpcd.confprofile static_eth0static ip_address=192.168.0.100/24static routers=192.168.0.1static domain_name_servers=8.8.8.8EOF</code></pre><blockquote><p>安装100，101，102，103 的格式设置其他的树莓派节点IP地址</p></blockquote><h3 id="设置网络代理"><a href="#设置网络代理" class="headerlink" title="设置网络代理"></a>设置网络代理</h3><p>因为后期会需要下载kubeadm,kubelete，docker等软件，因此需要设置网络代理，通过代理下载软件。</p><pre><code class="bash">$ export http_proxy=&quot;http://172.14.1.54:1080&quot;$ export https_proxy=&quot;http://172.14.1.54:1080&quot;</code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><p>这个步骤依赖于上面的网络代理设置的，如果设置成功了，那么这步才可能成功执行。</p><pre><code class="bash">$ curl -sSL get.docker.com | sh &amp;&amp; sudo usermod pi -aG docker$ newgrp docker</code></pre><h3 id="关闭swap"><a href="#关闭swap" class="headerlink" title="关闭swap"></a>关闭swap</h3><pre><code class="bash">$ sudo dphys-swapfile swapoff &amp;&amp; \  sudo dphys-swapfile uninstall &amp;&amp; \  sudo update-rc.d dphys-swapfile remove</code></pre><p>这个步骤是为了后面的 kubeadm init 成功执行！具体的缘由可以参考我之前的文章：<a href="https://www.41sh.cn/?id=8" target="_blank" rel="noopener">https://www.41sh.cn/?id=8</a></p><p>为了检测是否成功关闭，可以执行下面的命令，如果成功执行了，那么下面的命令将不会有任何的输出。</p><pre><code class="bash">$ sudo swapon --summary</code></pre><h3 id="编辑-boot-cmdline-txt"><a href="#编辑-boot-cmdline-txt" class="headerlink" title="编辑 /boot/cmdline.txt"></a>编辑 /boot/cmdline.txt</h3><p>打开 /boot/cmdline.txt，并在末尾添加如下指令。</p><pre><code class="bash">cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code></pre><blockquote><p>该步骤执行完成后，一定要重启！否则后面会有错误。</p></blockquote><h3 id="安装kubernetes"><a href="#安装kubernetes" class="headerlink" title="安装kubernetes"></a>安装kubernetes</h3><p>该步骤也同样依赖于设置网络代理那一步骤！</p><pre><code class="bash">$ sudo su # 切换到root用户$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &amp;&amp; \echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee /etc/apt/sources.list.d/kubernetes.list &amp;&amp; \apt-get update -q &amp;&amp; \apt-get install -qy kubeadm=1.10.2-00 kubectl=1.10.2-00 kubelet=1.10.2-00</code></pre><blockquote class="colorquote warning"><p>1). 这里有个<strong>小坑</strong>，切到root用户后，需要重新配置网络代理，然后就可以使用了。</p><pre><code class="bash">$ <span class="built_in">export</span> http_proxy=<span class="string">"http://172.14.1.54:1080"</span>$ <span class="built_in">export</span> https_proxy=<span class="string">"http://172.14.1.54:1080"</span></code></pre><p>2). 使用root时，不用加sudo前缀，这个已经踩了很多次了</p></blockquote><p>如果安装最新版本，不用指定版本。可以直接使用下面的命令：</p><pre><code class="bash">$ apt-get install -qy kubeadm kubectl kubelet</code></pre><h3 id="修改docker的网络代理"><a href="#修改docker的网络代理" class="headerlink" title="修改docker的网络代理"></a>修改docker的网络代理</h3><p>这一步非常重要！因为我们可能需要pull很多国内不能访问的镜像，因此需要设置docker网络代理，因为默认情况下，docker服务不是通过systemctl管理的，因此需要创建一个系统服务，具体的命令如下：</p><pre><code class="bash">$ mkdir -pv /etc/systemd/system/docker.service.d$ cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=&quot;HTTP_PROXY=http://172.16.3.12:1080/&quot; &quot;HTTPS_PROXY=http://172.16.3.12:1080/&quot;EOF $ systemctl daemon-reload$ systemctl restart docker</code></pre><blockquote><p><strong>参考：</strong><a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/</a></p></blockquote><h3 id="预先pull镜像"><a href="#预先pull镜像" class="headerlink" title="预先pull镜像"></a>预先pull镜像</h3><pre><code class="bash">$ kubeadm config images pull -v3</code></pre><h3 id="初始化master节点"><a href="#初始化master节点" class="headerlink" title="初始化master节点"></a>初始化master节点</h3><pre><code class="bash">$ kubeadm init --token-ttl=0 --pod-network-cidr=10.244.0.0/16</code></pre><p>这步如果成功的话，会打印下面的消息： </p><pre><code class="bash">Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:  kubeadm join 172.16.3.1:6443 --token xo78oj.02cia85vdh285aqj --discovery-token-ca-cert-hash sha256:9517c72036f8261ac912adaf8339b65583fdaa7dbb8dd60054c6e84e8880a3fd</code></pre><p> 然后根据输出的提示，执行下面的语句：</p><pre><code class="bash">$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><blockquote><p>上面的语句需要在pi用户下执行！否则不管用！</p></blockquote><p>这步完成之后，执行下面的命令，你会发现master节点NotReady的消息，这是因为我们的网络需要依赖于flannel网络组件！</p><blockquote class="colorquote info"><h2 id="Setup-networking-with-Weave-Net-or-Flannel"><a href="#Setup-networking-with-Weave-Net-or-Flannel" class="headerlink" title="Setup networking with Weave Net or Flannel"></a>Setup networking with Weave Net or Flannel</h2><p>Some users have reported stability issues with Weave Net on ARMHF. These issues do not appear to affect x86_64 (regular PCs/VMs). You may want to try Flannel instead of Weave Net for your RPi cluster.</p><h3 id="Weave-Net"><a href="#Weave-Net" class="headerlink" title="Weave Net"></a>Weave Net</h3><p>Install <a href="https://www.weave.works/oss/net/" target="_blank" rel="noopener">Weave Net</a> network driver</p><pre><code class="bash">$ kubectl apply -f <span class="string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="variable">$(kubectl version | base64 | tr -d '\n')</span>"</span></code></pre><p>If you run into any issues with Weaveworks’ networking then <a href="https://github.com/coreos/flannel" target="_blank" rel="noopener">flannel</a> is also a popular choice for the ARM platform.</p><h3 id="Flannel-alternative"><a href="#Flannel-alternative" class="headerlink" title="Flannel (alternative)"></a>Flannel (alternative)</h3><p>Apply the Flannel driver on the master:</p><pre><code class="bash">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/c5d10c8/Documentation/kube-flannel.yml</code></pre><p>On each node that joins including the master:</p><pre><code class="bash">$ sudo sysctl net.bridge.bridge-nf-call-iptables=1</code></pre></blockquote><h2 id="其他节点设置"><a href="#其他节点设置" class="headerlink" title="其他节点设置"></a>其他节点设置</h2><p>其他节点和master类似，最后需要执行下面的命令就可以了！</p><pre><code class="bash">$ kubeadm join 172.16.3.1:6443 --token xo78oj.02cia85vdh285aqj --discovery-token-ca-cert-hash sha256:9517c72036f8261ac912adaf8339b65583fdaa7dbb8dd60054c6e84e8880a3fd</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://www.41sh.cn/zb_users/upload/2019/01/201901161547625729333503.jpg&quot; alt=&quot;IMG_0468.jpg&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;最近学了Kubernetes一段时间后，突然想在自己的树莓派上玩玩，搭建一个集群出来，玩过树莓派的同学都知道，树莓派作为一个“卡片电脑式”的嵌入式电脑，它的性能是非常有限的，内存和CPU对于我们传统的机器智能是远远不可及的，因此我在这里PO出我为什么要做树莓派的kubernetes研究：&lt;/p&gt;
    
    </summary>
    
      <category term="RPI Kubernetes" scheme="https://readailib.com/categories/RPI-Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
  </entry>
  
</feed>
