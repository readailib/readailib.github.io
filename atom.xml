<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShenH.&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/543fc8a83d9b480e5f69c3842db96518</icon>
  <subtitle>Learn Anything, Anytime, Anywhere~</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://readailib.github.io/"/>
  <updated>2019-03-05T14:37:50.224Z</updated>
  <id>https://readailib.github.io/</id>
  
  <author>
    <name>ShenHengheng</name>
    <email>shenhengheng17g@ict.ac.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>了解與實作 Kubernetes Leader Election 功能</title>
    <link href="https://readailib.github.io/2019/02/05/kubernetes/k8s-leader-election/"/>
    <id>https://readailib.github.io/2019/02/05/kubernetes/k8s-leader-election/</id>
    <published>2019-02-05T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.224Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Golang" scheme="https://readailib.github.io/tags/Golang/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>利用 Minikube 快速建立測試用 Kubernetes 叢集</title>
    <link href="https://readailib.github.io/2019/01/22/kubernetes/deploy/minikube-multi-node/"/>
    <id>https://readailib.github.io/2019/01/22/kubernetes/deploy/minikube-multi-node/</id>
    <published>2019-01-22T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.223Z</updated>
    
    <content type="html"><![CDATA[<p>本文將說明如何透過 Minikube 建立多節點 Kubernetes 叢集。一般來說 Minikube 僅提供單節點功能，即透過虛擬機建立僅有一個具備 Master/Node 節點的 Kubernetes 叢集，但由時候需要測試多節點功能，因此自己改了一下 Minikube 來支援最新版本(v1.13.2)的多節點部署，且 CNI Plugin 採用 Calico，以方便測試 Network Policy 功能。</p><p><img src="/images/kube/minikube-logo.jpg" alt></p><a id="more"></a><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始部署叢集前需先確保以下條件已達成：</p><ul><li>在測試機器下載 Minikube 二進制執行檔：<ul><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-linux-amd64" target="_blank" rel="noopener">Linux</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-darwin-amd64" target="_blank" rel="noopener">Mac OS X</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-windows-amd64.exe" target="_blank" rel="noopener">Windows</a></li></ul></li></ul><blockquote class="colorquote warning"><p>如果上面連結掛了，可以透過以下方式安裝：</p><pre><code class="bash">$ git <span class="built_in">clone</span> https://github.com/kairen/minikube.git -b multi-node <span class="variable">$GOPATH</span>/src/k8s.io/minikube$ <span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/k8s.io/minikube$ make</code></pre></blockquote><ul><li>在測試機器下載 <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">Virtual Box</a> 來提供給 Minikube 建立虛擬機。</li></ul><blockquote class="colorquote warning"><ul><li><strong>IMPORTANT</strong>: 測試機器記得開啟 VT-x or AMD-v virtualization.</li><li>雖然建議用 VBox，但是討厭 Oracle 的人可以改用其他虛擬化工具(ex: KVM, xhyve)，理論上可以動。</li></ul></blockquote><ul><li>下載所屬作業系統的 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubeclt</a>。</li></ul><blockquote class="colorquote info"><ul><li>目前已測試過 Ubuntu 16.04 Desktop、Mac OS X 與 Windows 10 作業系統。</li><li>Windows 使用者建議用 git bash 來操作。</li></ul></blockquote><h2 id="建立叢集"><a href="#建立叢集" class="headerlink" title="建立叢集"></a>建立叢集</h2><p>本節將說明如何建立 Master 與 Node 節點，並將這些節點組成一個叢集。</p><p>在開始前確認之前是否已經裝過 Minikube，若有的話，就把上面下載二進制檔放任意方便你執行的位置，或者直接取代之前的，然後再開始前請先刪除 Home 目錄的<code>.minikube</code>資料夾：</p><pre><code class="bash">$ rm -rf $HOME/.minikube</code></pre><h3 id="Master-節點"><a href="#Master-節點" class="headerlink" title="Master 節點"></a>Master 節點</h3><p>首先透過 Minikube 執行以下指令來啟動 Master 節點，並透過 kubectl 檢查：</p><pre><code class="bash">$ minikube --profile k8s-m1 start --network-plugin=cni...Everything looks great. Please enjoy minikube!$ kubectl -n kube-system get po -o wideNAME                             READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATEScalico-node-8cbc6                2/2     Running   0          6m29s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-4nzlx         1/1     Running   0          6m32s   10.244.0.3       k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-9879v         1/1     Running   0          6m32s   10.244.0.2       k8s-m1   &lt;none&gt;           &lt;none&gt;etcd-k8s-m1                      1/1     Running   0          5m58s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-m1        1/1     Running   0          5m47s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-apiserver-k8s-m1            1/1     Running   0          5m43s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-controller-manager-k8s-m1   1/1     Running   0          5m47s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-proxy-qnq25                 1/1     Running   0          6m32s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-scheduler-k8s-m1            1/1     Running   0          5m59s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;storage-provisioner              1/1     Running   0          6m30s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;</code></pre><blockquote class="colorquote warning"><ul><li><code>--vm-driver</code> 可以選擇使用其他 VM driver 來啟動虛擬機，如 xhyve、hyperv、hyperkit 與 kvm2 等等。</li></ul></blockquote><p>完成後，確認 k8s-m1 節點處於 Ready 狀態：</p><pre><code class="bash">$ kubectl get noNAME     STATUS   ROLES    AGE    VERSIONk8s-m1   Ready    master   2m8s   v1.13.2</code></pre><h3 id="Node-節點"><a href="#Node-節點" class="headerlink" title="Node 節點"></a>Node 節點</h3><p>確認 Master 完成後，這邊接著透過 Minikube 開啟新的節點來加入：</p><pre><code class="bash">$ minikube --profile k8s-n1 start --network-plugin=cni --node...Stopping extra container runtimes...# 接著取得 Master IP 與 Token$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 執行以下指令進入 k8s-n1$ minikube --profile k8s-n1 ssh# 這邊為進入 k8s-n1 VM 內執行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下結果後，即可以在 k8s-m1 context 來操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><blockquote class="colorquote warning"><ul><li>另外上面的 IP 有可能會不同，請確認 Master 節點 IP。</li><li>其他節點以此類推。</li></ul></blockquote><p>完成後，透過 kubectl 檢查 Node 是否有加入叢集：</p><pre><code class="bash">$ kubectl config use-context k8s-m1Switched to context &quot;k8s-m1&quot;.$ kubectl get noNAME     STATUS   ROLES    AGE     VERSIONk8s-m1   Ready    master   3m44s   v1.13.2k8s-n1   Ready    &lt;none&gt;   80s     v1.13.2$ kubectl get csrNAME                                                   AGE    REQUESTOR                 CONDITIONnode-csr-Ut1k5mLXpXVsyZwjn2z2-fpie9HHyTkMU7wnrjDnD3E   118s   system:bootstrap:3qeeeu   Approved,Issued$ kubectl -n kube-system get po -o wideNAME                             READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATEScalico-node-qxkw5                2/2     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;calico-node-srhlk                2/2     Running   0          3m24s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-826nz         1/1     Running   0          3m27s   10.244.0.3       k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-9z7mr         1/1     Running   0          3m27s   10.244.0.2       k8s-m1   &lt;none&gt;           &lt;none&gt;etcd-k8s-m1                      1/1     Running   0          2m40s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-m1        1/1     Running   0          3m48s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-n1        1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-apiserver-k8s-m1            1/1     Running   0          2m36s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-controller-manager-k8s-m1   1/1     Running   0          2m50s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-proxy-768w8                 1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-proxy-b7ndj                 1/1     Running   0          3m27s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-scheduler-k8s-m1            1/1     Running   0          2m46s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;storage-provisioner              1/1     Running   0          3m17s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;</code></pre><p>這樣一個 Kubernetes 叢集就完成了，速度快一點不到 10 分鐘就可以建立好了。</p><h2 id="刪除虛擬機與檔案"><a href="#刪除虛擬機與檔案" class="headerlink" title="刪除虛擬機與檔案"></a>刪除虛擬機與檔案</h2><p>最後若想清除環境的話，直接刪除虛擬機即可：</p><pre><code class="bash">$ minikube --profile &lt;node_name&gt; delete</code></pre><p>而檔案只要刪除 Home 目錄的<code>.minikube</code>資料夾，以及<code>minikube</code>執行檔即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文將說明如何透過 Minikube 建立多節點 Kubernetes 叢集。一般來說 Minikube 僅提供單節點功能，即透過虛擬機建立僅有一個具備 Master/Node 節點的 Kubernetes 叢集，但由時候需要測試多節點功能，因此自己改了一下 Minikube 來支援最新版本(v1.13.2)的多節點部署，且 CNI Plugin 採用 Calico，以方便測試 Network Policy 功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/kube/minikube-logo.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Calico" scheme="https://readailib.github.io/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集</title>
    <link href="https://readailib.github.io/2018/08/12/kubernetes/deploy/k8s-ansible-ha/"/>
    <id>https://readailib.github.io/2018/08/12/kubernetes/deploy/k8s-ansible-ha/</id>
    <published>2018-08-12T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.219Z</updated>
    
    <content type="html"><![CDATA[<p>本篇將介紹如何透過 Ansible Playbooks 來快速部署多節點 Kubernetes，一般自建 Kubernetes 叢集時，很多初步入門都會透過 kubeadm 或腳本來部署，雖然 kubeadm 簡化了很多流程，但是還是需要很多手動操作過程，這使得當節點超過 5 - 8 台時就覺得很麻煩，因此許多人會撰寫腳本來解決這個問題，但是腳本的靈活性不佳，一旦設定過程過於龐大時也會造成其複雜性增加，因此這邊採用 Ansible 來完成許多重複的部署過程，並提供相關變數來調整叢集部署的元件、Container Runtime 等等。</p><p>這邊我將利用自己撰寫的 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 來部署一組 Kubernetes HA 叢集，而該 Playbooks 的 HA 是透過 HAProxy + Keepalived 來完成，這邊也會將 docker 取代成 containerd 來提供更輕量的 container runtime，另外該 Ansible 會採用全二進制檔案(kube-apiserver 等除外)方式進行安裝。</p><p>本次 Kubernetes 安裝版本：</p><ul><li>Kubernetes v1.11.2</li><li>Etcd v3.2.9</li><li>containerd v1.1.2</li></ul><a id="more"></a><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16+</code>，測試環境為實體主機：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.22.132.8</td><td>VIP</td><td></td><td></td></tr><tr><td>172.22.132.9</td><td>k8s-m1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.10</td><td>k8s-m2</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.11</td><td>k8s-m3</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.12</td><td>k8s-g1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.13</td><td>k8s-g2</td><td>4</td><td>16G</td></tr></tbody></table><blockquote><p>理論上<code>CentOS 7.x</code>或<code>Debian 8</code>都可以。</p></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認以下幾個項目：</p><ul><li>所有節點的網路之間可以互相溝通。</li><li><code>部署節點</code>對其他節點不需要 SSH 密碼即可登入。</li><li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li><li>所有節點需要安裝 <code>Python</code>。</li><li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li><li><code>部署節點</code>需要安裝 Ansible。</li></ul><p>Ubuntu 16.04 安裝 Ansible:</p><pre><code class="sh">$ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible</code></pre><p>CentOS 7 安裝 Ansible：</p><pre><code class="sh">$ sudo yum install -y epel-release$ sudo yum -y install ansible cowsay</code></pre><p>Mac OS X 安裝 Ansible:</p><pre><code class="sh">$ brew install ansible</code></pre><h2 id="透過-Ansible-部署-Kubernetes"><a href="#透過-Ansible-部署-Kubernetes" class="headerlink" title="透過 Ansible 部署 Kubernetes"></a>透過 Ansible 部署 Kubernetes</h2><p>本節將說明如何使用 Ansible 來部署 Kubernetes HA 叢集，首先我們透過 Git 取得專案:</p><pre><code class="sh">$ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible</code></pre><h3 id="Kubernetes-叢集"><a href="#Kubernetes-叢集" class="headerlink" title="Kubernetes 叢集"></a>Kubernetes 叢集</h3><p>首先建立一個檔案<code>inventory/hosts.ini</code>來描述被部署的節點與群組關析：</p><pre><code>[etcds]k8s-m[1:3] ansible_user=ubuntu[masters]k8s-m[1:3] ansible_user=ubuntu[nodes]k8s-g1 ansible_user=ubuntuk8s-g2 ansible_user=ubuntu[kube-cluster:children]mastersnodes</code></pre><blockquote><p><code>ansible_user</code>為作業系統 SSH 的使用者名稱。</p></blockquote><p>接著編輯<code>group_vars/all.yml</code>來根據需求設定功能，如以下範例：</p><pre><code class="yaml">kube_version: 1.11.2container_runtime: containerdcni_enable: truecontainer_network: calicocni_iface: &quot;&quot; # CNI 網路綁定的網卡vip_interface: &quot;&quot; # VIP 綁定的網卡vip_address: 172.22.132.8 # VIP 位址etcd_iface: &quot;&quot; # etcd 綁定的網卡enable_ingress: trueenable_dashboard: trueenable_logging: trueenable_monitoring: trueenable_metric_server: truegrafana_user: &quot;admin&quot;grafana_password: &quot;p@ssw0rd&quot;</code></pre><blockquote><p>上面綁定網卡若沒有輸入，通常會使用節點預設網卡(一般來說是第一張網卡)。</p></blockquote><p>完成設定<code>group_vars/all.yml</code>檔案後，就可以先透過 Ansible 來檢查叢集狀態：</p><pre><code class="sh">$ ansible -i inventory/hosts.ini all -m pingk8s-g1 | SUCCESS =&gt; {    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}...</code></pre><p>當叢集確認沒有問題後，即可執行<code>cluster.yml</code>來部署 Kubernetes 叢集：</p><pre><code class="sh">$ ansible-playbook -i inventory/hosts.ini cluster.yml...PLAY RECAP ***********************************************************************************************************************k8s-g1                     : ok=64   changed=32   unreachable=0    failed=0k8s-g2                     : ok=62   changed=32   unreachable=0    failed=0k8s-m1                     : ok=171  changed=85   unreachable=0    failed=0k8s-m2                     : ok=144  changed=69   unreachable=0    failed=0k8s-m3                     : ok=144  changed=69   unreachable=0    failed=0</code></pre><blockquote><p>確認都沒發生錯誤後，表示部署完成。</p></blockquote><p>這邊選擇一台 master 節點(<code>k8s-m1</code>)來 SSH 進入測試叢集是否正常，透過 kubectl 指令來查看：</p><pre><code class="sh"># 查看元件狀態$ kubectl get csNAME                 STATUS    MESSAGE              ERRORcontroller-manager   Healthy   okscheduler            Healthy   oketcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}# 查看節點狀態$ kubectl get noNAME      STATUS    ROLES     AGE       VERSIONk8s-g1    Ready     &lt;none&gt;    3m        v1.11.2k8s-g2    Ready     &lt;none&gt;    3m        v1.11.2k8s-m1    Ready     master    5m        v1.11.2k8s-m2    Ready     master    5m        v1.11.2k8s-m3    Ready     master    5m        v1.11.2</code></pre><h3 id="Addons-部署"><a href="#Addons-部署" class="headerlink" title="Addons 部署"></a>Addons 部署</h3><p>確認節點沒問題後，就可以透過<code>addons.yml</code>來部署 Kubernetes extra addons：</p><pre><code class="sh">$ ansible-playbook -i inventory/hosts.ini addons.yml...PLAY RECAP ***********************************************************************************************************************k8s-m1                     : ok=27   changed=22   unreachable=0    failed=0k8s-m2                     : ok=10   changed=5    unreachable=0    failed=0k8s-m3                     : ok=10   changed=5    unreachable=0    failed=0</code></pre><p>完成後即可透過 kubectl 來檢查服務，如 kubernetes-dashboard：</p><pre><code class="sh">$ kubectl get po,svc -n kube-system -l k8s-app=kubernetes-dashboardNAME                                       READY     STATUS    RESTARTS   AGEpod/kubernetes-dashboard-6948bdb78-bkqbr   1/1       Running   0          32mNAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGEservice/kubernetes-dashboard   ClusterIP   10.105.199.72   &lt;none&gt;        443/TCP   32m</code></pre><p>完成後，即可透過 API Server 的 Proxy 來存取 <a href="https://172.22.132.8:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/。" target="_blank" rel="noopener">https://172.22.132.8:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/。</a></p><p><img src="https://i.imgur.com/G3g4LLo.png" alt></p><h2 id="測試是否有-HA"><a href="#測試是否有-HA" class="headerlink" title="測試是否有 HA"></a>測試是否有 HA</h2><p>首先透過 etcdctl 來檢查狀態：</p><pre><code class="sh">$ export PKI=&quot;/etc/kubernetes/pki/etcd&quot;$ ETCDCTL_API=3 etcdctl \    --cacert=${PKI}/etcd-ca.pem \    --cert=${PKI}/etcd.pem \    --key=${PKI}/etcd-key.pem \    --endpoints=&quot;https://172.22.132.9:2379&quot; \    member listc9c9f1e905ce83ae, started, k8s-m1, https://172.22.132.9:2380, https://172.22.132.9:2379cb81b1446a3a689f, started, k8s-m3, https://172.22.132.11:2380, https://172.22.132.11:2379db0b2674ebb24f80, started, k8s-m2, https://172.22.132.10:2380, https://172.22.132.10:2379</code></pre><p>接著進入<code>k8s-m1</code>節點測試叢集 HA 功能，這邊先關閉該節點：</p><pre><code class="sh">$ sudo poweroff</code></pre><p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p><pre><code class="sh"># 先檢查元件狀態$ kubectl get csNAME                 STATUS      MESSAGE                                                                                                                                          ERRORcontroller-manager   Healthy     okscheduler            Healthy     oketcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-0               Unhealthy   Get https://172.22.132.9:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)# 檢查 nodes 狀態$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONk8s-g1    Ready      &lt;none&gt;    10m       v1.11.2k8s-g2    Ready      &lt;none&gt;    10m       v1.11.2k8s-m1    NotReady   master    12m       v1.11.2k8s-m2    Ready      master    12m       v1.11.2k8s-m3    Ready      master    12m       v1.11.2# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl expose pod nginx --port 80 --type NodePort$ kubectl get po,svcNAME        READY     STATUS    RESTARTS   AGEpod/nginx   1/1       Running   0          1mNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGEservice/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        3hservice/nginx        NodePort    10.102.191.102   &lt;none&gt;        80:31780/TCP   6s</code></pre><p>透過 cURL 檢查 NGINX 服務是否正常：</p><pre><code class="sh">$ curl 172.22.132.8:31780&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...</code></pre><h2 id="重置叢集"><a href="#重置叢集" class="headerlink" title="重置叢集"></a>重置叢集</h2><p>最後若想要重新部署叢集的話，可以透過<code>reset-cluster.yml</code>來清除叢集：</p><pre><code class="sh">$ ansible-playbook -i inventory/hosts.ini reset-cluster.yml</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇將介紹如何透過 Ansible Playbooks 來快速部署多節點 Kubernetes，一般自建 Kubernetes 叢集時，很多初步入門都會透過 kubeadm 或腳本來部署，雖然 kubeadm 簡化了很多流程，但是還是需要很多手動操作過程，這使得當節點超過 5 - 8 台時就覺得很麻煩，因此許多人會撰寫腳本來解決這個問題，但是腳本的靈活性不佳，一旦設定過程過於龐大時也會造成其複雜性增加，因此這邊採用 Ansible 來完成許多重複的部署過程，並提供相關變數來調整叢集部署的元件、Container Runtime 等等。&lt;/p&gt;
&lt;p&gt;這邊我將利用自己撰寫的 &lt;a href=&quot;https://github.com/kairen/kube-ansible&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-ansible&lt;/a&gt; 來部署一組 Kubernetes HA 叢集，而該 Playbooks 的 HA 是透過 HAProxy + Keepalived 來完成，這邊也會將 docker 取代成 containerd 來提供更輕量的 container runtime，另外該 Ansible 會採用全二進制檔案(kube-apiserver 等除外)方式進行安裝。&lt;/p&gt;
&lt;p&gt;本次 Kubernetes 安裝版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.11.2&lt;/li&gt;
&lt;li&gt;Etcd v3.2.9&lt;/li&gt;
&lt;li&gt;containerd v1.1.2&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Ansible" scheme="https://readailib.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>初探 Knative 基本功能與概念</title>
    <link href="https://readailib.github.io/2018/07/27/kubernetes/knative-intro/"/>
    <id>https://readailib.github.io/2018/07/27/kubernetes/knative-intro/</id>
    <published>2018-07-27T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.225Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/kube/knative-logo.png" alt></p><p><a href="https://github.com/knative" target="_blank" rel="noopener">Knative</a> 是基於 Kubernetes 平台建構、部署與管理現代 Serverless 工作負載的開源專案，其目標是要幫助雲端供應商與企業平臺營運商替任何雲端環境的開發者、操作者等提供 Serverless 服務體驗。Knative 採用了 Kubernetes 概念來建構函式與應用程式，並以 Istio 實現了叢集內的網路路由，以及進入服務的外部連接，這讓開發者在部署或執行變得更加簡單。而目前 Knative 元件焦距在解決許多平凡但困難的事情，例如以下：</p><ul><li>部署一個容器。</li><li>在 Kubernetes 上編排 Source-to-URL 的工作流程。</li><li>使用 Blue/Green 部署來路由與管理流量。</li><li>按需自動擴展與調整工作負載的大小。</li><li>將運行服務(Running services)綁定到事件生態系統(Eventing ecosystems)。</li><li>利用原始碼建構應用程式與函式。</li><li>讓應用程式能夠零停機升級。</li><li>自動增減應用程式與函式實例。</li><li>透過 HTTP request 觸發函式的呼叫。</li><li>為函式、應用程式與容器建立事件。</li></ul><p>而 Knative 的設計考慮了不同的工作角色使用情境：</p><p><img src="/images/kube/knative-audience.png" alt></p><p>然而 Knative 不只使用 Kubernetes 與 Istio 的功能，也自行開發了三個元件以提供更完整的 Serverless 平台。而下節將針對這三個元件進行說明。</p><h2 id="Knative-元件與概念"><a href="#Knative-元件與概念" class="headerlink" title="Knative 元件與概念"></a>Knative 元件與概念</h2><p>目前 Knative 提供了以下幾個元件來處理不同的功能需求，本節我們將針對這些元件進行說明。</p><h3 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h3><p><a href="https://github.com/knative/build" target="_blank" rel="noopener">Build</a> 是 Knative 中的自定義資源，並提供了 Build API object 來處理從原始碼(Sources)建構容器的可插拔(Pluggable)模型，這是基於 Google 的容器建構服務(Container Build Service) 而來，這允許開發者定義容器的來源來打包，例如 Git、Registery(ex: Docker hub)，另外也能將 Buildpacks 當作一種建構的插件來使用，這使 Knative 在建構功能上有更靈活的擴展。</p><blockquote class="colorquote info"><p>除了 Buildpacks 外，也能夠將 Google Container Builder、Bazel、Kaniko 與 Jib 等等當作建構插件使用。</p></blockquote><p>而一個 Knative builds 的主要特性如下：</p><ul><li>一個<code>Build</code>可以包含多個 step，其中每個 step 會指定一個<code>Builder</code>。</li><li>一個<code>Builder</code>是一種容器映像檔，可以建立該映像檔來完成任何任務，如流程中的單一 step，或是整個流程本身。</li><li><code>Build</code> 中的 steps 可以推送(push)到一個儲存庫(repository)。</li><li>一個<code>BuildTemplate</code>可用在定義重用的模板。</li><li>可以定義<code>Build</code>中的<code>source</code>來將檔案或專案掛載到 Kubernetes Volume(會掛載成<code>/workspace</code>)。目前支援：<ul><li>Git 儲存庫</li><li>Google Cloud Storage</li><li>任意的容器映像檔</li></ul></li><li>利用 Kubernetes Secrets 結合 ServiceAccount 進行身份認證</li></ul><blockquote class="colorquote info"><p>這邊的<code>step</code>可以看作是 Kubernetes 的 init-container。</p></blockquote><p>以下是一個提供使用者身份認證的 Build 範例，該範例包含多個 step 與 Git repo：</p><pre><code class="yml">apiVersion: build.knative.dev/v1alpha1kind: Buildmetadata:  name: example-buildspec:  serviceAccountName: build-auth-example  source:    git:      url: https://github.com/example/build-example.git      revision: master  steps:  - name: ubuntu-example    image: ubuntu    args: [&quot;ubuntu-build-example&quot;, &quot;SECRETS-example.md&quot;]  steps:  - image: gcr.io/example-builders/build-example    args: [&#39;echo&#39;, &#39;hello-example&#39;, &#39;build&#39;]</code></pre><h3 id="Serving"><a href="#Serving" class="headerlink" title="Serving"></a>Serving</h3><p><a href="https://github.com/knative/serving" target="_blank" rel="noopener">Serving</a> 以 Kubernetes 與 Istio 為基礎，實現了中介軟體原語(Middleware Primitives)來達到自動化從容器到函式執行的整個流程，另外也支援了快速部署容器並進行伸縮的功能，甚至能根據請求來讓容器實例降到 0，而 Serving 也會利用 Istio 在修訂版本之間路由流量，或是將流量傳送到同一個應用程式的多個修訂版本中，除了上述功能外， Serving 也能實現了不停機更新、Bule/Green 部署、部分負載測試，以及程式碼回滾等功能。</p><p><img src="/images/kube/object_model.png" alt></p><p>從上圖，可以得知 Serving 利用了 Kubernetes CRD 新增一組 API 來定義與控制在 Kubernetes 上的 Serverless 的行為，其分別為以下：</p><ul><li><p><strong>Service</strong>：該資源用來自動管理整個工作負載的生命週期，並提供單點控制。它控制了其他物件的建立，以確保應用程式與函式具備每次 Service 更新的 Route、Configuration 與 Revision，而 Service 也可以定義流量路由到最新 Revision 或固定的 Revision。</p><pre><code class="yml">apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata:  name: service-examplespec:  runLatest:    configuration:      revisionTemplate:        spec:          container:            image: gcr.io/knative-samples/helloworld-go            env:            - name: TARGET              value: &quot;Go Sample v1&quot;   </code></pre></li><li><p><strong>Route</strong>：該資源將網路端點映射到一個或多個 revision，並且能透過多種方式來管理流量，如部分的流量(fractional traffic)、命名路由(named routes)。</p><pre><code class="yml">apiVersion: serving.knative.dev/v1alpha1kind: Routemetadata:  name: route-examplespec:  traffic:  - configurationName: stock-configuration-example    percent: 100</code></pre></li><li><p><strong>Configuration</strong>：該資源維護部屬所需的狀態，它提供了程式碼與組態檔之間的分離，並遵循 Twelve-Factor  App 方法，若修改 Configuration 會建立新 revision。</p><pre><code class="yml">apiVersion: serving.knative.dev/v1alpha1kind: Configurationmetadata: name: configuration-examplespec: revisionTemplate:   metadata:     labels:       knative.dev/type: container   spec:     container:       image: github.com/knative/docs/serving/samples/rest-api-go       env:         - name: RESOURCE           value: stock       readinessProbe:         httpGet:           path: /         initialDelaySeconds: 3         periodSeconds: 3</code></pre></li><li><p><strong>Revision</strong>：該資源是記錄每個工作負載修改的程式碼與組態的時間點快照，而 Revision 是不可變物件，並且只要它還有用處，就會被長時間保留。</p><pre><code class="yml">apiVersion: serving.knative.dev/v1alpha1kind: Revisionmetadata: labels:   serving.knative.dev/configuration: helloworld-go name: revision-example namespace: defaultspec: concurrencyModel: Multi container:   env:   - name: TARGET     value: Go Sample v1   image: gcr.io/knative-samples/helloworld-go generation: 1 servingState: Active</code></pre></li></ul><h3 id="Eventing"><a href="#Eventing" class="headerlink" title="Eventing"></a>Eventing</h3><p><a href="https://github.com/knative/eventing" target="_blank" rel="noopener">Eventing</a> 提供用於 Consuming 以及 Producing 的事件建構區塊，並遵守著 <a href="https://github.com/cloudevents" target="_blank" rel="noopener">CloudEvents</a> 規範來實現，而該元件目標是對事件進行抽象處理，以讓開發者不需要關注後端相關具體細節，這樣開發者就不需要思考使用哪一套訊息佇列系統。</p><p><img src="/images/kube/knative-event-arch.png" alt></p><p>而 Knative Eventing 也透過 Kubernetes CRD 定義了一組新資源，這些資源被用在事件的 Producing 與 Consuming 上，而這類資源主要分成以下：</p><ul><li><p><strong>Channels</strong></p><ul><li>這些是發布者(Publishers)向其發送訊息的 Pub/Sub Topics，因此 Channel 可視為獲取或放置事件的位置目錄。</li><li>Bus。Channels 的後端供應者，即支援事件的訊息服務平台，如 Google Cloud PubSub、Apache Kafka 與 NATS 等等。</li></ul><pre><code class="yml">apiVersion: channels.knative.dev/v1alpha1kind: Busmetadata:  name: kafkaspec:  dispatcher:    args:    - -logtostderr    - -stderrthreshold    - INFO    env:    - name: KAFKA_BROKERS      valueFrom:        configMapKeyRef:          key: KAFKA_BROKERS          name: kafka-bus-config    image: gcr.io/knative-releases/github.com/knative/eventing/pkg/buses/kafka/dispatcher@sha256:d925663bb965001287b042c8d3ebdf8b4d3f0e7aa2a9e1528ed39dc78978bcdb    name: dispatcher</code></pre><ul><li>為應用程式與函式指定 Knative Service，並指明 Channel 所要傳遞的具體訊息。為程式與函式的進入位址。</li></ul></li><li><p><strong>Feeds</strong>: 提供一個抽象層來讓外部可以提供資料來源，並將之路由到叢集中。會將事件來源中的單個事件類型附加到某一個行為。</p><ul><li>EventSource 與 ClusterEventSource 是一個 Kubernetes 資源，被用來描述可能產生的 EventTypes 外部系統。</li></ul><pre><code class="yml">apiVersion: feeds.knative.dev/v1alpha1kind: EventSourcemetadata:  name: github  namespace: defaultspec:  image: gcr.io/knative-releases/github.com/knative/eventing/pkg/sources/github@sha256:a5f6733797d934cd4ba83cf529f02ee83e42fa06fd0e7a9d868dd684056f5db0  source: github  type: github</code></pre><ul><li>EventType 與 ClusterEventType 同樣是 Kubernetes 資源，被用來表示不同 EventSource 支援的事件類型。</li></ul><pre><code class="yml">apiVersion: feeds.knative.dev/v1alpha1kind: EventTypemetadata:  name: pullrequest  namespace: defaultspec:  description: notifications on pullrequests  eventSource: github</code></pre></li><li><p><strong>Flows</strong>: 該資源會將事件綁定到 Route(應用程式與函式端點)上，並選擇使用哪種事件路由的 Channel 與 Bus。</p><pre><code class="yml">apiVersion: flows.knative.dev/v1alpha1kind: Flowmetadata: name: k8s-event-flow namespace: defaultspec: serviceAccountName: feed-sa trigger:   eventType: dev.knative.k8s.event   resource: k8sevents/dev.knative.k8s.event   service: k8sevents   parameters:     namespace: default action:   target:     kind: Route     apiVersion: serving.knative.dev/v1alpha1     name: read-k8s-events</code></pre></li></ul><p>以上是簡單介紹，接下來我們將透過 Minikube 來初步玩玩 Knative 功能。</p><h2 id="透過-Minikube-初步入門"><a href="#透過-Minikube-初步入門" class="headerlink" title="透過 Minikube 初步入門"></a>透過 Minikube 初步入門</h2><p>本節將安裝 Minikube 來建立 Knative 環境，透過完成簡單範例來體驗。</p><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><ul><li><p>在測試機器安裝 Minikube 二進制執行檔，請至 <a href="https://github.com/kubernetes/minikube/releases" target="_blank" rel="noopener">Minikube Releases</a> 下載。</p></li><li><p>在測試機器下載 <a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">Virtual Box</a> 來提供給 Minikube 建立虛擬機。</p></li></ul><blockquote class="colorquote warning"><ul><li><strong>IMPORTANT</strong>: 測試機器記得開啟 VT-x or AMD-v virtualization.</li><li>雖然建議用 vbox，但是討厭 Oracle 的人可以改用其他虛擬化工具(ex: kvm, xhyve)，理論上可以動。</li></ul></blockquote><ul><li>下載 Kubernetes CLI 工具 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubeclt</a>。</li></ul><h3 id="啟動-Minukube"><a href="#啟動-Minukube" class="headerlink" title="啟動 Minukube"></a>啟動 Minukube</h3><p>首天透過 Minikube 來啟動一台 VM 部署單節點 Kubernetes，由於這邊會用到很多系統服務，因此需要開多一點系統資源：</p><pre><code class="sh">$ minikube start --memory=8192 --cpus=4 \  --kubernetes-version=v1.10.5 \  --extra-config=apiserver.admission-control=&quot;LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook&quot;</code></pre><p>完成後，透過 kubectl 檢查：</p><pre><code class="sh">$ kubectl get noNAME       STATUS    ROLES     AGE       VERSIONminikube   Ready     master    1m        v1.10.5</code></pre><h3 id="部署-Knative"><a href="#部署-Knative" class="headerlink" title="部署 Knative"></a>部署 Knative</h3><p>由於 Knative 是基於 Istio 所開發，因此需要先部署相關服務，這邊透過 kubectl 來建立：</p><pre><code class="sh">$ curl -L https://storage.googleapis.com/knative-releases/serving/latest/istio.yaml \  | sed &#39;s/LoadBalancer/NodePort/&#39; \  | kubectl apply -f -# 設定 inject namespace$ kubectl label namespace default istio-injection=enabled</code></pre><p>這邊會需要一點時間下載映像檔，並啟動 Istio 服務，完成後會如下所示：</p><pre><code class="sh">$ kubectl -n istio-system get poNAME                                       READY     STATUS      RESTARTS   AGEistio-citadel-7bdc7775c7-jn2bw             1/1       Running     0          5mistio-cleanup-old-ca-msvkn                 0/1       Completed   0          5mistio-egressgateway-795fc9b47-4nz7j        1/1       Running     0          6mistio-ingress-84659cf44c-pvqd5             1/1       Running     0          6mistio-ingressgateway-7d89dbf85f-tgm24      1/1       Running     0          6mistio-mixer-post-install-lvrjv             0/1       Completed   0          6mistio-pilot-66f4dd866c-zmbv5               2/2       Running     0          6mistio-policy-76c8896799-cqmdn              2/2       Running     0          6mistio-sidecar-injector-645c89bc64-9mdwx    1/1       Running     0          5mistio-statsd-prom-bridge-949999c4c-qhdgf   1/1       Running     0          6mistio-telemetry-6554768879-b6vss           2/2       Running     0          6m</code></pre><p>接著部署 Knative 元件至 Kubernetes 叢集，官方提供了一個<code>release-lite.yaml</code>檔案來協助建立輕量的測試環境，因此可以直接透過 kubectl 來建立：</p><pre><code class="sh">$ curl -L https://storage.googleapis.com/knative-releases/serving/latest/release-lite.yaml \  | sed &#39;s/LoadBalancer/NodePort/&#39; \  | kubectl apply -f -</code></pre><blockquote class="colorquote info"><ul><li>這邊會部署以 Prometheus 組成的 Monitoring 系統，以及 Knative Serving 與 Build。</li><li>若是其他環境上的 Kubernetes 可以參考 <a href="https://github.com/knative/docs/tree/master/install" target="_blank" rel="noopener">Knative Install</a>。</li></ul></blockquote><p>這邊同樣需要一點時間來下載映像檔，並啟動相關服務，一但完成後會如下所示：</p><pre><code class="sh"># Monitoring$ kubectl -n monitoring get poNAME                                  READY     STATUS    RESTARTS   AGEgrafana-798cf569ff-m8w9c              1/1       Running   0          4mkube-state-metrics-77597b45f8-mxhxv   4/4       Running   0          1mnode-exporter-8wbxd                   2/2       Running   0          4mprometheus-system-0                   1/1       Running   0          4mprometheus-system-1                   1/1       Running   0          4m# Knative build$ kubectl -n knative-build get poNAME                                READY     STATUS    RESTARTS   AGEbuild-controller-5cb4f5cb67-bs94k   1/1       Running   0          6mbuild-webhook-6b4c65546b-fzffg      1/1       Running   0          6m# Knative serving$ kubectl -n knative-serving get poNAME                          READY     STATUS    RESTARTS   AGEactivator-869d7d76c5-fngdm    2/2       Running   0          7mautoscaler-65855c89f6-pmzhr   2/2       Running   0          7mcontroller-5fbcf79dfb-q8cb8   1/1       Running   0          7mwebhook-c98c7c654-lpnjj       1/1       Running   0          7m</code></pre><p>到這邊已完成部署 Knative 元件，接下來將透過一些範例來了解 Knative 功能。</p><h3 id="部署-Knative-應用程式"><a href="#部署-Knative-應用程式" class="headerlink" title="部署 Knative 應用程式"></a>部署 Knative 應用程式</h3><p>當上述元件部署完成後，就可以開始建立 Knative 應用程式與函式，這邊將利用簡單 HTTP Server + Slack 來實作一個簡單 Channel 訊息傳送，過程中將會使用到 Build、BuildTemplate 與 Knative Service 等資源。在開始前，先透過 Git 來取得範例專案，這邊主要是使用裡面的 Kubernetes 部署檔案：</p><pre><code class="sh">$ git clone https://github.com/kairen/knative-slack-app$ cd knative-slack-app</code></pre><p>由於本範例會利用 Kaniko 來建構應用程式的容器映像檔，並將自動將建構好的映像檔上傳至 DockeHub，因此這邊為了確保能夠上傳到自己的 DockerHub，需要建立 Secert 與 Service Account 來提供 Docker ID 與 Passwrod 給 Knative serving 使用：</p><pre><code class="sh">$ export DOCKER_ID=$(echo -n &quot;username&quot; | base64)$ export DOCKER_PWD=$(echo -n &quot;password&quot; | base64)$ cat deploy/docker-secret.yml | \  sed &quot;s/BASE64_ENCODED_USERNAME/${DOCKER_ID}/&quot; | \  sed &quot;s/BASE64_ENCODED_PASSWORD/${DOCKER_PWD}/&quot; | \  kubectl apply -f -$ kubectl apply -f deploy/kaniko-sa.yml</code></pre><p>接著建立一個 Secret 來保存 Slack 的資訊以提供給 Slack App 使用，如 Token：</p><pre><code class="sh">$ export SLACK_TOKEN=$(echo -n &quot;slack-token&quot; | base64)$ export SLACK_CHANNEL_ID=$(echo -n &quot;slack-channel-id&quot; | base64)$ cat deploy/slack-secret.yml | \  sed &quot;s/BASE64_ENCODED_SLACK_TOKEN/${SLACK_TOKEN}/&quot; | \  sed &quot;s/BASE64_ENCODED_SLACK_CHANNEL_ID/${SLACK_CHANNEL_ID}/&quot; | \  kubectl apply -f -</code></pre><p>接著建立 Kaniko Build template 來提供給 Knative Service 建構使用：</p><pre><code class="sh">$ kubectl apply -f deploy/kaniko-buildtemplate.yml$ kubectl get buildtemplateNAME      CREATED ATkaniko    7s</code></pre><p>上面完成後，建立 Knative service 與 Istio HTTPS Service Entry 來提供應用程式，以及讓 Pod 能夠存取 Slack HTTPs API：</p><pre><code class="sh">$ kubectl apply -f deploy/slack-https-sn.yml$ kubectl apply -f deploy/slack-app-service.yml$ kubectl get po -wNAME                    READY     STATUS     RESTARTS   AGEslack-app-00001-9htqm   0/1       Init:2/3   0          8sslack-app-00001-9htqm   0/1       Init:2/3   0         8sslack-app-00001-9htqm   0/1       PodInitializing   0         3mslack-app-00001-9htqm   0/1       Completed   0         4mslack-app-00001-deployment-75f7f8dd8c-tskq8   0/3       Pending   0         0sslack-app-00001-deployment-75f7f8dd8c-tskq8   0/3       Pending   0         0sslack-app-00001-deployment-75f7f8dd8c-tskq8   0/3       Init:0/1   0         0sslack-app-00001-deployment-75f7f8dd8c-tskq8   0/3       PodInitializing   0         7sslack-app-00001-deployment-75f7f8dd8c-tskq8   2/3       Running   0         33s</code></pre><blockquote class="colorquote info"><p>這邊第一次執行會比較慢，因為需要下載 knative build 相關映像檔。</p></blockquote><p>經過一段時間完成後，透過以下指令來確認服務是否正常：</p><pre><code class="sh">$ export IP_ADDRESS=$(minikube ip):$(kubectl get svc knative-ingressgateway -n istio-system   -o &#39;jsonpath={.spec.ports[?(@.port==80)].nodePort}&#39;)$ export DOMAIN=$(kubectl get services.serving.knative.dev slack-app -o=jsonpath=&#39;{.status.domain}&#39;)# 透過 cURL 工具以 Get method 存取$ curl -X GET -H &quot;Host: ${DOMAIN}&quot; ${IP_ADDRESS}&lt;h1&gt;Hello slack app for Knative!!&lt;/h1&gt;# 透過 cURL 工具以 Post method 傳送 msg$ curl -X POST \  -H &#39;Content-type: application/json&#39; \  -H &quot;Host: ${DOMAIN}&quot; \  --data &#39;{&quot;msg&quot;:&quot;Hello, World!&quot;}&#39; \  ${IP_ADDRESS}success</code></pre><p>若成功的話，可以查看 Slack channel 是否有傳送訊息：</p><p><img src="/images/kube/slack-send-msg.png" alt></p><p>最後由於 Knative Serving 是 Request-driven，因此經過長時間沒有任何 request 時，將會自動縮減至 0 副本，直到再次收到 request 才會再次啟動一個實例，然而 Knative 也將大多資訊以 Prometheus 進行監控，因此我們能透過 Prometheus 來觀察狀態變化。</p><p>首先透過 kubectl 取得 Grafana NodePort 資訊：</p><pre><code class="sh">$ kubectl -n monitoring get svcNAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)               AGE...grafana                       NodePort    10.96.197.116   &lt;none&gt;        30802:30326/TCP       1hprometheus-system-np          NodePort    10.99.64.228    &lt;none&gt;        8080:32628/TCP        1h...</code></pre><p>透過瀏覽器開啟 <a href="http://minikube_ip:port" target="_blank" rel="noopener">http://minikube_ip:port</a> 來查看。</p><p><img src="https://i.imgur.com/IBR4rMO.png" alt></p><p>另外也可以查看 HTTP request 狀態。</p><p><img src="https://i.imgur.com/TIlIMhX.png" alt></p><p>由於 Knative 是蠻大的系統，這邊先暫時使用基本 Knative 的 Build 與 Serving，而 Eventing 將會在之後補充範例。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/kube/knative-logo.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/knative&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Knative&lt;/a&gt; 是基於 
      
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Serverless" scheme="https://readailib.github.io/tags/Serverless/"/>
    
      <category term="Istio" scheme="https://readailib.github.io/tags/Istio/"/>
    
  </entry>
  
  <entry>
    <title>以 ExternalDNS 自動同步 Kubernetes Ingress 與 Service DNS 資源紀錄</title>
    <link href="https://readailib.github.io/2018/07/19/kubernetes/k8s-external-dns/"/>
    <id>https://readailib.github.io/2018/07/19/kubernetes/k8s-external-dns/</id>
    <published>2018-07-19T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.224Z</updated>
    
    <content type="html"><![CDATA[<p>本篇說明如何透過 <a href="https://github.com/coredns/coredns" target="_blank" rel="noopener">CoreDNS</a> 自建一套 DNS 服務，並利用 <a href="https://github.com/kubernetes-incubator/external-dns" target="_blank" rel="noopener">Kubernetes ExternalDNS</a> 同步 Kubernetes 的 Ingress 與 Service API object 中的域名(Domain Name)來產生資源紀錄(Record Resources)，讓使用者能夠透過自建 DNS 服務來導向到 Kubernetes 上的應用服務。</p><a id="more"></a><h2 id="使用元件功能介紹"><a href="#使用元件功能介紹" class="headerlink" title="使用元件功能介紹"></a>使用元件功能介紹</h2><ul><li><strong>CoreDNS</strong>：用來提供使用者的 DNS 解析以處理服務導向，並利用 Etcd 插件來儲存與查詢 DNS 資源紀錄(Record resources)。CoreDNS 是由 CNCF 維護的開源 DNS 專案，該專案前身是 SkyDNS，其採用了 <a href="https://github.com/mholt/caddy" target="_blank" rel="noopener">Caddy</a> 的一部分來開發伺服器框架，使其能夠建構一套快速靈活的 DNS，而 CoreDNS 每個功能都可以被實作成一個插件的中介軟體，如 Log、Cache 等功能，甚至能夠將源紀錄儲存至 Redis、Etcd 中。另外 CoreDNS 目前也被 Kubernetes 作為一個內部服務查詢的核心元件，並慢慢取代 KubeDNS 來提供使用。</li></ul><blockquote class="colorquote info"><p>由於市面上大多以 Bind9 作為 DNS，但是 Bind9 並不支援插件與 REST API 功能，雖然效能高又穩定，但是在一些場景並不靈活。</p></blockquote><ul><li><strong>Etcd</strong>：用來儲存 CoreDNS 資源紀錄，並提供給整合的元件查詢與儲存使用。Etcd 是一套分散式鍵值(Key/Value)儲存系統，其功能類似 ZooKeeper，而 Etcd 在一致性演算法採用了 Raft 來處理多節點高可靠性問題，Etcd 好處是支援了 REST API、JSON 格式、SSL 與高效能等，而目前 Etcd  被應用在 Kubernetes 與 Cloud Foundry 等專案中。</li><li><p><strong>ExternalDNS</strong>：用於定期同步 Kubernetes Service 與 Ingress 資源，並依據 Kubernetes 資源內容產生 DNS 資源紀錄來設定 CoreDNS，架構中採用 Etcd 作為兩者溝通中介，一旦有資源紀錄產生就儲存至 Etcd 中，以提供給 CoreDNS 作為資源紀錄來確保服務辨識導向。ExternalDNS 是 Kubernetes 社區的專案，目前被用於同步 Kubernetes 自動設定公有雲 DNS 服務的資源紀錄。</p></li><li><p><strong>Ingress Controller</strong>：提供 Kubernetes Service 能夠以 Domain Name 方式提供外部的存取。Ingress Controller 會監聽 Kubernetes API Server 的 Ingress 與 Service 抽象資源，並依據對應資訊產生組態檔來設定到一個以 NGINX 為引擎的後端，當使用者存取對應服務時，會透過 NGINX 後端進入，這時會依據設定檔的 Domain Name 來轉送給對應 Kubernetes Service。</p></li></ul><blockquote class="colorquote info"><p>Ingress Controller 除了社區提供的專案外，還可以使用 <a href="https://docs.traefik.io/user-guide/kubernetes/" target="_blank" rel="noopener">Traefik</a>、<a href="https://github.com/Kong/kubernetes-ingress-controller" target="_blank" rel="noopener">Kong</a> 等專案。</p></blockquote><ul><li><strong>Kubernetes API Server</strong>：ExternalDNS 會定期抓取來至 API Server 的 Ingress 與 Service 抽象資源，並依據資源內容產生資源紀錄。</li></ul><h2 id="運作流程"><a href="#運作流程" class="headerlink" title="運作流程"></a>運作流程</h2><p>本節說明該架構運作流程，首先當使用者建立了一個 Kubernetes Service 或 Ingress(實作以同步 Ingress 為主)時，會透過與 API Server 溝通建立至 Kubernetes 叢集中，一旦 Service 或 Ingress 建立完成，並正確分配 Service external IP 或是 Ingress address 後，<code>ExternalDNS</code> 會在同步期間抓取所有 Namespace(或指定)中的 Service 與 Ingress 資源，並從 Service 的<code>metadata.annotations</code>取出<code>external-dns.alpha.kubernetes.io/hostname</code>鍵的值，以及從 Ingress 中的<code>spec.rules</code>取出 host 值來產生 DNS 資源紀錄(如 A record)，當完成產生資源紀錄後，再透過 Etcd 儲存該紀錄來讓 CoreDNS 在收到查詢請求時，能夠依據 Etcd 的紀錄來辨識導向。</p><p><img src="https://i.imgur.com/b4QPkr9.png" alt></p><p>拆解不同流程步驟如下：</p><ol><li>使用者建立一個帶有 annotations 的 Service 或是 Ingress。</li></ol><pre><code class="yaml=">apiVersion: v1kind: Servicemetadata:  name: nginx  annotations:    external-dns.alpha.kubernetes.io/hostname: nginx.k8s.local # 將被自動註冊的 domain name.spec:  type: NodePort  selector:    app: nginx  ports:    - protocol: TCP      port: 80      targetPort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: nginx-ingressspec:  rules:  - host: nginx.k8s.local # 將被自動註冊的 domain name.    http:      paths:      - backend:          serviceName: nginx          servicePort: 80</code></pre><blockquote><p>該範例中，若使用 Ingress 的話則不需要在 Service 塞入<code>external-dns.alpha.kubernetes.io/hostname</code>，且不需要使用 NodePort 與 LoadBalancer。</p></blockquote><ol start="2"><li>ExternalDNS 接收到 Service 與 Ingress 抽象資源，取出將被用來註冊 Domain Name 的資訊，並依據上述資訊產生 DNS 資源紀錄(Record resources)，然後儲存到 Etcd。</li><li>當使用者存取 <code>nginx.k8s.local</code> 時，將對 CoreDNS 提供的 DNS 伺服器發送查詢請求，這時 CoreDNS 會到 Etcd 找尋資源紀錄來進行辨識重導向功能，若找到資源紀錄回覆解析結果給使用者。</li><li>這時使用者正確地被導向位址。其中若使用 Service 則要額外輸入對應 Port，用 Ingress 則能夠透過 DN 存取到服務，這是由於 Ingress controller  提供了一個 NGINX Proxy 後端來轉至對應的內部服務。</li></ol><h2 id="測試環境部署"><a href="#測試環境部署" class="headerlink" title="測試環境部署"></a>測試環境部署</h2><p>本節將說明如何透過簡單測試環境來實作上述功能。</p><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>測試環境將需要一套 Kubernetes 叢集，作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>172.22.132.10</td><td>k8s-m1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.11</td><td>k8s-n1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.12</td><td>k8s-n2</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.13</td><td>k8s-g1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.14</td><td>k8s-g2</td><td>8</td><td>16G</td></tr></tbody></table><blockquote><p>這邊<code>m</code>為 k8s master，<code>n</code>為 k8s node。</p></blockquote><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>開始安裝前需要確保以下條件已達成：</p><ul><li>所有節點以 kubeadm 部署成 Kubernetes v1.10+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h3 id="部署-DNS-系統"><a href="#部署-DNS-系統" class="headerlink" title="部署 DNS 系統"></a>部署 DNS 系統</h3><p>首先透過 Git 取得部署用檔案：</p><pre><code class="bash">$ git clone https://github.com/kairen/k8s-external-coredns.git$ cd k8s-external-coredns</code></pre><p>執行以下指令修改一些部署檔案：</p><pre><code class="bash">$ sudo sed -i &quot;s/172.22.132.10/${MASTER_IP}/g&quot; ingress-controller/ingress-controller.yaml$ sudo sed -i &quot;s/172.22.132.10/${MASTER_IP}/g&quot; dns/coredns/coredns-svc-udp.yml$ sudo sed -i &quot;s/172.22.132.10/${MASTER_IP}/g&quot; dns/coredns/coredns-svc-tcp.yml$ sudo sed -i &quot;s/k8s.local/${DOMAIN_NAME}/g&quot; dns/coredns/coredns-cm.yml</code></pre><blockquote><p>這邊因為方便在我環境測試，所以檔案沒改 IP 跟 Domain Name。</p></blockquote><p>完成後開始部署至 Kubernetes 中，首先部署 Ingress Controller：</p><pre><code class="bash">$ kubectl create -f ingress-controller/namespace &quot;ingress-nginx&quot; createddeployment.extensions &quot;default-http-backend&quot; createdservice &quot;default-http-backend&quot; createdconfigmap &quot;nginx-configuration&quot; createdconfigmap &quot;tcp-services&quot; createdconfigmap &quot;udp-services&quot; createdserviceaccount &quot;nginx-ingress-serviceaccount&quot; createdclusterrole.rbac.authorization.k8s.io &quot;nginx-ingress-clusterrole&quot; createdrole.rbac.authorization.k8s.io &quot;nginx-ingress-role&quot; createdrolebinding.rbac.authorization.k8s.io &quot;nginx-ingress-role-nisa-binding&quot; createdclusterrolebinding.rbac.authorization.k8s.io &quot;nginx-ingress-clusterrole-nisa-binding&quot; createdservice &quot;ingress-nginx&quot; createddeployment.extensions &quot;nginx-ingress-controller&quot; created$ kubectl -n ingress-nginx get svc,poNAME                           TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGEservice/default-http-backend   ClusterIP      10.108.17.210   &lt;none&gt;          80/TCP         22sservice/ingress-nginx          LoadBalancer   10.100.149.79   172.22.132.10   80:30383/TCP   22sNAME                                            READY     STATUS    RESTARTS   AGEpod/default-http-backend-5c6d95c48-5qm4g        1/1       Running   0          22spod/nginx-ingress-controller-6c9fcdf8d9-fmnlf   1/1       Running   0          22s</code></pre><p>確認沒問題後，透過瀏覽器開啟<code>External-IP:80</code>來存取頁面。結果如下：</p><p><img src="https://i.imgur.com/wThG3PC.png" alt></p><p>接著部署 CoreDNS 服務：</p><pre><code class="bash">$ kubectl create -f dns/namespace &quot;dns&quot; created$ kubectl create -f dns/coredns/configmap &quot;coredns&quot; createddeployment.extensions &quot;coredns&quot; createdservice &quot;coredns-tcp&quot; createdservice &quot;coredns-udp&quot; createddeployment.extensions &quot;coredns-etcd&quot; createdservice &quot;coredns-etcd&quot; created$ kubectl -n dns get po,svcNAME                                READY     STATUS    RESTARTS   AGEpod/coredns-776f94cf7d-rntxg        1/1       Running   0          16spod/coredns-etcd-847b657579-bnmr5   1/1       Running   0          15sNAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                       AGEservice/coredns-etcd   ClusterIP      10.107.144.189   &lt;none&gt;          2379/TCP,2380/TCP             15sservice/coredns-tcp    LoadBalancer   10.96.34.152     172.22.132.10   53:30050/TCP,9153:32082/TCP   16sservice/coredns-udp    LoadBalancer   10.97.40.197     172.22.132.10   53:30477/UDP                  16s</code></pre><p>確認沒問題後，在使用的系統設定 DNS 伺服器，如下：</p><p><img src="https://i.imgur.com/p6vkPPw.png" alt></p><p>透過 dig 測試 SOA 結果：</p><pre><code class="bash">$ dig @172.22.132.10 SOA k8s.local +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 SOA k8s.local +noall +answer; (1 server found);; global options: +cmdk8s.local.        300    IN    SOA    ns.dns.k8s.local. hostmaster.k8s.local. 1530255393 7200 1800 86400 30</code></pre><p>接著部署 ExternalDNS 來提供自動註冊 Kubernetes record 至 CoreDNS：</p><pre><code class="bash">$ kubectl create -f dns/external-dns/serviceaccount &quot;external-dns&quot; createdclusterrole.rbac.authorization.k8s.io &quot;external-dns&quot; createdclusterrolebinding.rbac.authorization.k8s.io &quot;external-dns-viewer&quot; createddeployment.apps &quot;external-dns&quot; created$ kubectl -n dns get po -l k8s-app=external-dnsNAME                           READY     STATUS    RESTARTS   AGEexternal-dns-94647696b-m494c   1/1       Running   0          38s</code></pre><p>檢查 ExternalDNS Pod 是否正確：</p><pre><code class="bash">$ kubectl -n dns logs -f external-dns-94647696b-m494c...time=&quot;2018-06-29T06:58:35Z&quot; level=info msg=&quot;Connected to cluster at https://10.96.0.1:443&quot;time=&quot;2018-06-29T06:58:35Z&quot; level=debug msg=&quot;No endpoints could be generated from service default/kubernetes&quot;time=&quot;2018-06-29T06:58:35Z&quot; level=debug msg=&quot;No endpoints could be generated from service dns/coredns-etcd&quot;...</code></pre><h2 id="服務測試"><a href="#服務測試" class="headerlink" title="服務測試"></a>服務測試</h2><p>當部署完成後，這時就能夠透過建立一些簡單範例來測試功能是否正確。這邊透過建立一個 cheese 範例來解析三個不同的網頁內容：</p><ul><li>stilton.k8s.local 將導到<code>斯蒂爾頓</code>起司頁面。</li><li>cheddar.k8s.local 將導到<code>切達</code>起司頁面。</li><li>wensleydale.k8s.local 將導到<code>文斯勒德起司</code>起司頁面。</li></ul><p>開始前，先用 dig 來測試使用的 DN 是否能被解析：</p><pre><code class="bash">$ dig @172.22.132.10 A stilton.k8s.local +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 A stilton.k8s.local +noall +answer; (1 server found);; global options: +cmd</code></pre><blockquote><p>可以發現沒有任何 A Record 回傳。</p></blockquote><p>執行下述指令來完成部署：</p><pre><code class="bash">$ kubectl create -f apps/cheese/deployment.extensions &quot;stilton&quot; createddeployment.extensions &quot;cheddar&quot; createddeployment.extensions &quot;wensleydale&quot; createdingress.extensions &quot;cheese&quot; createdservice &quot;stilton&quot; createdservice &quot;cheddar&quot; createdservice &quot;wensleydale&quot; created$ kubectl get po,svc,ingNAME                               READY     STATUS    RESTARTS   AGEpod/cheddar-55cdc7bcc4-926tn       1/1       Running   0          26spod/stilton-5948f8889d-kmj2m       1/1       Running   0          26spod/wensleydale-788869b958-z2kzs   1/1       Running   0          26sNAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGEservice/cheddar       ClusterIP   10.109.22.242    &lt;none&gt;        80/TCP    26sservice/kubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   9dservice/stilton       ClusterIP   10.102.175.194   &lt;none&gt;        80/TCP    26sservice/wensleydale   ClusterIP   10.103.30.255    &lt;none&gt;        80/TCP    25sNAME                        HOSTS                                                       ADDRESS         PORTS     AGEingress.extensions/cheese   stilton.k8s.local,cheddar.k8s.local,wensleydale.k8s.local   172.22.132.10   80        26s</code></pre><p>確認完成部署後，透過 nslookup 來確認能夠解析 Domain Name：</p><pre><code class="bash">$ dig @172.22.132.10 A stilton.k8s.local +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 A stilton.k8s.local +noall +answer; (1 server found);; global options: +cmdstilton.k8s.local.    300    IN    A    172.22.132.10</code></pre><p>現在存取<code>stilton.k8s.local</code>、<code>cheddar.k8s.local</code>與<code>wensleydale.k8s.local</code>來查看差異吧。</p><p><img src="https://i.imgur.com/0nfDKev.jpg" alt></p><p><img src="https://i.imgur.com/tJmcBZL.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇說明如何透過 &lt;a href=&quot;https://github.com/coredns/coredns&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CoreDNS&lt;/a&gt; 自建一套 DNS 服務，並利用 &lt;a href=&quot;https://github.com/kubernetes-incubator/external-dns&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes ExternalDNS&lt;/a&gt; 同步 Kubernetes 的 Ingress 與 Service API object 中的域名(Domain Name)來產生資源紀錄(Record Resources)，讓使用者能夠透過自建 DNS 服務來導向到 Kubernetes 上的應用服務。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="CoreDNS" scheme="https://readailib.github.io/tags/CoreDNS/"/>
    
  </entry>
  
  <entry>
    <title>利用 kubeadm 部署 Kubernetes v1.11.x HA 叢集</title>
    <link href="https://readailib.github.io/2018/07/17/kubernetes/deploy/kubeadm-v1.11-ha/"/>
    <id>https://readailib.github.io/2018/07/17/kubernetes/deploy/kubeadm-v1.11-ha/</id>
    <published>2018-07-17T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.220Z</updated>
    
    <content type="html"><![CDATA[<p>本篇將說明如何透過 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/" target="_blank" rel="noopener">Kubeadm</a> 來部署 Kubernetes v1.11 版本的 High Availability 叢集，而本安裝主要是參考官方文件中的 <a href="https://kubernetes.io/docs/setup/independent/high-availability/" target="_blank" rel="noopener">Creating Highly Available Clusters with kubeadm</a> 內容來進行，這邊將透過 HAProxy 與 Keepalived 的結合來實現控制面的 Load Balancer 與 VIP。</p><a id="more"></a><h2 id="Kubernetes-部署資訊"><a href="#Kubernetes-部署資訊" class="headerlink" title="Kubernetes 部署資訊"></a>Kubernetes 部署資訊</h2><p>Kubernetes 部署的版本資訊：</p><ul><li>kubeadm: v1.11.0</li><li>Kubernetes: v1.11.0</li><li>CNI: v0.6.0</li><li>Etcd: v3.2.18</li><li>Docker: v18.06.0-ce</li><li>Flannel: v0.10.0</li></ul><p>Kubernetes 部署的網路資訊：</p><ul><li><strong>Cluster IP CIDR</strong>: 10.244.0.0/16</li><li><strong>Service Cluster IP CIDR</strong>: 10.96.0.0/12</li><li><strong>Service DNS IP</strong>: 10.96.0.10</li><li><strong>DNS DN</strong>: cluster.local</li><li><strong>Kubernetes API VIP</strong>: 172.22.132.9</li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學採用以下節點數與機器規格進行部署裸機(Bare-metal)，作業系統採用<code>Ubuntu 16+</code>(理論上 CentOS 7+ 也行)進行測試：</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.22.132.10</td><td>k8s-m1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.11</td><td>k8s-m2</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.12</td><td>k8s-m3</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.13</td><td>k8s-g1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.14</td><td>k8s-g2</td><td>4</td><td>16G</td></tr></tbody></table><p>另外由所有 master 節點提供一組 VIP <code>172.22.132.9</code>。</p><blockquote><ul><li>這邊<code>m</code>為 K8s Master 節點，<code>g</code>為 K8s Node 節點。</li><li>所有操作全部用<code>root</code>使用者進行，主要方便部署用。</li></ul></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始部署叢集前需先確保以下條件已達成：</p><ul><li><code>所有節點</code>彼此網路互通，並且<code>k8s-m1</code> SSH 登入其他節點為 passwdless，由於過程中很多會在某台節點(<code>k8s-m1</code>)上以 SSH 複製與操作其他節點。</li><li>確認所有防火牆與 SELinux 已關閉。如 CentOS：</li></ul><pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled</code></pre><blockquote><p>關閉是為了方便安裝使用，若有需要防火牆可以參考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubeadm/#check-required-ports" target="_blank" rel="noopener">Required ports</a> 來設定。</p></blockquote><ul><li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li></ul><pre><code class="sh">$ curl -fsSL https://get.docker.com/ | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><ul><li>所有節點需要加入 APT 與 YUM Kubernetes package 來源：</li></ul><pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add -$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee /etc/apt/sources.list.d/kubernetes.list</code></pre><blockquote><p>若是 CentOS 7 則執行以下方式：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF</code></pre></blockquote><ul><li><code>所有節點</code>需要設定以下系統參數。</li></ul><pre><code class="sh">$ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf</code></pre><blockquote><p>關於<code>bridge-nf-call-iptables</code>的啟用取決於是否將容器連接到<code>Linux bridge</code>或使用其他一些機制(如 SDN vSwitch)。</p></blockquote><ul><li>Kubernetes v1.8+ 要求關閉系統 Swap，請在<code>所有節點</code>利用以下指令關閉：</li></ul><pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# 不同機器會有差異$ sed &#39;/swap.img/d&#39; -i /etc/fstab</code></pre><blockquote><p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p></blockquote><h2 id="Kubernetes-Master-建立"><a href="#Kubernetes-Master-建立" class="headerlink" title="Kubernetes Master 建立"></a>Kubernetes Master 建立</h2><p>本節將說明如何部署與設定 Kubernetes Master 節點中的各元件。</p><p>在開始部署<code>master</code>節點元件前，請先安裝好 kubeadm、kubelet 等套件，並建立<code>/etc/kubernetes/manifests/</code>目錄存放 Static Pod 的 YAML 檔：</p><pre><code class="sh">$ export KUBE_VERSION=&quot;1.11.0&quot;$ apt-get update &amp;&amp; apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00 kubectl=${KUBE_VERSION}-00$ mkdir -p /etc/kubernetes/manifests/</code></pre><p>完成後，依照下面小節完成部署。</p><h3 id="HAProxy"><a href="#HAProxy" class="headerlink" title="HAProxy"></a>HAProxy</h3><p>本節將說明如何建立 HAProxy 來提供 Kubernetes API Server 的負載平衡。在所有<code>master</code>節點<code>/etc/haproxy/</code>目錄：</p><pre><code class="sh">$ mkdir -p /etc/haproxy/</code></pre><p>接著在所有<code>master</code>節點新增<code>/etc/haproxy/haproxy.cfg</code>設定檔，並加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfgglobal  log 127.0.0.1 local0  log 127.0.0.1 local1 notice  tune.ssl.default-dh-param 2048defaults  log global  mode http  option dontlognull  timeout connect 5000ms  timeout client  600000ms  timeout server  600000mslisten stats    bind :9090    mode http    balance    stats uri /haproxy_stats    stats auth admin:admin123    stats admin if TRUEfrontend kube-apiserver-https   mode tcp   bind :8443   default_backend kube-apiserver-backendbackend kube-apiserver-backend    mode tcp    balance roundrobin    stick-table type ip size 200k expire 30m    stick on src    server apiserver1 172.22.132.10:6443 check    server apiserver2 172.22.132.11:6443 check    server apiserver3 172.22.132.12:6443 checkEOF</code></pre><blockquote><p>這邊會綁定<code>8443</code>作為 API Server 的 Proxy。</p></blockquote><p>接著在新增一個路徑為<code>/etc/kubernetes/manifests/haproxy.yaml</code>的 YAML 檔來提供 HAProxy 的 Static Pod 部署，其內容如下：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/haproxy.yamlkind: PodapiVersion: v1metadata:  annotations:    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;  labels:    component: haproxy    tier: control-plane  name: kube-haproxy  namespace: kube-systemspec:  hostNetwork: true  priorityClassName: system-cluster-critical  containers:  - name: kube-haproxy    image: docker.io/haproxy:1.7-alpine    resources:      requests:        cpu: 100m    volumeMounts:    - name: haproxy-cfg      readOnly: true      mountPath: /usr/local/etc/haproxy/haproxy.cfg  volumes:  - name: haproxy-cfg    hostPath:      path: /etc/haproxy/haproxy.cfg      type: FileOrCreateEOF</code></pre><p>接下來將新增另一個 YAML 來提供部署 Keepalived。</p><h3 id="Keepalived"><a href="#Keepalived" class="headerlink" title="Keepalived"></a>Keepalived</h3><p>本節將說明如何建立 Keepalived 來提供 Kubernetes API Server 的 VIP。在所有<code>master</code>節點新增一個路徑為<code>/etc/kubernetes/manifests/keepalived.yaml</code>的 YAML 檔來提供 HAProxy 的 Static Pod 部署，其內容如下：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/keepalived.yamlkind: PodapiVersion: v1metadata:  annotations:    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;  labels:    component: keepalived    tier: control-plane  name: kube-keepalived  namespace: kube-systemspec:  hostNetwork: true  priorityClassName: system-cluster-critical  containers:  - name: kube-keepalived    image: docker.io/osixia/keepalived:1.4.5    env:    - name: KEEPALIVED_VIRTUAL_IPS      value: 172.22.132.9    - name: KEEPALIVED_INTERFACE      value: enp3s0    - name: KEEPALIVED_UNICAST_PEERS      value: &quot;#PYTHON2BASH:[&#39;172.22.132.10&#39;, &#39;172.22.132.11&#39;, &#39;172.22.132.12&#39;]&quot;    - name: KEEPALIVED_PASSWORD      value: d0cker    - name: KEEPALIVED_PRIORITY      value: &quot;100&quot;    - name: KEEPALIVED_ROUTER_ID      value: &quot;51&quot;    resources:      requests:        cpu: 100m    securityContext:      privileged: true      capabilities:        add:        - NET_ADMINEOF</code></pre><blockquote><ul><li><code>KEEPALIVED_VIRTUAL_IPS</code>：Keepalived 提供的 VIPs。</li><li><code>KEEPALIVED_INTERFACE</code>：VIPs 綁定的網卡。</li><li><code>KEEPALIVED_UNICAST_PEERS</code>：其他 Keepalived 節點的單點傳播 IP。</li><li><code>KEEPALIVED_PASSWORD</code>： Keepalived auth_type 的 Password。</li><li><code>KEEPALIVED_PRIORITY</code>：指定了備援發生時，接手的介面之順序，數字越小，優先順序越高。這邊<code>k8s-m1</code>設為 100，其餘為<code>150</code>。</li><li><code>KEEPALIVED_ROUTER_ID</code>：一組 Keepalived instance 的數字識別子。</li></ul></blockquote><h3 id="Kubernetes-Control-Plane"><a href="#Kubernetes-Control-Plane" class="headerlink" title="Kubernetes Control Plane"></a>Kubernetes Control Plane</h3><p>本節將說明如何透過 kubeadm 來建立 control plane 元件，這邊會分別對不同<code>master</code>節點建立初始化 config 檔，並執行不同初始化指令來建立。</p><h4 id="Master1"><a href="#Master1" class="headerlink" title="Master1"></a>Master1</h4><p>首先在<code>k8s-m1</code>節點建立<code>kubeadm-config.yaml</code>的 Kubeadm Master Configuration 檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0apiServerCertSANs:- &quot;172.22.132.9&quot;api:  controlPlaneEndpoint: &quot;172.22.132.9:8443&quot;etcd:  local:    extraArgs:      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.22.132.10:2379&quot;      advertise-client-urls: &quot;https://172.22.132.10:2379&quot;      listen-peer-urls: &quot;https://172.22.132.10:2380&quot;      initial-advertise-peer-urls: &quot;https://172.22.132.10:2380&quot;      initial-cluster: &quot;k8s-m1=https://172.22.132.10:2380&quot;    serverCertSANs:      - k8s-m1      - 172.22.132.10    peerCertSANs:      - k8s-m1      - 172.22.132.10networking:  podSubnet: &quot;10.244.0.0/16&quot;EOF</code></pre><blockquote><p><code>apiServerCertSANs</code>欄位要填入 VIPs; <code>api.controlPlaneEndpoint</code>填入 VIPs 與 bind port。</p></blockquote><p>新增完後，透過 kubeadm 來初始化 control plane：</p><pre><code class="sh">$ kubeadm init --config kubeadm-config.yaml</code></pre><blockquote><p>這邊記得記下來 join 節點資訊，方便後面使用。雖然也可以用 token list 來取得。</p></blockquote><p>經過一段時間完成後，接著透過 netstat 檢查是否正常啟動服務：</p><pre><code class="sh">$ netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      9775/kube-proxytcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      9410/kube-schedulertcp        0      0 172.22.132.10:2379      0.0.0.0:*               LISTEN      9339/etcdtcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      9339/etcdtcp        0      0 172.22.132.10:2380      0.0.0.0:*               LISTEN      9339/etcdtcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      9271/kube-controlletcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      9519/haproxytcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      9519/haproxytcp        0      0 127.0.0.1:44614         0.0.0.0:*               LISTEN      8767/kubelettcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8767/kubelettcp6       0      0 :::10250                :::*                    LISTEN      8767/kubelettcp6       0      0 :::6443                 :::*                    LISTEN      9548/kube-apiservertcp6       0      0 :::10256                :::*                    LISTEN      9775/kube-proxy</code></pre><p>經過一段時間完成後，執行以下指令來使用 kubeconfig：</p><pre><code class="sh">$ mkdir -p $HOME/.kube$ cp -rp /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><p>透過 kubectl 檢查 Kubernetes 叢集狀況：</p><pre><code class="sh">$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    5m        v1.11.0$ kubectl -n kube-system get poNAME                             READY     STATUS    RESTARTS   AGEcoredns-78fcdf6894-9pplt         0/1       Pending   0          5mcoredns-78fcdf6894-qwg58         0/1       Pending   0          5metcd-k8s-m1                      1/1       Running   0          4mkube-apiserver-k8s-m1            1/1       Running   0          4mkube-controller-manager-k8s-m1   1/1       Running   0          4mkube-haproxy-k8s-m1              1/1       Running   0          4mkube-keepalived-k8s-m1           1/1       Running   0          4mkube-proxy-kngb6                 1/1       Running   0          5mkube-scheduler-k8s-m1            1/1       Running   0          4m</code></pre><p>上面完成後，在<code>k8s-m1</code>將 CA 與 Certs 複製到其他<code>master</code>節點上以供使用：</p><pre><code class="sh">$ export DIR=/etc/kubernetes/$ for NODE in k8s-m2 k8s-m3; do    echo &quot;------ ${NODE} ------&quot;    ssh ${NODE} &quot;mkdir -p ${DIR}/pki/etcd&quot;    scp ${DIR}/pki/ca.crt ${NODE}:${DIR}/pki/ca.crt    scp ${DIR}/pki/ca.key ${NODE}:${DIR}/pki/ca.key    scp ${DIR}/pki/sa.key ${NODE}:${DIR}/pki/sa.key    scp ${DIR}/pki/sa.pub ${NODE}:${DIR}/pki/sa.pub    scp ${DIR}/pki/front-proxy-ca.crt ${NODE}:${DIR}/pki/front-proxy-ca.crt    scp ${DIR}/pki/front-proxy-ca.key ${NODE}:${DIR}/pki/front-proxy-ca.key    scp ${DIR}/pki/etcd/ca.crt ${NODE}:${DIR}/pki/etcd/ca.crt    scp ${DIR}/pki/etcd/ca.key ${NODE}:${DIR}/pki/etcd/ca.key    scp ${DIR}/admin.conf ${NODE}:${DIR}/admin.conf  done</code></pre><h4 id="Master2"><a href="#Master2" class="headerlink" title="Master2"></a>Master2</h4><p>首先在<code>k8s-m2</code>節點建立<code>kubeadm-config.yaml</code>的 Kubeadm Master Configuration 檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0apiServerCertSANs:- &quot;172.22.132.9&quot;api:  controlPlaneEndpoint: &quot;172.22.132.9:8443&quot;etcd:  local:    extraArgs:      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.22.132.11:2379&quot;      advertise-client-urls: &quot;https://172.22.132.11:2379&quot;      listen-peer-urls: &quot;https://172.22.132.11:2380&quot;      initial-advertise-peer-urls: &quot;https://172.22.132.11:2380&quot;      initial-cluster: &quot;k8s-m1=https://172.22.132.10:2380,k8s-m2=https://172.22.132.11:2380&quot;      initial-cluster-state: existing    serverCertSANs:      - k8s-m2      - 172.22.132.11    peerCertSANs:      - k8s-m2      - 172.22.132.11networking:  podSubnet: &quot;10.244.0.0/16&quot;EOF</code></pre><p>新增完後，透過 kubeadm phase 來啟動<code>k8s-m2</code>的 kubelet：</p><pre><code class="sh">$ kubeadm alpha phase certs all --config kubeadm-config.yaml$ kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml$ kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml$ kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml$ systemctl start kubelet</code></pre><p>接著執行以下指令來加入節點至 etcd cluster：</p><pre><code class="sh">$ export ETCD1_NAME=k8s-m1; export ETCD1_IP=172.22.132.10$ export ETCD2_NAME=k8s-m2; export ETCD2_IP=172.22.132.11$ export KUBECONFIG=/etc/kubernetes/admin.conf$ kubectl exec -n kube-system etcd-${ETCD1_NAME} -- etcdctl \    --ca-file /etc/kubernetes/pki/etcd/ca.crt \    --cert-file /etc/kubernetes/pki/etcd/peer.crt \    --key-file /etc/kubernetes/pki/etcd/peer.key \    --endpoints=https://${ETCD1_IP}:2379 member add ${ETCD2_NAME} https://${ETCD2_IP}:2380$ kubeadm alpha phase etcd local --config kubeadm-config.yaml</code></pre><p>最後執行以下指令來部署 control plane：</p><pre><code class="sh">$ kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml$ kubeadm alpha phase controlplane all --config kubeadm-config.yaml$ kubeadm alpha phase mark-master --config kubeadm-config.yaml</code></pre><p>經過一段時間完成後，執行以下指令來使用 kubeconfig：</p><pre><code class="sh">$ mkdir -p $HOME/.kube$ cp -rp /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><p>透過 kubectl 檢查 Kubernetes 叢集狀況：</p><pre><code class="sh">$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    10m       v1.11.0k8s-m2    NotReady   master    1m        v1.11.0</code></pre><h4 id="Master3"><a href="#Master3" class="headerlink" title="Master3"></a>Master3</h4><p>首先在<code>k8s-m3</code>節點建立<code>kubeadm-config.yaml</code>的 Kubeadm Master Configuration 檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0apiServerCertSANs:- &quot;172.22.132.9&quot;api:  controlPlaneEndpoint: &quot;172.22.132.9:8443&quot;etcd:  local:    extraArgs:      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.22.132.12:2379&quot;      advertise-client-urls: &quot;https://172.22.132.12:2379&quot;      listen-peer-urls: &quot;https://172.22.132.12:2380&quot;      initial-advertise-peer-urls: &quot;https://172.22.132.12:2380&quot;      initial-cluster: &quot;k8s-m1=https://172.22.132.10:2380,k8s-m2=https://172.22.132.11:2380,k8s-m3=https://172.22.132.12:2380&quot;      initial-cluster-state: existing    serverCertSANs:      - k8s-m3      - 172.22.132.12    peerCertSANs:      - k8s-m3      - 172.22.132.12networking:  podSubnet: &quot;10.244.0.0/16&quot;EOF</code></pre><p>新增完後，透過 kubeadm phase 來啟動<code>k8s-m3</code>的 kubelet：</p><pre><code class="sh">$ kubeadm alpha phase certs all --config kubeadm-config.yaml$ kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml$ kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml$ kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml$ systemctl start kubelet</code></pre><p>接著執行以下指令來加入節點至 etcd cluster：</p><pre><code class="sh">$ export ETCD1_NAME=k8s-m1; export ETCD1_IP=172.22.132.10$ export ETCD3_NAME=k8s-m3; export ETCD3_IP=172.22.132.12$ export KUBECONFIG=/etc/kubernetes/admin.conf$ kubectl exec -n kube-system etcd-${ETCD1_NAME} -- etcdctl \    --ca-file /etc/kubernetes/pki/etcd/ca.crt \    --cert-file /etc/kubernetes/pki/etcd/peer.crt \    --key-file /etc/kubernetes/pki/etcd/peer.key \    --endpoints=https://${ETCD1_IP}:2379 member add ${ETCD3_NAME} https://${ETCD3_IP}:2380$ kubeadm alpha phase etcd local --config kubeadm-config.yaml</code></pre><blockquote><p>此過程與<code>k8s-m2</code>相同，只是修改要加入的 member 名稱與 IP。</p></blockquote><p>最後執行以下指令來部署 control plane：</p><pre><code class="sh">$ kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml$ kubeadm alpha phase controlplane all --config kubeadm-config.yaml$ kubeadm alpha phase mark-master --config kubeadm-config.yaml</code></pre><blockquote><p>此過程與<code>k8s-m2</code>相同。</p></blockquote><p>經過一段時間完成後，執行以下指令來使用 kubeconfig：</p><pre><code class="sh">$ mkdir -p $HOME/.kube$ cp -rp /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><blockquote><p>此過程與<code>k8s-m2</code>相同。</p></blockquote><p>透過 kubectl 檢查 Kubernetes 叢集狀況：</p><pre><code class="sh">$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    20m       v1.11.0k8s-m2    NotReady   master    10m       v1.11.0k8s-m3    NotReady   master    1m        v1.11.0</code></pre><blockquote><p>此過程與<code>k8s-m2</code>相同。</p></blockquote><p>若有更多<code>master</code>節點則以此類推部署，建議透過 CM tools(ex: ansible、puppet) 來撰寫腳本完成。</p><h3 id="建立-Pod-Network"><a href="#建立-Pod-Network" class="headerlink" title="建立 Pod Network"></a>建立 Pod Network</h3><p>當<code>master</code>節點都完成部署後，接著要在此 Kubernetes 部署 Pod Network Plugin，這邊採用 CoreOS Flannel 來提供簡單 Overlay Network 來讓 Pod 中的容器能夠互相溝通：</p><pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.ymlclusterrole &quot;flannel&quot; createdclusterrolebinding &quot;flannel&quot; createdserviceaccount &quot;flannel&quot; configuredconfigmap &quot;kube-flannel-cfg&quot; configureddaemonset &quot;kube-flannel-ds&quot; configured</code></pre><blockquote><ul><li>若參數 <code>--pod-network-cidr=10.244.0.0/16</code> 改變時，在<code>kube-flannel.yml</code>檔案也需修改<code>net-conf.json</code>欄位的 CIDR。</li><li>若使用 Virtualbox 的話，請修改<code>kube-flannel.yml</code>中的 command 綁定 iface，如<code>command: [ &quot;/opt/bin/flanneld&quot;, &quot;--ip-masq&quot;, &quot;--kube-subnet-mgr&quot;, &quot;--iface=eth1&quot; ]</code>。</li><li>其他 Pod Network 可以參考 <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">Installing a pod network</a>。</li></ul></blockquote><p>接著透過 kubectl 查看 Flannel 是否正確在每個 Node 部署：<br>確認 Flannel 部署正確：</p><pre><code class="sh">$ kubectl -n kube-system get po -l app=flannel -o wideNAME                    READY     STATUS    RESTARTS   AGE       IP              NODEkube-flannel-ds-2ssnj   1/1       Running   0          58s       172.22.132.10   k8s-m1kube-flannel-ds-pgfpd   1/1       Running   0          58s       172.22.132.11   k8s-m2kube-flannel-ds-vmt2h   1/1       Running   0          58s       172.22.132.12   k8s-m3$ ip -4 a show flannel.15: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default    inet 10.244.0.0/32 scope global flannel.1       valid_lft forever preferred_lft forever$ route -nKernel IP routing tableDestination     Gateway         Genmask         Flags Metric Ref    Use Iface...10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni010.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.110.244.2.0      10.244.2.0      255.255.255.0   UG    0      0        0 flannel.1</code></pre><h2 id="Kubernetes-Node-建立"><a href="#Kubernetes-Node-建立" class="headerlink" title="Kubernetes Node 建立"></a>Kubernetes Node 建立</h2><p>本節將說明如何部署與設定 Kubernetes Node 節點中。</p><p>在開始部署<code>node</code>節點元件前，請先安裝好 kubeadm、kubelet 等套件，並建立<code>/etc/kubernetes/manifests/</code>目錄存放 Static Pod 的 YAML 檔：</p><pre><code class="sh">$ export KUBE_VERSION=&quot;1.11.0&quot;$ apt-get update &amp;&amp; apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00$ mkdir -p /etc/kubernetes/manifests/</code></pre><p>安裝好後，接著在所有<code>node</code>節點透過 kubeadm 來加入節點：</p><pre><code class="sh">$ kubeadm join 172.22.132.9:8443 \    --token t4zvev.8pasuf89x2ze8htv \    --discovery-token-ca-cert-hash sha256:19c373b19e71b03d89cfc6bdbd59e8f11bd691399b38e7eea11b6043ba73f91d</code></pre><h2 id="部署結果測試"><a href="#部署結果測試" class="headerlink" title="部署結果測試"></a>部署結果測試</h2><p>當節點都完成後，進入<code>master</code>節點透過 kubectl 來檢查：</p><pre><code class="sh">$ kubectl get noNAME      STATUS    ROLES     AGE       VERSIONk8s-g1    Ready     &lt;none&gt;    1m        v1.11.0k8s-g2    Ready     &lt;none&gt;    1m        v1.11.0k8s-m1    Ready     master    30m       v1.11.0k8s-m2    Ready     master    20m       v1.11.0k8s-m3    Ready     master    11m       v1.11.0$  kubectl get csNAME                 STATUS    MESSAGE              ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}</code></pre><blockquote><p>kubeadm 的方式會讓狀態只顯示 etcd-0。</p></blockquote><p>接著進入<code>k8s-m1</code>節點測試叢集 HA 功能，這邊先關閉該節點：</p><pre><code class="sh">$ sudo poweroff</code></pre><p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p><pre><code class="sh"># 先檢查元件狀態$ kubectl get csNAME                 STATUS    MESSAGE              ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}# 檢查 nodes 狀態$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONk8s-g1    Ready      &lt;none&gt;    7m        v1.11.0k8s-g2    Ready      &lt;none&gt;    7m        v1.11.0k8s-m1    NotReady   master    37m       v1.11.0k8s-m2    Ready      master    27m       v1.11.0k8s-m3    Ready      master    18m       v1.11.0# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl expose pod nginx --port 80 --type NodePort$ kubectl get po,svcNAME        READY     STATUS    RESTARTS   AGEpod/nginx   1/1       Running   0          1mNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGEservice/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        3hservice/nginx        NodePort    10.102.191.102   &lt;none&gt;        80:31780/TCP   6s</code></pre><p>透過 cURL 檢查 NGINX 服務是否正常：</p><pre><code class="sh">$ curl 172.22.132.11:31780&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇將說明如何透過 &lt;a href=&quot;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubeadm&lt;/a&gt; 來部署 Kubernetes v1.11 版本的 High Availability 叢集，而本安裝主要是參考官方文件中的 &lt;a href=&quot;https://kubernetes.io/docs/setup/independent/high-availability/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Creating Highly Available Clusters with kubeadm&lt;/a&gt; 內容來進行，這邊將透過 HAProxy 與 Keepalived 的結合來實現控制面的 Load Balancer 與 VIP。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Calico" scheme="https://readailib.github.io/tags/Calico/"/>
    
      <category term="kubeadm" scheme="https://readailib.github.io/tags/kubeadm/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes v1.11.x HA 全手動苦工安裝教學(TL;DR)</title>
    <link href="https://readailib.github.io/2018/07/09/kubernetes/deploy/manual-v1.11/"/>
    <id>https://readailib.github.io/2018/07/09/kubernetes/deploy/manual-v1.11/</id>
    <published>2018-07-09T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.222Z</updated>
    
    <content type="html"><![CDATA[<p>本篇延續過往<code>手動安裝方式</code>來部署 Kubernetes v1.11.x 版本的 High Availability 叢集，而此次教學將直接透過裸機進行部署 Kubernetes 叢集。以手動安裝的目標是學習 Kubernetes 各元件關析、流程、設定與部署方式。若不想這麼累的話，可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a> 來選擇自己最喜歡的方式。</p><p><img src="/images/kube/kubernetes-aa-ha.png" alt></p><a id="more"></a><h2 id="Kubernetes-部署資訊"><a href="#Kubernetes-部署資訊" class="headerlink" title="Kubernetes 部署資訊"></a>Kubernetes 部署資訊</h2><p>Kubernetes 部署的版本資訊：</p><ul><li>Kubernetes: v1.11.0</li><li>CNI: v0.7.1</li><li>Etcd: v3.3.8</li><li>Docker: v18.05.0-ce</li><li>Calico: v3.1</li></ul><p>Kubernetes 部署的網路資訊：</p><ul><li><strong>Cluster IP CIDR</strong>: 10.244.0.0/16</li><li><strong>Service Cluster IP CIDR</strong>: 10.96.0.0/12</li><li><strong>Service DNS IP</strong>: 10.96.0.10</li><li><strong>DNS DN</strong>: cluster.local</li><li><strong>Kubernetes API VIP</strong>: 172.22.132.9</li><li><strong>Kubernetes Ingress VIP</strong>: 172.22.132.8</li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學採用以下節點數與機器規格進行部署裸機(Bare-metal)，作業系統採用<code>Ubuntu 16+</code>(理論上 CentOS 7+ 也行)進行測試：</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th><th>Extra Device</th></tr></thead><tbody><tr><td>172.22.132.10</td><td>k8s-m1</td><td>4</td><td>16G</td><td>None</td></tr><tr><td>172.22.132.11</td><td>k8s-m2</td><td>4</td><td>16G</td><td>None</td></tr><tr><td>172.22.132.12</td><td>k8s-m3</td><td>4</td><td>16G</td><td>None</td></tr><tr><td>172.22.132.13</td><td>k8s-g1</td><td>4</td><td>16G</td><td>GTX 1060 3G</td></tr><tr><td>172.22.132.14</td><td>k8s-g2</td><td>4</td><td>16G</td><td>GTX 1060 3G</td></tr></tbody></table><p>另外由所有 master 節點提供一組 VIP <code>172.22.132.9</code>。</p><blockquote><ul><li>這邊<code>m</code>為 K8s Master 節點，<code>g</code>為 K8s Node 節點。</li><li>所有操作全部用<code>root</code>使用者進行，主要方便部署用。</li></ul></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始部署叢集前需先確保以下條件已達成：</p><ul><li><code>所有節點</code>彼此網路互通，並且<code>k8s-m1</code> SSH 登入其他節點為 passwdless，由於過程中很多會在某台節點(<code>k8s-m1</code>)上以 SSH 複製與操作其他節點。</li><li>確認所有防火牆與 SELinux 已關閉。如 CentOS：</li></ul><pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled</code></pre><blockquote><p>關閉是為了方便安裝使用，若有需要防火牆可以參考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubeadm/#check-required-ports" target="_blank" rel="noopener">Required ports</a> 來設定。</p></blockquote><ul><li><code>所有節點</code>需要設定<code>/etc/hosts</code>解析到所有叢集主機。</li></ul><pre><code>...172.22.132.10 k8s-m1172.22.132.11 k8s-m2172.22.132.12 k8s-m3172.22.132.13 k8s-g1172.22.132.14 k8s-g2</code></pre><ul><li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li></ul><pre><code class="sh">$ curl -fsSL https://get.docker.com/ | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><ul><li><code>所有節點</code>需要設定以下系統參數。</li></ul><pre><code class="sh">$ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf</code></pre><blockquote><p>關於<code>bridge-nf-call-iptables</code>的啟用取決於是否將容器連接到<code>Linux bridge</code>或使用其他一些機制(如 SDN vSwitch)。</p></blockquote><ul><li>Kubernetes v1.8+ 要求關閉系統 Swap，請在<code>所有節點</code>利用以下指令關閉：</li></ul><pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# 不同機器會有差異$ sed &#39;/swap.img/d&#39; -i /etc/fstab</code></pre><blockquote><p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p></blockquote><ul><li>在<code>所有節點</code>下載 Kubernetes 二進制執行檔：</li></ul><pre><code class="sh">$ export KUBE_URL=https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/amd64$ wget ${KUBE_URL}/kubelet -O /usr/local/bin/kubelet$ chmod +x /usr/local/bin/kubelet# Node 可忽略下載 kubectl$ wget ${KUBE_URL}/kubectl -O /usr/local/bin/kubectl$ chmod +x /usr/local/bin/kubectl</code></pre><ul><li>在<code>所有節點</code>下載 Kubernetes CNI 二進制執行檔：</li></ul><pre><code class="sh">$ export CNI_URL=https://github.com/containernetworking/plugins/releases/download$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin$ wget -qO- --show-progress &quot;${CNI_URL}/v0.7.1/cni-plugins-amd64-v0.7.1.tgz&quot; | tar -zx</code></pre><ul><li>在<code>k8s-m1</code>節點安裝<code>cfssl</code>工具，這將會用來建立 CA ，並產生 TLS 憑證。</li></ul><pre><code class="sh">$ export CFSSL_URL=https://pkg.cfssl.org/R1.2$ wget ${CFSSL_URL}/cfssl_linux-amd64 -O /usr/local/bin/cfssl$ wget ${CFSSL_URL}/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson</code></pre><h2 id="建立-CA-與產生-TLS-憑證"><a href="#建立-CA-與產生-TLS-憑證" class="headerlink" title="建立 CA 與產生 TLS 憑證"></a>建立 CA 與產生 TLS 憑證</h2><p>本節將會透過 CFSSL 工具來產生不同元件的憑證，如 Etcd、Kubernetes API Server 等等，其中各元件都會有一個根數位憑證認證機構(Root Certificate Authority)被用在元件之間的認證。</p><blockquote><p>要注意 CA JSON 檔中的<code>CN(Common Name)</code>與<code>O(Organization)</code>等內容是會影響 Kubernetes 元件認證的。</p></blockquote><p>首先在<code>k8s-m1</code>透過 Git 取得部署用檔案：</p><pre><code class="sh">$ git clone https://github.com/kairen/k8s-manual-files.git ~/k8s-manual-files$ cd ~/k8s-manual-files/pki</code></pre><h3 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h3><p>在<code>k8s-m1</code>建立<code>/etc/etcd/ssl</code>資料夾，並產生 Etcd CA：</p><pre><code class="sh">$ export DIR=/etc/etcd/ssl$ mkdir -p ${DIR}$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare ${DIR}/etcd-ca</code></pre><p>接著產生 Etcd 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${DIR}/etcd-ca.pem \  -ca-key=${DIR}/etcd-ca-key.pem \  -config=ca-config.json \  -hostname=127.0.0.1,172.22.132.10,172.22.132.11,172.22.132.12 \  -profile=kubernetes \  etcd-csr.json | cfssljson -bare ${DIR}/etcd</code></pre><blockquote><p><code>-hostname</code>需修改成所有 masters 節點。</p></blockquote><p>刪除不必要的檔案，並檢查<code>/etc/etcd/ssl</code>目錄是否成功建立以下檔案：</p><pre><code class="sh">$ rm -rf ${DIR}/*.csr$ ls /etc/etcd/ssletcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem</code></pre><p>複製檔案至其他 Etcd 節點，這邊為所有<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot; mkdir -p /etc/etcd/ssl&quot;    for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}    done  done</code></pre><h3 id="Kubernetes-元件"><a href="#Kubernetes-元件" class="headerlink" title="Kubernetes 元件"></a>Kubernetes 元件</h3><p>在<code>k8s-m1</code>建立<code>/etc/kubernetes/pki</code>資料夾，並依據下面指令來產生 CA：</p><pre><code class="sh">$ export K8S_DIR=/etc/kubernetes$ export PKI_DIR=${K8S_DIR}/pki$ export KUBE_APISERVER=https://172.22.132.9:6443$ mkdir -p ${PKI_DIR}$ cfssl gencert -initca ca-csr.json | cfssljson -bare ${PKI_DIR}/ca$ ls ${PKI_DIR}/ca*.pem/etc/kubernetes/pki/ca-key.pem  /etc/kubernetes/pki/ca.pem</code></pre><blockquote><p><code>KUBE_APISERVER</code>這邊設定為 VIP 位址。</p></blockquote><p>接著依照以下小節來建立各元件的 TLS 憑證。</p><h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>此憑證將被用於 API Server 與 Kubelet Client 溝通使用。首先透過以下指令產生 Kubernetes API Server 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${PKI_DIR}/ca.pem \  -ca-key=${PKI_DIR}/ca-key.pem \  -config=ca-config.json \  -hostname=10.96.0.1,172.22.132.9,127.0.0.1,kubernetes.default \  -profile=kubernetes \  apiserver-csr.json | cfssljson -bare ${PKI_DIR}/apiserver$ ls ${PKI_DIR}/apiserver*.pem/etc/kubernetes/pki/apiserver-key.pem  /etc/kubernetes/pki/apiserver.pem</code></pre><blockquote><p>這邊<code>-hostname</code>的<code>10.96.0.1</code>是 Cluster IP 的 Kubernetes 端點; <code>172.22.132.9</code>為 VIP 位址; <code>kubernetes.default</code>為 Kubernetes 系統在 default namespace 自動建立的 API service domain name。</p></blockquote><h4 id="Front-Proxy-Client"><a href="#Front-Proxy-Client" class="headerlink" title="Front Proxy Client"></a>Front Proxy Client</h4><p>此憑證將被用於 Authenticating Proxy 的功能上，而該功能主要是提供 API Aggregation 的認證。首先透過以下指令產生 CA：</p><pre><code class="sh">$ cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-ca$ ls ${PKI_DIR}/front-proxy-ca*.pem/etc/kubernetes/pki/front-proxy-ca-key.pem  /etc/kubernetes/pki/front-proxy-ca.pem</code></pre><p>接著產生 Front proxy client 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${PKI_DIR}/front-proxy-ca.pem \  -ca-key=${PKI_DIR}/front-proxy-ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  front-proxy-client-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-client$ ls ${PKI_DIR}/front-proxy-client*.pem/etc/kubernetes/pki/front-proxy-client-key.pem  /etc/kubernetes/pki/front-proxy-client.pem</code></pre><h4 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h4><p>憑證會建立<code>system:kube-controller-manager</code>的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的<code>system:kube-controller-manager</code>來讓 Controller Manager 元件能夠存取需要的 API object。這邊透過以下指令產生 Controller Manager 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${PKI_DIR}/ca.pem \  -ca-key=${PKI_DIR}/ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  manager-csr.json | cfssljson -bare ${PKI_DIR}/controller-manager$ ls ${PKI_DIR}/controller-manager*.pem/etc/kubernetes/pki/controller-manager-key.pem  /etc/kubernetes/pki/controller-manager.pem</code></pre><p>接著利用 kubectl 來產生 Controller Manager 的 kubeconfig 檔：</p><pre><code class="sh">$ kubectl config set-cluster kubernetes \    --certificate-authority=${PKI_DIR}/ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=${K8S_DIR}/controller-manager.conf$ kubectl config set-credentials system:kube-controller-manager \    --client-certificate=${PKI_DIR}/controller-manager.pem \    --client-key=${PKI_DIR}/controller-manager-key.pem \    --embed-certs=true \    --kubeconfig=${K8S_DIR}/controller-manager.conf$ kubectl config set-context system:kube-controller-manager@kubernetes \    --cluster=kubernetes \    --user=system:kube-controller-manager \    --kubeconfig=${K8S_DIR}/controller-manager.conf$ kubectl config use-context system:kube-controller-manager@kubernetes \    --kubeconfig=${K8S_DIR}/controller-manager.conf</code></pre><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>憑證會建立<code>system:kube-scheduler</code>的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的<code>system:kube-scheduler</code>來讓 Scheduler 元件能夠存取需要的 API object。這邊透過以下指令產生 Scheduler 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${PKI_DIR}/ca.pem \  -ca-key=${PKI_DIR}/ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  scheduler-csr.json | cfssljson -bare ${PKI_DIR}/scheduler$ ls ${PKI_DIR}/scheduler*.pem/etc/kubernetes/pki/scheduler-key.pem  /etc/kubernetes/pki/scheduler.pem</code></pre><p>接著利用 kubectl 來產生 Scheduler 的 kubeconfig 檔：</p><pre><code class="sh">$ kubectl config set-cluster kubernetes \    --certificate-authority=${PKI_DIR}/ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=${K8S_DIR}/scheduler.conf$ kubectl config set-credentials system:kube-scheduler \    --client-certificate=${PKI_DIR}/scheduler.pem \    --client-key=${PKI_DIR}/scheduler-key.pem \    --embed-certs=true \    --kubeconfig=${K8S_DIR}/scheduler.conf$ kubectl config set-context system:kube-scheduler@kubernetes \    --cluster=kubernetes \    --user=system:kube-scheduler \    --kubeconfig=${K8S_DIR}/scheduler.conf$ kubectl config use-context system:kube-scheduler@kubernetes \    --kubeconfig=${K8S_DIR}/scheduler.conf</code></pre><h4 id="Admin"><a href="#Admin" class="headerlink" title="Admin"></a>Admin</h4><p>Admin 被用來綁定 RBAC Cluster Role 中 cluster-admin，當想要操作所有 Kubernetes 叢集功能時，就必須利用這邊產生的 kubeconfig 檔案。這邊透過以下指令產生 Kubernetes Admin 憑證：</p><pre><code class="sh">$ cfssl gencert \  -ca=${PKI_DIR}/ca.pem \  -ca-key=${PKI_DIR}/ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  admin-csr.json | cfssljson -bare ${PKI_DIR}/admin$ ls ${PKI_DIR}/admin*.pem/etc/kubernetes/pki/admin-key.pem  /etc/kubernetes/pki/admin.pem</code></pre><p>接著利用 kubectl 來產生 Admin 的 kubeconfig 檔：</p><pre><code class="sh">$ kubectl config set-cluster kubernetes \    --certificate-authority=${PKI_DIR}/ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=${K8S_DIR}/admin.conf$ kubectl config set-credentials kubernetes-admin \    --client-certificate=${PKI_DIR}/admin.pem \    --client-key=${PKI_DIR}/admin-key.pem \    --embed-certs=true \    --kubeconfig=${K8S_DIR}/admin.conf$ kubectl config set-context kubernetes-admin@kubernetes \    --cluster=kubernetes \    --user=kubernetes-admin \    --kubeconfig=${K8S_DIR}/admin.conf$ kubectl config use-context kubernetes-admin@kubernetes \    --kubeconfig=${K8S_DIR}/admin.conf</code></pre><h4 id="Masters-Kubelet"><a href="#Masters-Kubelet" class="headerlink" title="Masters Kubelet"></a>Masters Kubelet</h4><p>這邊使用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/" target="_blank" rel="noopener">Node authorizer</a> 來讓節點的 kubelet 能夠存取如 services、endpoints 等 API，而使用 Node authorizer 需定義 <code>system:nodes</code> 群組(憑證的 Organization)，並且包含<code>system:node:&lt;nodeName&gt;</code>的使用者名稱(憑證的 Common Name)。</p><p>首先在<code>k8s-m1</code>節點產生所有 master 節點的 kubelet 憑證，這邊透過下面腳本來產生：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    cp kubelet-csr.json kubelet-$NODE-csr.json;    sed -i &quot;s/\$NODE/$NODE/g&quot; kubelet-$NODE-csr.json;    cfssl gencert \      -ca=${PKI_DIR}/ca.pem \      -ca-key=${PKI_DIR}/ca-key.pem \      -config=ca-config.json \      -hostname=$NODE \      -profile=kubernetes \      kubelet-$NODE-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-$NODE;    rm kubelet-$NODE-csr.json  done$ ls ${PKI_DIR}/kubelet*.pem/etc/kubernetes/pki/kubelet-k8s-m1-key.pem  /etc/kubernetes/pki/kubelet-k8s-m2.pem/etc/kubernetes/pki/kubelet-k8s-m1.pem      /etc/kubernetes/pki/kubelet-k8s-m3-key.pem/etc/kubernetes/pki/kubelet-k8s-m2-key.pem  /etc/kubernetes/pki/kubelet-k8s-m3.pem</code></pre><p>產生完成後，將 kubelet 憑證複製到所有<code>master</code>節點上：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p ${PKI_DIR}&quot;    scp ${PKI_DIR}/ca.pem ${NODE}:${PKI_DIR}/ca.pem    scp ${PKI_DIR}/kubelet-$NODE-key.pem ${NODE}:${PKI_DIR}/kubelet-key.pem    scp ${PKI_DIR}/kubelet-$NODE.pem ${NODE}:${PKI_DIR}/kubelet.pem    rm ${PKI_DIR}/kubelet-$NODE-key.pem ${PKI_DIR}/kubelet-$NODE.pem  done</code></pre><p>接著利用 kubectl 來產生 kubelet 的 kubeconfig 檔，這邊透過腳本來產生所有<code>master</code>節點的檔案：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;cd ${PKI_DIR} &amp;&amp; \      kubectl config set-cluster kubernetes \        --certificate-authority=${PKI_DIR}/ca.pem \        --embed-certs=true \        --server=${KUBE_APISERVER} \        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \      kubectl config set-credentials system:node:${NODE} \        --client-certificate=${PKI_DIR}/kubelet.pem \        --client-key=${PKI_DIR}/kubelet-key.pem \        --embed-certs=true \        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \      kubectl config set-context system:node:${NODE}@kubernetes \        --cluster=kubernetes \        --user=system:node:${NODE} \        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \      kubectl config use-context system:node:${NODE}@kubernetes \        --kubeconfig=${K8S_DIR}/kubelet.conf&quot;  done</code></pre><h4 id="Service-Account-Key"><a href="#Service-Account-Key" class="headerlink" title="Service Account Key"></a>Service Account Key</h4><p>Kubernetes Controller Manager 利用 Key pair 來產生與簽署 Service Account 的 tokens，而這邊不透過 CA 做認證，而是建立一組公私鑰來讓 API Server 與 Controller Manager 使用：</p><pre><code class="sh">$ openssl genrsa -out ${PKI_DIR}/sa.key 2048$ openssl rsa -in ${PKI_DIR}/sa.key -pubout -out ${PKI_DIR}/sa.pub$ ls ${PKI_DIR}/sa.*/etc/kubernetes/pki/sa.key  /etc/kubernetes/pki/sa.pub</code></pre><h4 id="刪除不必要檔案"><a href="#刪除不必要檔案" class="headerlink" title="刪除不必要檔案"></a>刪除不必要檔案</h4><p>當所有檔案建立與產生完成後，將一些不必要檔案刪除：</p><pre><code class="sh">$ rm -rf ${PKI_DIR}/*.csr \    ${PKI_DIR}/scheduler*.pem \    ${PKI_DIR}/controller-manager*.pem \    ${PKI_DIR}/admin*.pem \    ${PKI_DIR}/kubelet*.pem</code></pre><h4 id="複製檔案至其他節點"><a href="#複製檔案至其他節點" class="headerlink" title="複製檔案至其他節點"></a>複製檔案至其他節點</h4><p>將憑證複製到其他<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    for FILE in $(ls ${PKI_DIR}); do      scp ${PKI_DIR}/${FILE} ${NODE}:${PKI_DIR}/${FILE}    done  done</code></pre><p>複製各元件 kubeconfig 檔案至其他<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    for FILE in admin.conf controller-manager.conf scheduler.conf; do      scp ${K8S_DIR}/${FILE} ${NODE}:${K8S_DIR}/${FILE}    done  done</code></pre><h2 id="Kubernetes-Masters"><a href="#Kubernetes-Masters" class="headerlink" title="Kubernetes Masters"></a>Kubernetes Masters</h2><p>本節將說明如何部署與設定 Kubernetes Master 角色中的各元件，在開始前先簡單了解一下各元件功能：</p><ul><li><strong>kubelet</strong>：負責管理容器的生命週期，定期從 API Server 取得節點上的預期狀態(如網路、儲存等等配置)資源，並呼叫對應的容器介面(CRI、CNI 等)來達成這個狀態。任何 Kubernetes 節點都會擁有該元件。</li><li><strong>kube-apiserver</strong>：以 REST APIs 提供 Kubernetes 資源的 CRUD，如授權、認證、存取控制與 API 註冊等機制。</li><li><strong>kube-controller-manager</strong>：透過核心控制循環(Core Control Loop)監聽 Kubernetes API 的資源來維護叢集的狀態，這些資源會被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而這些控制器會處理著自動擴展、滾動更新等等功能。</li><li><strong>kube-scheduler</strong>：負責將一個(或多個)容器依據排程策略分配到對應節點上讓容器引擎(如 Docker)執行。而排程受到 QoS 要求、軟硬體約束、親和性(Affinity)等等規範影響。</li><li><strong>Etcd</strong>：用來保存叢集所有狀態的 Key/Value 儲存系統，所有 Kubernetes 元件會透過 API Server 來跟 Etcd 進行溝通來保存或取得資源狀態。</li><li><strong>HAProxy</strong>：提供多個 API Server 的負載平衡(Load Balance)。</li><li><strong>Keepalived</strong>：建立一個虛擬 IP(VIP) 來作為 API Server 統一存取端點。</li></ul><p>而上述元件除了 kubelet 外，其他將透過 kubelet 以 <a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/" target="_blank" rel="noopener">Static Pod</a> 方式進行部署，這種方式可以減少管理 Systemd 的服務，並且能透過 kubectl 來觀察啟動的容器狀況。</p><h3 id="部署與設定"><a href="#部署與設定" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>首先在<code>k8s-m1</code>節點進入<code>k8s-manual-files</code>目錄，並依序執行下述指令來完成部署：</p><pre><code class="sh">$ cd ~/k8s-manual-files</code></pre><p>首先利用<code>./hack/gen-configs.sh</code>腳本在每台<code>master</code>節點產生組態檔：</p><pre><code class="sh">$ export NODES=&quot;k8s-m1 k8s-m2 k8s-m3&quot;$ ./hack/gen-configs.shk8s-m1 config generated...k8s-m2 config generated...k8s-m3 config generated...</code></pre><p>完成後記得檢查<code>/etc/etcd/config.yml</code>與<code>/etc/haproxy/haproxy.cfg</code>是否設定正確。</p><blockquote><p>這邊主要確認檔案中的<code>${xxx}</code>字串是否有被更改，並且符合環境。詳細內容可以查看<code>k8s-manual-files</code>。</p></blockquote><p>接著利用<code>./hack/gen-manifests.sh</code>腳本在每台<code>master</code>節點產生 Static pod YAML 檔案，以及其他相關設定檔(如 EncryptionConfig)：</p><pre><code class="sh">$ export NODES=&quot;k8s-m1 k8s-m2 k8s-m3&quot;$ ./hack/gen-manifests.shk8s-m1 manifests generated...k8s-m2 manifests generated...k8s-m3 manifests generated...</code></pre><p>完成後記得檢查<code>/etc/kubernetes/manifests</code>、<code>/etc/kubernetes/encryption</code>與<code>/etc/kubernetes/audit</code>目錄中的檔案是否是定正確。</p><blockquote><p>這邊主要確認檔案中的<code>${xxx}</code>字串是否有被更改，並且符合環境需求。詳細內容可以查看<code>k8s-manual-files</code>。</p></blockquote><p>確認上述兩個產生檔案步驟完成後，即可設定所有<code>master</code>節點的 kubelet systemd 來啟動 Kubernetes 元件。首先複製下列檔案到指定路徑：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d&quot;    scp master/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml    scp master/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service    scp master/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf  done</code></pre><p>接著在<code>k8s-m1</code>透過 SSH 啟動所有<code>master</code>節點的 kubelet：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    ssh ${NODE} &quot;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&quot;  done</code></pre><p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p><pre><code class="sh">$ watch netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      9407/kube-schedulertcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      9338/kube-controlletcp        0      0 127.0.0.1:38420         0.0.0.0:*               LISTEN      8676/kubelettcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      9602/haproxytcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      9602/haproxytcp6       0      0 :::10250                :::*                    LISTEN      8676/kubelettcp6       0      0 :::2379                 :::*                    LISTEN      9487/etcdtcp6       0      0 :::6443                 :::*                    LISTEN      9133/kube-apiservertcp6       0      0 :::2380                 :::*                    LISTEN      9487/etcd...</code></pre><blockquote><p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker</code>指令來查看。</p></blockquote><p>接下來將建立 TLS Bootstrapping 來讓 Node 簽證並授權註冊到叢集。</p><h3 id="建立-TLS-Bootstrapping"><a href="#建立-TLS-Bootstrapping" class="headerlink" title="建立 TLS Bootstrapping"></a>建立 TLS Bootstrapping</h3><p>由於本教學採用 TLS 認證來確保 Kubernetes 叢集的安全性，因此每個節點的 kubelet 都需要透過 API Server 的 CA 進行身份驗證後，才能與 API Server 進行溝通，而這過程過去都是採用手動方式針對每台節點(<code>master</code>與<code>node</code>)單獨簽署憑證，再設定給 kubelet 使用，然而這種方式是一件繁瑣的事情，因為當節點擴展到一定程度時，將會非常費時，甚至延伸初管理不易問題。</p><p>而由於上述問題，Kubernetes 實現了 TLS Bootstrapping 來解決此問題，這種做法是先讓 kubelet 以一個低權限使用者(一個能存取 CSR API 的 Token)存取 API Server，接著對 API Server 提出申請憑證簽署請求，並在受理後由 API Server 動態簽署 kubelet 憑證提供給對應的<code>node</code>節點使用。具體作法請參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a> 與 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank" rel="noopener">Authenticating with Bootstrap Tokens</a>。</p><p>在<code>k8s-m1</code>建立 bootstrap 使用者的 kubeconfig 檔：</p><pre><code class="sh">$ export TOKEN_ID=$(openssl rand 3 -hex)$ export TOKEN_SECRET=$(openssl rand 8 -hex)$ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET}$ export KUBE_APISERVER=&quot;https://172.22.132.9:6443&quot;$ kubectl config set-cluster kubernetes \    --certificate-authority=/etc/kubernetes/pki/ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config set-credentials tls-bootstrap-token-user \    --token=${BOOTSTRAP_TOKEN} \    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config set-context tls-bootstrap-token-user@kubernetes \    --cluster=kubernetes \    --user=tls-bootstrap-token-user \    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config use-context tls-bootstrap-token-user@kubernetes \    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</code></pre><blockquote><p><code>KUBE_APISERVER</code>這邊設定為 VIP 位址。若想要用手動簽署憑證來進行授權的話，可以參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/" target="_blank" rel="noopener">Certificate</a>。</p></blockquote><p>接著在<code>k8s-m1</code>建立 TLS Bootstrap Secret 來提供自動簽證使用：</p><pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Secretmetadata:  name: bootstrap-token-${TOKEN_ID}  namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData:  token-id: &quot;${TOKEN_ID}&quot;  token-secret: &quot;${TOKEN_SECRET}&quot;  usage-bootstrap-authentication: &quot;true&quot;  usage-bootstrap-signing: &quot;true&quot;  auth-extra-groups: system:bootstrappers:default-node-tokenEOFsecret &quot;bootstrap-token-65a3a9&quot; created</code></pre><p>然後建立 TLS Bootstrap Autoapprove RBAC 來提供自動受理 CSR：</p><pre><code class="sh">$ kubectl apply -f master/resources/kubelet-bootstrap-rbac.ymlclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created</code></pre><h3 id="驗證-Master-節點"><a href="#驗證-Master-節點" class="headerlink" title="驗證 Master 節點"></a>驗證 Master 節點</h3><p>完成後，在任意一台<code>master</code>節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證：</p><pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config$ kubectl get csNAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   {&quot;health&quot;:&quot;true&quot;}etcd-1               Healthy   {&quot;health&quot;:&quot;true&quot;}etcd-2               Healthy   {&quot;health&quot;:&quot;true&quot;}$ kubectl -n kube-system get poNAME                             READY     STATUS    RESTARTS   AGEetcd-k8s-m1                      1/1       Running   0          1hetcd-k8s-m2                      1/1       Running   0          1hetcd-k8s-m3                      1/1       Running   0          1hkube-apiserver-k8s-m1            1/1       Running   0          1hkube-apiserver-k8s-m2            1/1       Running   0          1hkube-apiserver-k8s-m3            1/1       Running   0          1h...$ kubectl get nodeNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    38s       v1.11.0k8s-m2    NotReady   master    37s       v1.11.0k8s-m3    NotReady   master    36s       v1.11.0</code></pre><blockquote><p>在這階段狀態處於<code>NotReady</code>是正常，往下進行就會了解為何。</p></blockquote><p>透過 kubectl logs 來查看容器的日誌：</p><pre><code class="sh">$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-k8s-m1)</code></pre><blockquote><p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源存取權限，屬於正常。</p></blockquote><p>為了方便管理叢集，因此需要透過 kubectl logs 來查看，但由於 API 權限問題，故需要建立一個  RBAC Role 來獲取存取權限，這邊在<code>k8s-m1</code>節點執行以下指令建立：</p><pre><code class="sh">$ kubectl apply -f master/resources/apiserver-to-kubelet-rbac.ymlclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created</code></pre><p>完成後，再次透過 kubectl logs 查看 Pod：</p><pre><code class="sh">$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1I0708 15:22:33.906269       1 get.go:245] Starting watch for /api/v1/services, rv=2494 labels= fields= timeout=8m29sI0708 15:22:40.919638       1 get.go:245] Starting watch for /apis/certificates.k8s.io/v1beta1/certificatesigningrequests, rv=11084 labels= fields= timeout=7m29s...</code></pre><p>接著設定 <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">Taints and Tolerations</a> 來讓一些特定 Pod 能夠排程到所有<code>master</code>節點上：</p><pre><code class="sh">$ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --allnode &quot;k8s-m1&quot; taintednode &quot;k8s-m2&quot; taintednode &quot;k8s-m3&quot; tainted</code></pre><p>截至這邊已完成<code>master</code>節點部署，接下來將針對<code>node</code>的部署進行說明。</p><h2 id="Kubernetes-Nodes"><a href="#Kubernetes-Nodes" class="headerlink" title="Kubernetes Nodes"></a>Kubernetes Nodes</h2><p>本節將說明如何建立與設定 Kubernetes Node 節點，Node 是主要執行容器實例(Pod)的工作節點。這過程只需要將 PKI、Bootstrap conf 等檔案複製到機器上，再用 kubelet 啟動即可。</p><p>在開始部署前，在<code>k8-m1</code>將需要用到的檔案複製到所有<code>node</code>節點上：</p><pre><code class="sh">$ for NODE in k8s-g1 k8s-g2; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;    for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}    done  done</code></pre><h3 id="部署與設定-1"><a href="#部署與設定-1" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>確認檔案都複製後，即可設定所有<code>node</code>節點的 kubelet systemd 來啟動 Kubernetes 元件。首先在<code>k8s-m1</code>複製下列檔案到指定路徑：</p><pre><code class="sh">$ cd ~/k8s-manual-files$ for NODE in k8s-g1 k8s-g2; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests&quot;    scp node/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml    scp node/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service    scp node/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf  done</code></pre><p>接著在<code>k8s-m1</code>透過 SSH 啟動所有<code>node</code>節點的 kubelet：</p><pre><code class="sh">$ for NODE in k8s-g1 k8s-g2; do    ssh ${NODE} &quot;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&quot;  done</code></pre><h3 id="驗證-Node-節點"><a href="#驗證-Node-節點" class="headerlink" title="驗證 Node 節點"></a>驗證 Node 節點</h3><p>完成後，在任意一台<code>master</code>節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證：</p><pre><code class="sh">$ kubectl get csrNAME                                                   AGE       REQUESTOR                 CONDITIONcsr-99n76                                              1h        system:node:k8s-m2        Approved,Issuedcsr-9n88h                                              1h        system:node:k8s-m1        Approved,Issuedcsr-vdtqr                                              1h        system:node:k8s-m3        Approved,Issuednode-csr-5VkCjWvb8tGVtO-d2gXiQrnst-G1xe_iA0AtQuYNEMI   2m        system:bootstrap:872255   Approved,Issuednode-csr-Uwpss9OhJrAgOB18P4OIEH02VHJwpFrSoMOWkkrK-lo   2m        system:bootstrap:872255   Approved,Issued$ kubectl get nodesNAME      STATUS     ROLES     AGE       VERSIONk8s-g1    NotReady   &lt;none&gt;    8m        v1.11.0k8s-g2    NotReady   &lt;none&gt;    8m        v1.11.0k8s-m1    NotReady   master    20m       v1.11.0k8s-m2    NotReady   master    20m       v1.11.0k8s-m3    NotReady   master    20m       v1.11.0</code></pre><blockquote><p>在這階段狀態處於<code>NotReady</code>是正常，往下進行就會了解為何。</p></blockquote><p>到這邊就表示<code>node</code>節點部署已完成了，接下來章節將針對 Kubernetes Addons 安裝進行說明。</p><h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成<code>master</code>與<code>node</code>節點的部署，並組合成一個可運作叢集後，就可以開始透過 kubectl 部署 Addons，Kubernetes 官方提供了多種 Addons 來加強 Kubernetes 的各種功能，如叢集 DNS 解析的<code>kube-dns(or CoreDNS)</code>、外部存取服務的<code>kube-proxy</code>與 Web-based 管理介面的<code>dashboard</code>等等。而其中有些 Addons 是被 Kubernetes 認定為必要的，因此本節將說明如何部署這些 Addons。</p><p>首先在<code>k8s-m1</code>節點進入<code>k8s-manual-files</code>目錄，並依序執行下述指令來完成部署：</p><pre><code class="sh">$ cd ~/k8s-manual-files</code></pre><h3 id="Kubernetes-Proxy"><a href="#Kubernetes-Proxy" class="headerlink" title="Kubernetes Proxy"></a>Kubernetes Proxy</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">kube-proxy</a> 是實現 Kubernetes Service 資源功能的關鍵元件，這個元件會透過 DaemonSet 在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的事件，並依據資源預期狀態透過 iptables 或 ipvs 來實現網路轉發，而本次安裝採用 ipvs。</p><p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立，並檢查是否部署成功：</p><pre><code class="sh">$ export KUBE_APISERVER=https://172.22.132.9:6443$ sed -i &quot;s/\${KUBE_APISERVER}/${KUBE_APISERVER}/g&quot; addons/kube-proxy/kube-proxy-cm.yml$ kubectl -f addons/kube-proxy/$ kubectl -n kube-system get po -l k8s-app=kube-proxyNAME               READY     STATUS    RESTARTS   AGEkube-proxy-dd2m7   1/1       Running   0          8mkube-proxy-fwgx8   1/1       Running   0          8mkube-proxy-kjn57   1/1       Running   0          8mkube-proxy-vp47w   1/1       Running   0          8mkube-proxy-xsncw   1/1       Running   0          8m# 檢查 log 是否使用 ipvs$ kubectl -n kube-system logs -f kube-proxy-fwgx8I0709 08:41:48.220815       1 feature_gate.go:230] feature gates: &amp;{map[SupportIPVSProxyMode:true]}I0709 08:41:48.231009       1 server_others.go:183] Using ipvs Proxier....</code></pre><p>若有安裝 ipvsadm 的話，可以透過以下指令查看 proxy 規則：</p><pre><code class="sh">$ ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConnTCP  10.96.0.1:443 rr  -&gt; 172.22.132.9:5443            Masq    1      0          0</code></pre><h3 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h3><p>本節將透過 CoreDNS 取代 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 作為叢集服務發現元件，由於 Kubernetes 需要讓 Pod 與 Pod 之間能夠互相溝通，然而要能夠溝通需要知道彼此的 IP 才行，而這種做法通常是透過 Kubernetes API 來取得達到，但是 Pod IP 會因為生命週期變化而改變，因此這種做法無法彈性使用，且還會增加 API Server 負擔，基於此問題 Kubernetes 提供了 DNS 服務來作為查詢，讓 Pod 能夠以 Service 名稱作為域名來查詢 IP 位址，因此使用者就再不需要關切實際 Pod IP，而 DNS 也會根據 Pod 變化更新資源紀錄(Record resources)。</p><p><a href="https://github.com/coredns/coredns" target="_blank" rel="noopener">CoreDNS</a> 是由 CNCF 維護的開源 DNS 專案，該專案前身是 SkyDNS，其採用了 Caddy 的一部分來開發伺服器框架，使其能夠建構一套快速靈活的 DNS，而 CoreDNS 每個功能都可以被實作成一個插件的中介軟體，如 Log、Cache、Kubernetes 等功能，甚至能夠將源紀錄儲存至 Redis、Etcd 中。</p><p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立，並檢查是否部署成功：</p><pre><code class="sh">$ kubectl create -f addons/coredns/$ kubectl -n kube-system get po -l k8s-app=kube-dnsNAME                       READY     STATUS    RESTARTS   AGEcoredns-589dd74cb6-5mv5c   0/1       Pending   0          3mcoredns-589dd74cb6-d42ft   0/1       Pending   0          3m</code></pre><p>這邊會發現 Pod 處於<code>Pending</code>狀態，這是由於 Kubernetes 的叢集網路沒有建立，因此所有節點會處於<code>NotReady</code>狀態，而這也導致 Kubernetes Scheduler 無法替 Pod 找到適合節點而處於<code>Pending</code>，為了解決這個問題，下節將說明與建立 Kubernetes 叢集網路。</p><blockquote><p>若 Pod 是被 DaemonSet 管理，且設定使用<code>hostNetwork</code>的話，則不會處於<code>Pending</code>狀態。</p></blockquote><h2 id="Kubernetes-叢集網路"><a href="#Kubernetes-叢集網路" class="headerlink" title="Kubernetes 叢集網路"></a>Kubernetes 叢集網路</h2><p>Kubernetes 在預設情況下與 Docker 的網路有所不同。在 Kubernetes 中有四個問題是需要被解決的，分別為：</p><ul><li><strong>高耦合的容器到容器溝通</strong>：透過 Pods 與 Localhost 的溝通來解決。</li><li><strong>Pod 到 Pod 的溝通</strong>：透過實現網路模型來解決。</li><li><strong>Pod 到 Service 溝通</strong>：由 Service objects 結合 kube-proxy 解決。</li><li><strong>外部到 Service 溝通</strong>：一樣由 Service objects 結合 kube-proxy 解決。</li></ul><p>而 Kubernetes 對於任何網路的實現都需要滿足以下基本要求(除非是有意調整的網路分段策略)：</p><ul><li>所有容器能夠在沒有 NAT 的情況下與其他容器溝通。</li><li>所有節點能夠在沒有 NAT 情況下與所有容器溝通(反之亦然)。</li><li>容器看到的 IP 與其他人看到的 IP 是一樣的。</li></ul><p>慶幸的是 Kubernetes 已經有非常多種的<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model" target="_blank" rel="noopener">網路模型</a>以<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" rel="noopener">網路插件(Network Plugins)</a>方式被實現，因此可以選用滿足自己需求的網路功能來使用。另外 Kubernetes 中的網路插件有以下兩種形式：</p><ul><li><strong>CNI plugins</strong>：以 appc/CNI 標準規範所實現的網路，詳細可以閱讀 <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="noopener">CNI Specification</a>。</li><li><strong>Kubenet plugin</strong>：使用 CNI plugins 的 bridge 與 host-local 來實現基本的 cbr0。這通常被用在公有雲服務上的 Kubernetes 叢集網路。</li></ul><blockquote><p>如果想了解如何選擇可以閱讀 Chris Love 的 <a href="https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/" target="_blank" rel="noopener">Choosing a CNI Network Provider for Kubernetes</a> 文章。</p></blockquote><h3 id="網路部署與設定"><a href="#網路部署與設定" class="headerlink" title="網路部署與設定"></a>網路部署與設定</h3><p>從上述了解 Kubernetes 有多種網路能夠選擇，而本教學選擇了 <a href="https://www.projectcalico.org/" target="_blank" rel="noopener">Calico</a> 作為叢集網路的使用。Calico 是一款純 Layer 3 的網路，其好處是它整合了各種雲原生平台(Docker、Mesos 與 OpenStack 等)，且 Calico 不採用 vSwitch，而是在每個 Kubernetes 節點使用 vRouter 功能，並透過 Linux Kernel 既有的 L3 forwarding 功能，而當資料中心複雜度增加時，Calico 也可以利用 BGP route reflector 來達成。</p><blockquote><p>想了解 Calico 與傳統 overlay networks 的差異，可以閱讀 <a href="https://www.projectcalico.org/learn/" target="_blank" rel="noopener">Difficulties with traditional overlay networks</a> 文章。</p></blockquote><p>由於 Calico 提供了 Kubernetes resources YAML 檔來快速以容器方式部署網路插件至所有節點上，因此只需要在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立：</p><pre><code class="sh">$ cd ~/k8s-manual-files$ sed -i &#39;s/192.168.0.0\/16/10.244.0.0\/16/g&#39; cni/calico/v3.1/calico.yaml$ kubectl -f cni/calico/v3.1/</code></pre><blockquote><ul><li>這邊要記得將<code>CALICO_IPV4POOL_CIDR</code>的網路修改 Cluster IP CIDR。</li><li>另外當節點超過 50 台，可以使用 Calico 的 <a href="https://github.com/projectcalico/typha" target="_blank" rel="noopener">Typha</a> 模式來減少透過 Kubernetes datastore 造成 API Server 的負擔。</li></ul></blockquote><p>部署後透過 kubectl 檢查是否有啟動：</p><pre><code class="sh">$ kubectl -n kube-system get po -l k8s-app=calico-nodeNAME                READY     STATUS    RESTARTS   AGEcalico-node-27jwl   2/2       Running   0          59scalico-node-4fgv6   2/2       Running   0          59scalico-node-mvrt7   2/2       Running   0          59scalico-node-p2q9g   2/2       Running   0          59scalico-node-zchsz   2/2       Running   0          59s</code></pre><p>確認 calico-node 都正常運作後，透過 kubectl exec 進入 calicoctl pod 來檢查功能是否正常：</p><pre><code class="sh">$ kubectl exec -ti -n kube-system calicoctl -- calicoctl get profiles -o wideNAME              LABELSkns.default       map[]kns.kube-public   map[]kns.kube-system   map[]$ kubectl exec -ti -n kube-system calicoctl -- calicoctl get node -o wideNAME     ASN         IPV4               IPV6k8s-g1   (unknown)   172.22.132.13/24k8s-g2   (unknown)   172.22.132.14/24k8s-m1   (unknown)   172.22.132.10/24k8s-m2   (unknown)   172.22.132.11/24k8s-m3   (unknown)   172.22.132.12/24</code></pre><blockquote><p>若沒問題，就可以將 kube-system 下的 calicoctl pod 刪除。</p></blockquote><p>完成後，透過檢查節點是否不再是<code>NotReady</code>，以及 Pod 是否不再處於<code>Pending</code>：</p><pre><code class="sh">$ kubectl get noNAME      STATUS    ROLES     AGE       VERSIONk8s-g1    Ready     &lt;none&gt;    35m       v1.11.0k8s-g2    Ready     &lt;none&gt;    35m       v1.11.0k8s-m1    Ready     master    35m       v1.11.0k8s-m2    Ready     master    35m       v1.11.0k8s-m3    Ready     master    35m       v1.11.0$ kubectl -n kube-system get po -l k8s-app=kube-dns -o wideNAME                       READY     STATUS    RESTARTS   AGE       IP           NODEcoredns-589dd74cb6-5mv5c   1/1       Running   0          10m       10.244.4.2   k8s-g2coredns-589dd74cb6-d42ft   1/1       Running   0          10m       10.244.3.2   k8s-g1</code></pre><p>當成功到這邊時，一個能運作的 Kubernetes 叢集基本上就完成了，接下來將介紹一些好用的 Addons 來幫助使用與管理 Kubernetes。</p><h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的額外 Addons，如 Dashboard、Metrics Server 與 Ingress Controller 等等。</p><p>所有 Addons 部署檔案均存已放至<code>k8s-manual-files</code>中，因此在<code>k8s-m1</code>進入該目錄，並依序下小節建立：</p><pre><code class="sh">$ cd ~/k8s-manual-files</code></pre><h3 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h3><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">Ingress</a> 是 Kubernetes 中的一個抽象資源，其功能是透過 Web Server 的 Virtual Host 概念以域名(Domain Name)方式轉發到內部 Service，這避免了使用 Service 中的 NodePort 與 LoadBalancer 類型所帶來的限制(如 Port 數量上限)，而實現 Ingress 功能則是透過 <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers" target="_blank" rel="noopener">Ingress Controller</a> 來達成，它會負責監聽 Kubernetes API 中的 Ingress 與 Service 資源物件，並在發生資源變化時，依據資源預期的結果來設定 Web Server。另外 Ingress Controller 有許多實現可以選擇：</p><ul><li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">Ingress NGINX</a>: Kubernetes 官方維護的專案，也是本次安裝使用的 Controller。</li><li><a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.5/" target="_blank" rel="noopener">F5 BIG-IP Controller</a>: F5 所開發的 Controller，它能夠讓管理員透過 CLI 或 API 從 Kubernetes 與 OpenShift 管理 F5 BIG-IP 設備。</li><li><a href="https://konghq.com/blog/kubernetes-ingress-controller-for-kong/" target="_blank" rel="noopener">Ingress Kong</a>: 著名的開源 API Gateway 專案所維護的 Kubernetes Ingress Controller。</li><li><a href="https://github.com/containous/traefik" target="_blank" rel="noopener">Træfik</a>: 是一套開源的 HTTP 反向代理與負載平衡器，而它也支援了 Ingress。</li><li><a href="https://github.com/appscode/voyager" target="_blank" rel="noopener">Voyager</a>: 一套以 HAProxy 為底的 Ingress Controller。</li></ul><blockquote><p>而 Ingress Controller 的實現不只這些專案，還有很多可以在網路上找到，未來自己也會寫一篇 Ingress Controller 的實作方式文章。</p></blockquote><p>首先在<code>k8s-m1</code>執行下述指令來建立 Ingress Controller，並檢查是否部署正常：</p><pre><code class="sh">$ export INGRESS_VIP=172.22.132.8$ sed -i &quot;s/\${INGRESS_VIP}/${INGRESS_VIP}/g&quot; addons/ingress-controller/ingress-controller-svc.yml$ kubectl create ns ingress-nginx$ kubectl apply -f addons/ingress-controller$ kubectl -n ingress-nginx get po,svcNAME                                           READY     STATUS    RESTARTS   AGEpod/default-http-backend-846b65fb5f-l5hrc      1/1       Running   0          2mpod/nginx-ingress-controller-5db8d65fb-z2lf9   1/1       Running   0          2mNAME                           TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGEservice/default-http-backend   ClusterIP      10.99.105.112   &lt;none&gt;         80/TCP         2mservice/ingress-nginx          LoadBalancer   10.106.18.106   172.22.132.8   80:31197/TCP   2m</code></pre><p>完成後透過瀏覽器存取 <a href="http://172.22.132.8:80" target="_blank" rel="noopener">http://172.22.132.8:80</a> 來查看是否能連線，若可以會如下圖結果。</p><p><img src="https://i.imgur.com/CfbLwOP.png" alt></p><p>當確認上面步驟都沒問題後，就可以透過 kubeclt 建立簡單 NGINX 來測試功能：</p><pre><code class="sh">$ kubectl apply -f apps/nginx/deployment.extensions/nginx createdingress.extensions/nginx-ingress createdservice/nginx created$ kubectl get po,svc,ingNAME                        READY     STATUS    RESTARTS   AGEpod/nginx-966857787-78kth   1/1       Running   0          32sNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGEservice/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   2dservice/nginx        ClusterIP   10.104.180.119   &lt;none&gt;        80/TCP    32sNAME                               HOSTS             ADDRESS        PORTS     AGEingress.extensions/nginx-ingress   nginx.k8s.local   172.22.132.8   80        33s</code></pre><blockquote><p>P.S. Ingress 規則也支援不同 Path 的服務轉發，可以參考上面提供的官方文件來設定。</p></blockquote><p>完成後透過 cURL 工具來測試功能是否正常：</p><pre><code class="sh">$ curl 172.22.132.8 -H &#39;Host: nginx.k8s.local&#39;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...# 測試其他 domain name 是否會回傳 404$ curl 172.22.132.8 -H &#39;Host: nginx1.k8s.local&#39;default backend - 404</code></pre><p>雖然 Ingress 能夠讓我們透過域名方式存取 Kubernetes 內部服務，但是若域名於法被測試機器解析的話，將會顯示<code>default backend - 404</code>結果，而這經常發生在內部自建環境上，雖然可以透過修改主機<code>/etc/hosts</code>來描述，但並不彈性，因此下節將說明如何建立一個 External DNS 與 DNS 伺服器來提供自動解析 Ingress 域名。</p><h3 id="External-DNS"><a href="#External-DNS" class="headerlink" title="External DNS"></a>External DNS</h3><p><a href="https://github.com/kubernetes-incubator/external-dns" target="_blank" rel="noopener">External DNS</a> 是 Kubernetes 社區的孵化專案，被用於定期同步 Kubernetes Service 與 Ingress 資源，並依據資源內容來自動設定公有雲 DNS 服務的資源紀錄(Record resources)。而由於部署不是公有雲環境，因此需要透過 CoreDNS 提供一個內部 DNS 伺服器，再由 ExternalDNS 與這個 CoreDNS 做串接。</p><p>首先在<code>k8s-m1</code>執行下述指令來建立 CoreDNS Server，並檢查是否部署正常：</p><pre><code class="sh">$ export DNS_VIP=172.22.132.8$ sed -i &quot;s/\${DNS_VIP}/${DNS_VIP}/g&quot; addons/external-dns/coredns/coredns-svc-tcp.yml$ sed -i &quot;s/\${DNS_VIP}/${DNS_VIP}/g&quot; addons/external-dns/coredns/coredns-svc-udp.yml$ kubectl create -f addons/external-dns/coredns/$ kubectl -n external-dns get po,svcNAME                                READY     STATUS    RESTARTS   AGEpod/coredns-54bcfcbd5b-5grb5        1/1       Running   0          2mpod/coredns-etcd-6c9c68fd76-n8rhj   1/1       Running   0          2mNAME                   TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                       AGEservice/coredns-etcd   ClusterIP      10.110.186.83    &lt;none&gt;         2379/TCP,2380/TCP             2mservice/coredns-tcp    LoadBalancer   10.109.105.166   172.22.132.8   53:32169/TCP,9153:32150/TCP   2mservice/coredns-udp    LoadBalancer   10.110.242.185   172.22.132.8   53:31210/UDP</code></pre><blockquote><p>這邊域名為<code>k8s.local</code>，可以修改檔案中的<code>coredns-cm.yml</code>來改變。</p></blockquote><p>完成後，透過 dig 工具來檢查是否 DNS 是否正常：</p><pre><code>$ dig @172.22.132.8 SOA nginx.k8s.local +noall +answer +time=2 +tries=1...; (1 server found);; global options: +cmdk8s.local.        300    IN    SOA    ns.dns.k8s.local. hostmaster.k8s.local. 1531299150 7200 1800 86400 30</code></pre><p>接著部署 ExternalDNS 來與 CoreDNS 同步資源紀錄：</p><pre><code class="sh">$ kubectl apply -f addons/external-dns/external-dns/$ kubectl -n external-dns get po -l k8s-app=external-dnsNAME                            READY     STATUS    RESTARTS   AGEexternal-dns-86f67f6df8-ljnhj   1/1       Running   0          1m</code></pre><p>完成後，透過 dig 與 nslookup 工具檢查上節測試 Ingress 的 NGINX 服務：</p><pre><code>$ dig @172.22.132.8 A nginx.k8s.local +noall +answer +time=2 +tries=1...; (1 server found);; global options: +cmdnginx.k8s.local.    300    IN    A    172.22.132.8$ nslookup nginx.k8s.localServer:        172.22.132.8Address:    172.22.132.8#53** server can&#39;t find nginx.k8s.local: NXDOMAIN</code></pre><p>這時會無法透過 nslookup 解析域名，這是因為測試機器並沒有使用這個 DNS 伺服器，可以透過修改<code>/etc/resolv.conf</code>來加入，或者類似下圖方式(不同 OS 有差異，不過都在網路設定中改)。</p><p><img src="https://i.imgur.com/MVDhXKi.png" alt></p><p>再次透過 nslookup 檢查，會發現可以解析了，這時也就能透過 cURL 來測試結果：</p><pre><code class="sh">$ nslookup nginx.k8s.localServer:        172.22.132.8Address:    172.22.132.8#53Name:    nginx.k8s.localAddress: 172.22.132.8$ curl nginx.k8s.local&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...</code></pre><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 官方開發的 Web-based 儀表板，目的是提升管理 Kubernetes 叢集資源便利性，並以資源視覺化方式，來讓人更直覺的看到整個叢集資源狀態，</p><p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立 Dashboard 至 Kubernetes，並檢查是否正確部署：</p><pre><code class="sh">$ cd ~/k8s-manual-files$ kubectl apply -f addons/dashboard/$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboardNAME                                       READY     STATUS    RESTARTS   AGEpod/kubernetes-dashboard-6948bdb78-w26qc   1/1       Running   0          2mNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGEservice/kubernetes-dashboard   ClusterIP   10.109.31.80   &lt;none&gt;        443/TCP   2m</code></pre><p>在這邊會額外建立名稱為<code>anonymous-dashboard-proxy</code>的 Cluster Role(Binding) 來讓<code>system:anonymous</code>這個匿名使用者能夠透過 API Server 來 proxy 到 Kubernetes Dashboard，而這個 RBAC 規則僅能夠存取<code>services/proxy</code>資源，以及<code>https:kubernetes-dashboard:</code>資源名稱。</p><p>因此我們能夠在完成後，透過以下連結來進入 Kubernetes Dashboard：</p><ul><li><a href="https://YOUR_VIP:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">https://{YOUR_VIP}:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></li></ul><p>由於 Kubernetes Dashboard v1.7 版本以後不再提供 Admin 權限，因此需要透過 kubeconfig 或者 Service Account 來進行登入才能取得資源來呈現，這邊建立一個 Service Account 來綁定<code>cluster-admin</code> 以測試功能：</p><pre><code class="sh">$ kubectl -n kube-system create sa dashboard$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;)$ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw</code></pre><blockquote><p>複製<code>token</code>然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。</p></blockquote><p><img src="/images/kube/kubernetes-dashboard.png" alt></p><h3 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h3><p>由於 <a href="https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md" target="_blank" rel="noopener">Heapster</a> 將要被移棄，因此這邊選用 <a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a> 作為第三方的叢集監控方案。而本次安裝採用 CoreOS 開發的 <a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">Prometheus Operator</a> 用於管理在 Kubernetes 上的 Prometheus 叢集與資源，更多關於 Prometheus Operator 的資訊可以參考小弟的 <a href="https://kairen.github.io/2018/06/23/devops/prometheus-operator/" target="_blank" rel="noopener">Prometheus Operator 介紹與安裝</a> 文章。</p><p>首先在<code>k8s-m1</code>執行下述指令來部署所有 Prometheus 需要的元件：</p><pre><code class="sh">$ kubectl apply -f addons/prometheus/$ kubectl apply -f addons/prometheus/operator/# 這邊要等 operator 起來並建立好 CRDs 才能進行$ kubectl apply -f addons/prometheus/alertmanater/$ kubectl apply -f addons/prometheus/node-exporter/$ kubectl apply -f addons/prometheus/kube-state-metrics/$ kubectl apply -f addons/prometheus/grafana/$ kubectl apply -f addons/prometheus/kube-service-discovery/$ kubectl apply -f addons/prometheus/prometheus/$ kubectl apply -f addons/prometheus/servicemonitor/</code></pre><p>完成後，透過 kubectl 檢查服務是否正常運行：</p><pre><code class="sh">$ kubectl -n monitoring get po,svc,ingNAME                                      READY     STATUS    RESTARTS   AGEpod/alertmanager-main-0                   1/2       Running   0          1mpod/grafana-6d495c46d5-jpf6r              1/1       Running   0          43spod/kube-state-metrics-b84cfb86-4b8qg     4/4       Running   0          37spod/node-exporter-2f4lh                   2/2       Running   0          59spod/node-exporter-7cz5s                   2/2       Running   0          59spod/node-exporter-djdtk                   2/2       Running   0          59spod/node-exporter-kfpzt                   2/2       Running   0          59spod/node-exporter-qp2jf                   2/2       Running   0          59spod/prometheus-k8s-0                      3/3       Running   0          28spod/prometheus-k8s-1                      3/3       Running   0          15spod/prometheus-operator-9ffd6bdd9-rvqsz   1/1       Running   0          1mNAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGEservice/alertmanager-main       ClusterIP   10.110.188.2     &lt;none&gt;        9093/TCP            1mservice/alertmanager-operated   ClusterIP   None             &lt;none&gt;        9093/TCP,6783/TCP   1mservice/grafana                 ClusterIP   10.104.147.154   &lt;none&gt;        3000/TCP            43sservice/kube-state-metrics      ClusterIP   None             &lt;none&gt;        8443/TCP,9443/TCP   51sservice/node-exporter           ClusterIP   None             &lt;none&gt;        9100/TCP            1mservice/prometheus-k8s          ClusterIP   10.96.78.58      &lt;none&gt;        9090/TCP            28sservice/prometheus-operated     ClusterIP   None             &lt;none&gt;        9090/TCP            33sservice/prometheus-operator     ClusterIP   10.99.251.16     &lt;none&gt;        8080/TCP            1mNAME                                HOSTS                             ADDRESS        PORTS     AGEingress.extensions/grafana-ing      grafana.monitoring.k8s.local      172.22.132.8   80        45singress.extensions/prometheus-ing   prometheus.monitoring.k8s.local   172.22.132.8   80        34s</code></pre><p>確認沒問題後，透過瀏覽器查看 <a href="http://prometheus.monitoring.k8s.local" target="_blank" rel="noopener">prometheus.monitoring.k8s.local</a> 與 <a href="http://grafana.monitoring.k8s.local" target="_blank" rel="noopener">grafana.monitoring.k8s.local</a> 是否正常，若沒問題就可以看到如下圖所示結果。</p><p><img src="https://i.imgur.com/XFTZ4eF.png" alt></p><p><img src="https://i.imgur.com/YB5KAPe.png" alt></p><blockquote><p>另外這邊也推薦用 <a href="https://github.com/weaveworks/scope" target="_blank" rel="noopener">Weave Scope</a> 來監控容器的網路 Flow 拓樸圖。</p></blockquote><h3 id="Metrics-Server"><a href="#Metrics-Server" class="headerlink" title="Metrics Server"></a>Metrics Server</h3><p><a href="https://github.com/kubernetes-incubator/metrics-server" target="_blank" rel="noopener">Metrics Server</a> 是實現了 Metrics API 的元件，其目標是取代 Heapster 作為 Pod 與 Node 提供資源的 Usage metrics，該元件會從每個 Kubernetes 節點上的 Kubelet 所公開的 Summary API 中收集 Metrics。</p><p>首先在<code>k8s-m1</code>測試一下 kubectl top 指令：</p><pre><code class="sh">$ kubectl top nodeerror: metrics not available yet</code></pre><p>發現 top 指令無法取得 Metrics，這表示 Kubernetes 叢集沒有安裝 Heapster 或是 Metrics Server 來提供 Metrics API 給 top 指令取得資源使用量。</p><p>由於上述問題，我們要在<code>k8s-m1</code>節點透過 kubectl 部署 Metrics Server 元件來解決：</p><pre><code class="sh">$ kubectl create -f addons/metric-server/$ kubectl -n kube-system get po -l k8s-app=metrics-serverNAME                                  READY     STATUS    RESTARTS   AGEpod/metrics-server-86bd9d7667-5hbn6   1/1       Running   0          1m</code></pre><p>完成後，等待一點時間(約 30s - 1m)收集 Metrics，再次執行 kubectl top 指令查看：</p><pre><code class="sh">$ kubectl top nodeNAME      CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%k8s-g1    106m         2%        1037Mi          6%k8s-g2    212m         5%        1043Mi          8%k8s-m1    386m         9%        2125Mi          13%k8s-m2    320m         8%        1834Mi          11%k8s-m3    457m         11%       1818Mi          11%</code></pre><p>而這時若有使用 <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener">HPA</a> 的話，就能夠正確抓到 Pod 的 CPU 與 Memory 使用量了。</p><blockquote><p>若想讓 HPA 使用 Prometheus 的 Metrics 的話，可以閱讀 <a href="https://github.com/stefanprodan/k8s-prom-hpa#setting-up-a-custom-metrics-server" target="_blank" rel="noopener">Custom Metrics Server</a> 來了解。</p></blockquote><h3 id="Helm-Tiller-Server"><a href="#Helm-Tiller-Server" class="headerlink" title="Helm Tiller Server"></a>Helm Tiller Server</h3><p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源。其中<code>Tiller Server</code>主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</p><p>首先在<code>k8s-m1</code>安裝 Helm tool：</p><pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/</code></pre><p>另外在所有<code>node</code>節點安裝 socat：</p><pre><code class="sh">$ sudo apt-get install -y socat</code></pre><p>接著初始化 Helm(這邊會安裝 Tiller Server)：</p><pre><code class="sh">$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller...Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Happy Helming!$ kubectl -n kube-system get po -l app=helmNAME                            READY     STATUS    RESTARTS   AGEtiller-deploy-759cb9df9-rfhqw   1/1       Running   0          19s$ helm versionClient: &amp;version.Version{SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;}</code></pre><h4 id="測試-Helm-功能"><a href="#測試-Helm-功能" class="headerlink" title="測試 Helm 功能"></a>測試 Helm 功能</h4><p>這邊部署簡單 Jenkins 來進行功能測試：</p><pre><code class="sh">$ helm install --name demo --set Persistence.Enabled=false stable/jenkins$ kubectl get po,svc  -l app=demo-jenkinsNAME                           READY     STATUS    RESTARTS   AGEdemo-jenkins-7bf4bfcff-q74nt   1/1       Running   0          2mNAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEdemo-jenkins         LoadBalancer   10.103.15.129    &lt;pending&gt;     8080:31161/TCP   2mdemo-jenkins-agent   ClusterIP      10.103.160.126   &lt;none&gt;        50000/TCP        2m# 取得 admin 帳號的密碼$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echor6y9FMuF2u</code></pre><p>當服務都正常運作時，就可以透過瀏覽器查看 <a href="http://node_ip:31161" target="_blank" rel="noopener">http://node_ip:31161</a> 頁面。</p><p><img src="/images/kube/helm-jenkins-v1.10.png" alt></p><p>測試完成後，就可以透過以下指令來刪除 Release：</p><pre><code class="sh">$ helm lsNAME    REVISION    UPDATED                     STATUS      CHART             NAMESPACEdemo    1           Tue Apr 10 07:29:51 2018    DEPLOYED    jenkins-0.14.4    default$ helm delete demo --purgerelease &quot;demo&quot; deleted</code></pre><p>想要了解更多 Helm Apps 的話，可以到 <a href="https://hub.kubeapps.com/" target="_blank" rel="noopener">Kubeapps Hub</a> 網站尋找。</p><h2 id="測試叢集-HA-功能"><a href="#測試叢集-HA-功能" class="headerlink" title="測試叢集 HA 功能"></a>測試叢集 HA 功能</h2><p>首先進入<code>k8s-m1</code>節點，然後關閉該節點：</p><pre><code class="sh">$ sudo poweroff</code></pre><p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p><pre><code># 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷$ kubectl get csNAME                 STATUS      MESSAGE                                                                                                                                          ERRORscheduler            Healthy     okcontroller-manager   Healthy     oketcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-0               Unhealthy   Get https://172.22.132.10:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl get poNAME      READY     STATUS    RESTARTS   AGEnginx     1/1       Running   0          22s</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇延續過往&lt;code&gt;手動安裝方式&lt;/code&gt;來部署 Kubernetes v1.11.x 版本的 High Availability 叢集，而此次教學將直接透過裸機進行部署 Kubernetes 叢集。以手動安裝的目標是學習 Kubernetes 各元件關析、流程、設定與部署方式。若不想這麼累的話，可以參考 &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Picking the Right Solution&lt;/a&gt; 來選擇自己最喜歡的方式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/kube/kubernetes-aa-ha.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Calico" scheme="https://readailib.github.io/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus 高可靠實現方式</title>
    <link href="https://readailib.github.io/2018/07/01/devops/prometheus-ha/"/>
    <id>https://readailib.github.io/2018/07/01/devops/prometheus-ha/</id>
    <published>2018-07-01T04:23:01.000Z</published>
    <updated>2019-03-05T14:37:50.217Z</updated>
    
    <content type="html"><![CDATA[<p>前面幾篇提到了 Prometheus 儲存系統與 Federation 功能，其中在儲存系統可以得知 Local on-disk 方式雖然能夠帶來很好的效能，但是卻也存在著單點故障的問題，並且限制了 Prometehsu 的可擴展性，引發資料的持久等問題，也因此 Prometheus 提供了遠端儲存(Remote storage)的特性來解決擴展性問題。</p><p>而除了儲存問題外，另一方面就是要考量單一 Prometheus 在大規模環境下的採集樣本效能與乘載量(所能夠處理的時間序列資料)，因此這時候可以利用 Federation 來將不同監測任務劃分到不同實例當中，以解決單台 Prometheus 無法有效處理的狀況。</p><p>而本節主要探討各種 Prometheus 的高可靠(High Availability)架構。</p><blockquote class="colorquote info"><p>這邊不探討 Alert Manager 如何實現高可靠性架構。</p></blockquote><h2 id="服務的高可靠性架構-最基本的-HA"><a href="#服務的高可靠性架構-最基本的-HA" class="headerlink" title="服務的高可靠性架構(最基本的 HA)"></a>服務的高可靠性架構(最基本的 HA)</h2><p>從前面介紹可以得知 Promehteus 是以 Pull-based 進行設計，因此收集時間序列資料(Mtertics)都是透過 Prometheus 本身主動發起，而為了保證 Prometheus 服務能夠正常運作，這邊只需要建立多台 Prometheus 節點來收集同樣的 Metrics(同樣的 Exporter target)即可。</p><p><img src="https://i.imgur.com/ryuQexH.png" alt></p><p>這種做法雖然能夠保證服務的高可靠，但是並無法解決不同 Prometheus Server 之間的資料<code>一致性</code>問題，也無法讓取得的資料進行<code>長時間儲存</code>，且當規模大到單一 Prometheus 無法負荷時，將延伸出效能瓶頸問題，因此這種架構只適合在小規模叢集進行監測，且 Prometheus Server 處於的環境比較不嚴苛，也不會頻繁發生遷移狀況與儲存長週期的資料(Long-term store)。</p><p>上述總結：</p><ul><li><strong>Pros</strong>:<ul><li>服務能夠提供可靠性</li><li>適合小規模監測、只需要短期資料儲存(5ms)、不用經常遷移節點</li></ul></li><li><strong>Cons</strong>:<ul><li>無法動態擴展</li><li>資料會有不一致問題</li><li>資料無法長時間儲存</li><li>不適合在頻繁遷移的狀況</li><li>當乘載量過大時，單一 Prometheus Server 會無法負荷</li></ul></li></ul><h2 id="服務高可靠性結合遠端儲存-基本-HA-Remote-Storage"><a href="#服務高可靠性結合遠端儲存-基本-HA-Remote-Storage" class="headerlink" title="服務高可靠性結合遠端儲存(基本 HA + Remote Storage)"></a>服務高可靠性結合遠端儲存(基本 HA + Remote Storage)</h2><p>這種架構即在基本 HA 上加入遠端儲存功能，讓 Prometheus Server 的讀寫來至第三方儲存系統。</p><p><img src="https://image.ibb.co/iNkteo/prometheus_remote_ha_storage.png" alt></p><p>該架構解決了資料持久性儲存問題，且當 Prometheus Server 發生故障或者當機時，重新啟動能夠快速的恢復資料，同時 Prometheus Server 能夠更好睇進行遷移，但是這只適合在較小規模的監測使用。</p><p>上述總結：</p><ul><li><strong>Pros</strong>:<ul><li>服務能夠提供可靠性</li><li>適合小規模監測</li><li>資料能夠被持久性保存在第三方儲存系統</li><li>Prometheus Server 能夠遷移</li><li>能夠達到資料復原</li></ul></li><li><strong>Cons</strong>:<ul><li>不適合大規模監測</li><li>當乘載量過大時，單一 Prometheus Server 會無法負荷</li></ul></li></ul><h2 id="服務高可靠性結合遠端儲存與聯邦-基本-HA-Remote-Storage-Federation"><a href="#服務高可靠性結合遠端儲存與聯邦-基本-HA-Remote-Storage-Federation" class="headerlink" title="服務高可靠性結合遠端儲存與聯邦(基本 HA + Remote Storage + Federation)"></a>服務高可靠性結合遠端儲存與聯邦(基本 HA + Remote Storage + Federation)</h2><p>這種架構主要是解決單一 Promethes Server 無法處理大量資料收集任務問題，並且加強 Prometheus 的擴展性，透過將不同收集任務劃分到不同 Prometheus 實例上。</p><p><img src="https://i.imgur.com/JAwV0cH.png" alt></p><p>該架構通常有兩種使用場景：</p><ul><li><p><strong>單一資料中心，但是有大量的收集任務</strong>：這種場景下 Prometheus Server 可能會發生效能上瓶頸，主要是單一 Prometheus Server 要乘載大量的資料收集任務，這時候就能夠透過 Federation 來將不同類型的任務分到不同的子 Prometheus Server 上，再由最上層進行聚合資料。</p></li><li><p><strong>多資料中心</strong>：在多資料中心下，這種架構也能夠適用，當不同資料中心的 Exporter 無法讓最上層的 Prometheus 去拉取資料時，就能透過 Federation 來進行分層處理，在每個資料中心建置一組收集該資料中心的子 Prometheus Server，再由最上層的 Prometheus 來進行抓取，並且也能夠依據每個收集任務的乘載量來部署與劃分層級，但是這需要確保上下層的 Prometheus Server 彼此能夠互相溝通。</p></li></ul><p>上述總結：</p><ul><li><strong>Pros</strong>:<ul><li>服務能夠提供可靠性</li><li>資料能夠被持久性保存在第三方儲存系統</li><li>Prometheus Server 能夠遷移</li><li>能夠達到資料復原</li><li>能夠依據不同任務進行層級劃分</li><li>適合不同規模監測</li><li>能夠很好的擴展 Prometheus Server</li></ul></li><li><strong>Cons</strong>:<ul><li>部署架構複雜</li><li>維護困難性增加</li><li>在 Kubernetes 上部署不易</li></ul></li></ul><h2 id="單一收集任務的實例-Scrape-Target-過多問題"><a href="#單一收集任務的實例-Scrape-Target-過多問題" class="headerlink" title="單一收集任務的實例(Scrape Target)過多問題"></a>單一收集任務的實例(Scrape Target)過多問題</h2><p>這問題可能發生在單個 Job 設定太多 Target 數，這時候透過 Federation 來區分可能也無法解決問題，這種情況下只能透過在實例(Instance)級別進行功能劃分。這種做法是將不同實例的資料收集劃分到不同 Prometheus Server 實例，再透過 <code>Relabel</code> 設定來確保當前的 Prometheus Server 只收集當前收集任務的一部分實例監測資料。</p><p>一個簡單範例組態檔：</p><pre><code class="yaml=">global:  external_labels:    slave: 1  # This is the 2nd slave. This prevents clashes between slaves.scrape_configs:  - job_name: some_job    # Add usual service discovery here, such as static_configs    relabel_configs:    - source_labels: [__address__]      modulus:       4    # 4 slaves      target_label:  __tmp_hash      action:        hashmod    - source_labels: [__tmp_hash]      regex:         ^1$  # This is the 2nd slave      action:        keep</code></pre><h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul><li><a href="https://prometheus.io/docs/introduction/faq/#can-prometheus-be-made-highly-available" target="_blank" rel="noopener">https://prometheus.io/docs/introduction/faq/#can-prometheus-be-made-highly-available</a></li><li><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/high-availability.md" target="_blank" rel="noopener">https://github.com/coreos/prometheus-operator/blob/master/Documentation/high-availability.md</a></li><li><a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">https://github.com/coreos/prometheus-operator</a></li><li><a href="https://coreos.com/operators/prometheus/docs/latest/high-availability.html" target="_blank" rel="noopener">https://coreos.com/operators/prometheus/docs/latest/high-availability.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面幾篇提到了 Prometheus 儲存系統與 Federation 功能，其中在儲存系統可以得知 Local on-disk 方式雖然能夠帶來很好的效能，但是卻也存在著單點故障的問題，並且限制了 Prometehsu 的可擴展性，引發資料的持久等問題，也因此 Prome
      
    
    </summary>
    
      <category term="DevOps" scheme="https://readailib.github.io/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="https://readailib.github.io/tags/DevOps/"/>
    
      <category term="Monitoring" scheme="https://readailib.github.io/tags/Monitoring/"/>
    
      <category term="CNCF" scheme="https://readailib.github.io/tags/CNCF/"/>
    
      <category term="Prometheus" scheme="https://readailib.github.io/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>了解 Prometheus Federation 功能</title>
    <link href="https://readailib.github.io/2018/06/29/devops/prometheus-federation/"/>
    <id>https://readailib.github.io/2018/06/29/devops/prometheus-federation/</id>
    <published>2018-06-29T04:23:01.000Z</published>
    <updated>2019-03-05T14:37:50.217Z</updated>
    
    <content type="html"><![CDATA[<p>Prometheus 在效能上是能夠以單個 Server 支撐百萬個時間序列，當然根據不同規模的改變，Promethes 是能夠進行擴展的，這邊將介紹 Prometheus Federation 來達到此效果。</p><p>Prometheus Federation 允許一台 Prometheus Server 從另一台 Prometheus Server 刮取選定的時間序列資料。Federation 提供 Prometheus 擴展能力，這能夠讓 Prometheus 節點擴展至多個，並且能夠實現高可靠性(High Availability)與切片(Sharding)。對於 Prometheus 的 Federation 有不同的使用方式，一般分為<code>Cross-service federation</code>與<code>Hierarchical federation</code>。</p><a id="more"></a><h2 id="Cross-service-federation"><a href="#Cross-service-federation" class="headerlink" title="Cross-service federation"></a>Cross-service federation</h2><p>這種方式的 Federation 會將一個 Prometheus Server 設定成從另一個 Prometheus Server 中獲取選中的時間序列資料，使得這個 Prometheus 能夠對兩個資料來源進行查詢(Query)與警告(Alert)，比如說有一個 Prometheus A 收集了多個服務叢集排程器曝露的資訊使用資訊(CPU、Memory 等)，而另一個在叢集上的 Promethues B 則只收集應用程式指定的服務 Metrics，這時想讓 Prometheus B 收集 Prometheus A 的資源使用量的話，就可以利用 Federation 來取得。</p><p>又或者假設想要監控 mysqld 與 node 的資訊，但是這兩個在不同叢集中，這時可以採用一個 Master Prometheus + 兩個 Sharding Prometheus，其中 Sharding Prometheus 一個收集 node_exporter 的 Metrics，另一個則收集 mysql_exporter，最後 Master Prometheus 透過 Federation 來匯總兩個 Sharding 的時間序列資料。</p><p><img src="https://i.imgur.com/ism3t0M.png" alt></p><h2 id="Hierarchical-federation"><a href="#Hierarchical-federation" class="headerlink" title="Hierarchical federation"></a>Hierarchical federation</h2><p>這種方式能夠讓 Prometheus 擴展到多個資料中心，或者多個節點數量，當建立一個 Federation 叢集時，其拓樸結構會類似一個樹狀結構，並且每一層級會有所對應的級別，比如說較高層級的 Prometheus Server 會從大量低層級的 Prometheus Server 中檢索或聚合時間序列資料。</p><p><img src="https://i.imgur.com/dOinJCq.png" alt></p><p>這種方式適合當單一的 Prometheus 收集 Metrics 的任務(Job)量過大而無法負荷時，可將任務的實例(Instance)進行水平擴展，讓任務的目標實例拆分到不同 Prometheus 中，再由當前資料中心的主 Prometheus 來收集聚合。</p><h2 id="Federation-部署"><a href="#Federation-部署" class="headerlink" title="Federation 部署"></a>Federation 部署</h2><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>測試環境將利用當一節點執行多個 Prometheus 來模擬，作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p><table><thead><tr><th>Name</th><th>Role</th><th>Port</th></tr></thead><tbody><tr><td>Prometheus-global</td><td>Master</td><td>9090</td></tr><tr><td>Prometheus-node</td><td>Collector</td><td>9091</td></tr><tr><td>Prometheus-docker</td><td>Collector</td><td>9092</td></tr></tbody></table><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>開始安裝前需要確保以下條件已達成：</p><ul><li>安裝與設定 Dockerd 提供 Metrics：</li></ul><pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh# 編輯 /etc/docker/daemon.json 加入下面內容$ sudo vim /etc/docker/daemon.json{ &quot;metrics-addr&quot; : &quot;127.0.0.1:9323&quot;, &quot;experimental&quot; : true}# 完成後重新啟動$ sudo systemctl restart docker$ curl 127.0.0.1:9323/metrics</code></pre><ul><li>透過 Docker 部署 Node Exporter：</li></ul><pre><code class="sh">$ docker run -d \  --net=&quot;host&quot; \  --pid=&quot;host&quot; \  --name node-exporter \  quay.io/prometheus/node-exporter$ curl 127.0.0.1:9100/metrics</code></pre><ul><li>在模擬節點下載 Prometheus 伺服器執行檔：</li></ul><pre><code class="sh">$ wget https://github.com/prometheus/prometheus/releases/download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz$ tar xvfz prometheus-*.tar.gz$ mv prometheus-2.3.0.linux-amd64 prometheus-2.3.0$ cd prometheus-2.3.0</code></pre><h3 id="部署-Prometheus-Federation"><a href="#部署-Prometheus-Federation" class="headerlink" title="部署 Prometheus Federation"></a>部署 Prometheus Federation</h3><p>首先新增三個設定檔案，分別給 Global、Docker 與 Node 使用。</p><p>新增一個檔案<code>prometheus-docker.yml</code>，並加入以下內容:</p><pre><code class="yaml=">global:  scrape_interval:     15s  evaluation_interval: 15s  external_labels:      server: &#39;docker-monitor&#39;scrape_configs:  - job_name: &#39;docker&#39;    scrape_interval: 5s    static_configs:      - targets: [&#39;localhost:9323&#39;]</code></pre><p>新增一個檔案<code>prometheus-node.yml</code>，並加入以下內容:</p><pre><code class="yaml=">global:  scrape_interval:     15s  evaluation_interval: 15s  external_labels:      server: &#39;node-monitor&#39;scrape_configs:  - job_name: &#39;node&#39;    scrape_interval: 5s    static_configs:      - targets: [&#39;localhost:9100&#39;]</code></pre><p>新增一個檔案<code>prometheus-global.yml</code>，並加入以下內容:</p><pre><code class="yaml=">global:  scrape_interval:     15s  evaluation_interval: 15s  external_labels:      server: &#39;global-monitor&#39;scrape_configs:  - job_name: &#39;federate&#39;    scrape_interval: 15s    honor_labels: true    metrics_path: &#39;/federate&#39;    params:      &#39;match[]&#39;:        - &#39;{job=~&quot;prometheus.*&quot;}&#39;        - &#39;{job=&quot;docker&quot;}&#39;        - &#39;{job=&quot;node&quot;}&#39;    static_configs:      - targets:        - &#39;localhost:9091&#39;        - &#39;localhost:9092&#39;</code></pre><blockquote><ul><li>當設定 Federation 時，將透過 URL 中的 macth[] 參數指定需要獲取的時間序列資料，match[] 必須是一個向量選擇器資訊，如 up 或者 <code>{job=&quot;api-server&quot;}</code> 等。</li><li>設定<code>honor_labels</code>是避免資料衝突。</li></ul></blockquote><p>完成後，開啟三個 Terminal 來啟動 Prometheus Server：</p><pre><code class="sh"># 啟動收集 Docker metrics 的 Prometheus server$ ./prometheus --config.file=prometheus-docker.yml \     --storage.tsdb.path=./data-docker \     --web.listen-address=&quot;0.0.0.0:9092&quot;# 啟動收集 Node metrics 的 Prometheus server$ ./prometheus --config.file=prometheus-node.yml \     --storage.tsdb.path=./data-node \     --web.listen-address=&quot;0.0.0.0:9091&quot;# 啟動收集 Global 的 Prometheus server$ ./prometheus --config.file=prometheus-global.yml \     --storage.tsdb.path=./data-global \     --web.listen-address=&quot;0.0.0.0:9090&quot;</code></pre><p>正常啟動後分別透過瀏覽器觀察<code>:9090</code>、<code>:9091</code>與<code>:9092</code>會發現 Master 會擁有 Node 與 Docker 的 Metrics，而其他兩者只會有自己所屬 Metrics。</p><blockquote><p>注意，在 Alert 部分還是建議在各自 Sharding 的 Prometheus Server 處理，因為放到 Global 有可能會有接延遲。</p></blockquote><h3 id="部署-Grafana"><a href="#部署-Grafana" class="headerlink" title="部署 Grafana"></a>部署 Grafana</h3><p>在測試節點透過 Docker 部署 Grafana 來提供資料視覺化用：</p><pre><code class="sh">$ docker run \  -d \  -p 3000:3000 \  --name=grafana \  -e &quot;GF_SECURITY_ADMIN_PASSWORD=secret&quot; \  grafana/grafana</code></pre><p>完成後透過瀏覽器查看<code>:3000</code>，並設定 Grafana 將 Prometheus Global 資料做呈現，請至<code>Configuration</code>的<code>Data Sources</code>進行設定。</p><p><img src="https://i.imgur.com/vqGFTXA.png" alt></p><p>接著分別下載以下 Dashbaord JSON 檔案：</p><ul><li><a href="https://grafana.com/api/dashboards/1860/revisions/12/download" target="_blank" rel="noopener">Node Exporter Server Metrics</a></li><li><a href="https://grafana.com/api/dashboards/1229/revisions/3/download" target="_blank" rel="noopener">Docker Metrics</a></li></ul><p>並在 Grafana 點選 Import 選擇上面兩個下載的 JSON 檔案。</p><p><img src="https://i.imgur.com/RdwP0vl.png" alt></p><p>Import 後選擇 Prometheus data source：</p><p><img src="https://i.imgur.com/0NprMK4.png" alt></p><p>確認沒問題後點選<code>Import</code>，這時候就可以在 Dashboard 看到視覺化的 Metrics 了。</p><p><img src="https://i.imgur.com/AgSahRP.png" alt></p><p>Docker Metrics 資訊：</p><p><img src="https://i.imgur.com/Tjpc4Fs.png" alt></p><p>更多的 Dashboard 可以至官方 <a href="https://grafana.com/dashboards" target="_blank" rel="noopener">Dashboards</a> 尋找。</p><h2 id="Prometheus-Federation-不適用地方"><a href="#Prometheus-Federation-不適用地方" class="headerlink" title="Prometheus Federation 不適用地方"></a>Prometheus Federation 不適用地方</h2><p>經上述兩者說明，可以知道 Prometheus Federation 大多被用來從另一個 Prometheus 拉取受限或聚合的時間序列資料集，但是不只上述功能，該 Prometheus 本身還是要肩負警報(Alert)與圖形(Graph)資料查詢工作。而什麼狀況是 Prometheus Federation 不適用的？那就是使用在從另一個 Prometheus 拉取大量時間序列(甚至所有時間序列資料)，並且只從該 Prometheus 做警報(Alert)與圖形(Graph)處理。</p><p>這邊列出三個原因：</p><ul><li><p><strong>效能(Performance)與縮放(Scaling)問題</strong>：Prometheus 的限制因素主要是一台機器所能處理的時間序列資料量，然而讓所有資料路由到一個 Global 的 Prometheus Server 將限制這台 Server 所能處理的監控。取而代之，若只拉取聚合的時間序列資料，只限於一個資料中心的 Prometheus 能夠處理，因此請允許新增資料中心來避免擴大 Global Prometheus。而 Federation 請求本身也能夠大量地服務於接收 Prometheus。</p></li><li><p><strong>可靠性(Reliability)</strong>：如果需要進行警報(Alert)的資料從一個 Prometheus 移動到另一個時，那麼這樣就會多出一個額外的故障點。當牽扯到諸如互聯網之類的廣域網路連接時，是特別危險的。在可能的情況下，應該盡量將警報(Alert)推送到 Federation 層級較深的 Prometheus上。</p></li><li><p><strong>正確性(Correctness)</strong>：由於工作原理關析，Federation 會在被刮取(scraped)後的某一段時間拉取資料，並且可能因 Race 問題而遺失一些資料。雖然這問題在 Global Promethesu 能夠被容忍，但是用於處理警報(Alert)與圖表查詢的資料中心 Prometheus 就可能造成問題。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Prometheus 在效能上是能夠以單個 Server 支撐百萬個時間序列，當然根據不同規模的改變，Promethes 是能夠進行擴展的，這邊將介紹 Prometheus Federation 來達到此效果。&lt;/p&gt;
&lt;p&gt;Prometheus Federation 允許一台 Prometheus Server 從另一台 Prometheus Server 刮取選定的時間序列資料。Federation 提供 Prometheus 擴展能力，這能夠讓 Prometheus 節點擴展至多個，並且能夠實現高可靠性(High Availability)與切片(Sharding)。對於 Prometheus 的 Federation 有不同的使用方式，一般分為&lt;code&gt;Cross-service federation&lt;/code&gt;與&lt;code&gt;Hierarchical federation&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://readailib.github.io/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="https://readailib.github.io/tags/DevOps/"/>
    
      <category term="Monitoring" scheme="https://readailib.github.io/tags/Monitoring/"/>
    
      <category term="CNCF" scheme="https://readailib.github.io/tags/CNCF/"/>
    
      <category term="Prometheus" scheme="https://readailib.github.io/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes exec API 串接分析</title>
    <link href="https://readailib.github.io/2018/06/25/kubernetes/k8x-exec-api/"/>
    <id>https://readailib.github.io/2018/06/25/kubernetes/k8x-exec-api/</id>
    <published>2018-06-25T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.225Z</updated>
    
    <content type="html"><![CDATA[<p>本篇將說明 Kubernetes exec API 的運作方式，並以簡單範例進行開發在前後端上。雖然 Kubernetes 提供了不同資源的 RESTful API 來進行 CRUD 操作，但是部分 API 並非單純的回傳一個資料，有些是需要透過 SPDY 或 WebSocket 建立長連線串流，這種 API 以 exec、attach 為主，目標是對一個 Pod 執行指定指令，或者進入該 Pod 進行互動等等。</p><a id="more"></a><h2 id="Exec-API-Endpoint"><a href="#Exec-API-Endpoint" class="headerlink" title="Exec API Endpoint"></a>Exec API Endpoint</h2><p>首先了解一下 Kubernetes exec API endpoint，由於 Kubernetes 官方文件並未提供相關資訊，因此這邊透過 kubectl 指令來了解 API 的結構：</p><pre><code class="bash">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:  name: ubuntuspec:  containers:  - name: ubuntu    image: ubuntu:16.04    command: [&#39;/bin/bash&#39;, &#39;-c&#39;, &#39;while :; do  echo Hello; sleep 1; done &#39;]EOF$ kubectl -v=8 exec -ti ubuntu bash...I0625 10:39:33.716271   93099 round_trippers.go:383] POST https://xxx.xxx.xxx.xxx:8443/api/v1/namespaces/default/pods/ubuntu/exec?command=bash&amp;container=ubuntu&amp;container=ubuntu&amp;stdin=true&amp;stdout=true&amp;tty=true...</code></pre><p>從上述得知 exec API 結構大致如下圖所示：</p><p><img src="https://i.imgur.com/wMcqqMe.png" alt></p><p>其中 API 中的 Querys 又可細分以下資訊：</p><ul><li><strong>command</strong>：將被執行的指令。若指令為<code>ping 8.8.8.8</code>，則 API 為<code>command=ping&amp;command=8.8.8.8</code>。類型為<code>string</code>值。</li><li><strong>container</strong>：哪個容器將被執行指令。若 Pod 只有一個容器，一般會用 API 找出名稱塞到該參數中，若多個則選擇讓人輸入名稱。類型為<code>string</code>值。</li><li><strong>stdin</strong>：是否開啟標準輸入，通常由使用者決定是否開啟。類型為<code>bool</code>值。</li><li><strong>stdout</strong>：是否開啟標準輸出，通常是<code>預設開啟</code>。類型為<code>bool</code>值。</li><li><strong>stderr</strong>：是否開啟標準錯誤輸出，通常是<code>預設開啟</code>。類型為<code>bool</code>值。</li><li><strong>tty</strong>：是否分配一個偽終端設備(Pseudo TTY, PTY)。ㄒ為<code>bool</code>值。</li></ul><h2 id="Protocol"><a href="#Protocol" class="headerlink" title="Protocol"></a>Protocol</h2><p>Execute 是利用 SPDY 與 WebSocket 協定進行串流溝通的 API，其中 SPDY 在 Kubernetes 官方的 client-go 已經有實現(參考 <a href="https://github.com/kubernetes/client-go/blob/master/tools/remotecommand/remotecommand.go" target="_blank" rel="noopener">Remote command</a>)，而 kubectl 正是使用 SPDY，但是 SPDY 目前已經規劃在未來將被<a href="https://github.com/kubernetes/features/issues/384" target="_blank" rel="noopener">移棄</a>，因此建議選擇使用 WebSocket 來作為串流溝通。但而無論是使用哪一個協定，都要注意請求的 Header 必須有<code>Connection: Upgrade</code>、<code>Upgrade: xxx</code>等，不然 API Server 會拒絕存取請求。</p><h2 id="HTTP-Headers"><a href="#HTTP-Headers" class="headerlink" title="HTTP Headers"></a>HTTP Headers</h2><p>除了 SPDY 與 WebSocket 所需要的 Headers(如 Upgrade 等)外，使用者與開發者還必須提供兩個 Headers 來確保能夠正確授權並溝通：</p><ul><li><strong>Authorization</strong>：該 Header 是用來提供給 API Server 做認證請求的資訊，通常會是以<code>Authorization: Bearer &lt;token&gt;</code>的形式。</li><li><strong>Accept</strong>：指定客戶端能夠接收的內容類型，一般為<code>Accept: application/json</code>，若輸入不支援的類型將會被 API 以<code>406 Not Acceptable</code> 拒絕請求。</li></ul><h2 id="溝通協定"><a href="#溝通協定" class="headerlink" title="溝通協定"></a>溝通協定</h2><p>一旦符合上述所有資訊後，WebSocket(或 SPDY)就能夠建立連線，並且與 API Server 進行溝通。而當寫入 WebSocket 時，資料將被傳送到標準輸入(stdin)，而 WebSocket 的接收將會是標準輸出(stdout)與輸出錯誤(stderr)。Kubernetes API Server 簡單定義了一個協定來復用 stdout 與 stderr。因此可以理解當 WebSocket 建立連線後，傳送資料時需要再 Buffer 的第一個字元定義為 stdin(buf[0] = 0)，而接收資料時要判斷 stdout(buf[0] = 1) 與 stderr(buf[0] = 2)。其資訊如下：</p><table><thead><tr><th>Code</th><th>標準串流</th></tr></thead><tbody><tr><td>0</td><td>stdin</td></tr><tr><td>1</td><td>stdout</td></tr><tr><td>2</td><td>stderr</td></tr></tbody></table><p>下面簡單以發送<code>ls</code>指令為例：</p><pre><code class="bash"># 傳送`ls`指令，必須 buf[0] 自行塞入 0 字元來表示 stdin。buf = [0 108 115 10]# Receivebuf = [1 108 115 13 10 27 91 48 109 27 91 ...]</code></pre><blockquote><p>最後需要注意 Timeout 問題，由於可能對 WebSocket 設定 TCP Timeout，因此建議每一段時間發送一個 stdin 空訊息來保持連線。</p></blockquote><p>實作參考一些專案自行練習寫了 Go 語言版本 <a href="https://github.com/kairen/k8s-ws-exec" target="_blank" rel="noopener">CLI</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇將說明 Kubernetes exec API 的運作方式，並以簡單範例進行開發在前後端上。雖然 Kubernetes 提供了不同資源的 RESTful API 來進行 CRUD 操作，但是部分 API 並非單純的回傳一個資料，有些是需要透過 SPDY 或 WebSocket 建立長連線串流，這種 API 以 exec、attach 為主，目標是對一個 Pod 執行指定指令，或者進入該 Pod 進行互動等等。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus Operator 介紹與安裝</title>
    <link href="https://readailib.github.io/2018/06/23/devops/prometheus-operator/"/>
    <id>https://readailib.github.io/2018/06/23/devops/prometheus-operator/</id>
    <published>2018-06-23T04:23:01.000Z</published>
    <updated>2019-03-05T14:37:50.218Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">Prometheus Operator</a> 是 CoreOS 開源的一套用於管理在 Kubernetes 上的 Prometheus 控制器，目標當然就是簡化部署與維護 Prometheus 上的事情，其架構如下所示：</p><p><img src="https://coreos.com/sites/default/files/inline-images/p1.png" alt></p><a id="more"></a><p>架構中的每一個部分都執行於 Kubernetes 的資源，這些資源分別負責不同作用與意義：</p><ul><li><strong><a href="https://coreos.com/operators/" target="_blank" rel="noopener">Operator</a></strong>：Operator 是整個系統的主要控制器，會以 Deployment 方式執行於 Kubernetes 叢集上，並根據自定義的資源(Custom Resource Definition，CRDs)來負責管理與部署 Prometheus Server。而 Operator 會透過監聽這些自定義資源的事件變化來做對應處理。</li><li><strong>Prometheus Server</strong>：由 Operator 依據一個自定義資源 Prometheus 類型中，所描述的內容而部署的 Prometheus Server 叢集，可以將這個自定義資源看作是一種特別用來管理 Prometheus Server 的 StatefulSets 資源。</li></ul><pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata:  name: k8s  labels:    prometheus: k8sspec:  version: v2.3.0  replicas: 2  serviceMonitors:  - selector:      matchLabels:        k8s-app: kubelet...</code></pre><ul><li><strong>ServiceMonitor</strong>：一個 Kubernetes 自定義資源，該資源描述了 Prometheus Server 的 Target 列表，Operator 會監聽這個資源的變化來動態的更新 Prometheus Server 的 Scrape targets。而該資源主要透過 Selector 來依據 Labels 選取對應的 Service Endpoint，並讓 Prometheus Server 透過 Service 進行拉取(Pull) Metrics 資料。</li></ul><pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: kubelet  labels:    k8s-app: kubeletspec:  jobLabel: k8s-app  endpoints:  - port: cadvisor    interval: 30s # scrape the endpoint every 10 seconds    honorLabels: true  selector:    matchLabels:      k8s-app: kubelet  namespaceSelector:    matchNames:    - kube-system</code></pre><blockquote><p>這是一個抓取 Cadvisor metrics 的範例。</p></blockquote><ul><li><p><strong>Service</strong>：Kubernetes 中的 Service 資源，這邊主要用來對應 Kubernetes 中 Metrics Server Pod，然後提供給 ServiceMonitor 選取讓 Prometheus Server 拉取資料。在 Prometheus 術語中，可以稱為 Target，即被 Prometheus 監測的對象，如一個部署在 Kubernetes 上的 Node Exporter Service。</p></li><li><p><strong>Alertmanager</strong>：Prometheus Operator 不只提供 Prometheus Server 管理與部署，也包含了 AlertManager，並且一樣透過一個 Alertmanager 自定義資源來描述資訊，再由 Operator 依據描述內容部署 Alertmanager 叢集。</p></li></ul><pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1kind: Alertmanagermetadata:  name: main  labels:    alertmanager: mainspec:  replicas: 3...</code></pre><h2 id="部署-Prometheus-Operator"><a href="#部署-Prometheus-Operator" class="headerlink" title="部署 Prometheus Operator"></a>部署 Prometheus Operator</h2><p>本節將說明如何部署 Prometheus Operator 來管理 Kubernetes 上的 Prometheus 資源。</p><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>測試環境將需要一套 Kubernetes 叢集，作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>172.22.132.10</td><td>k8s-m1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.11</td><td>k8s-n1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.12</td><td>k8s-n2</td><td>8</td><td>16G</td></tr></tbody></table><blockquote><p>這邊<code>m</code> 為 K8s master，<code>n</code>為 K8s node。</p></blockquote><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>開始安裝前需要確保以下條件已達成：</p><ul><li><p>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</p></li><li><p>在 Kubernetes 叢集部署 Helm 與 Tiller server。</p></li></ul><pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller</code></pre><ul><li>在<code>k8s-m1</code>透過 kubectl 來建立 Ingress Controller 即可：</li></ul><pre><code class="sh">$ kubectl create ns ingress-nginx$ wget https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf -O ingress-controller.yml$ sed -i ingress-controller.yml &#39;s/192.16.35.10/172.22.132.10/g&#39;$ kubectl apply -f ingress-controller.yml.conf</code></pre><h3 id="部署-Prometheus-Operator-1"><a href="#部署-Prometheus-Operator-1" class="headerlink" title="部署 Prometheus Operator"></a>部署 Prometheus Operator</h3><p>Prometheus Operator 提供了多種方式部署至 Kubernetes 上，一般會使用手動(or 腳本)與 Helm 來進行部署。</p><h4 id="手動-腳本-部署"><a href="#手動-腳本-部署" class="headerlink" title="手動(腳本)部署"></a>手動(腳本)部署</h4><p>透過 Git 取得最新版本腳本：</p><pre><code class="sh">$ git clone https://github.com/camilb/prometheus-kubernetes.git$ cd prometheus-kubernetes</code></pre><p>接著執行<code>deploy</code>腳本來部署到 Kubernetes：</p><pre><code class="sh">$ ./deployCheck for uncommitted changesOK! No uncommitted changes detectedCreating &#39;monitoring&#39; namespace.Error from server (AlreadyExists): namespaces &quot;monitoring&quot; already exists1) AWS2) GCP3) Azure4) CustomPlease select your cloud provider:4Deploying on custom providers without persistenceSetting components versionEnter Prometheus Operator version [v0.19.0]:Enter Prometheus version [v2.2.1]:Enter Prometheus storage retention period in hours [168h]:Enter Prometheus storage volume size [40Gi]:Enter Prometheus memory request in Gi or Mi [1Gi]:Enter Grafana version [5.1.1]:Enter Alert Manager version [v0.15.0-rc.1]:Enter Node Exporter version [v0.16.0-rc.3]:Enter Kube State Metrics version [v1.3.1]:Enter Prometheus external Url [http://127.0.0.1:9090]:Enter Alertmanager external Url [http://127.0.0.1:9093]:Do you want to use NodeSelector  to assign monitoring components on dedicated nodes?Y/N [N]:Do you want to set up an SMTP relay?Y/N [N]:Do you want to set up slack alerts?Y/N [N]:# 這邊會跑一下部署階段，完成後要接著輸入一些資訊，如 Grafana username and passwdEnter Grafana administrator username [admin]:Enter Grafana administrator password: ******...Done</code></pre><blockquote><p>沒有輸入部分請直接按<code>Enter</code>。</p></blockquote><p>當確認看到 Done 後就可以查看 <code>monitoring</code> namespace：</p><pre><code class="sh">$ kubectl -n monitoring get poNAME                                  READY     STATUS    RESTARTS   AGEalertmanager-main-0                   2/2       Running   0          4malertmanager-main-1                   2/2       Running   0          3malertmanager-main-2                   2/2       Running   0          3mgrafana-568b569696-nltbh              2/2       Running   0          14skube-state-metrics-86467959c6-kxtl4   2/2       Running   0          3mnode-exporter-526nw                   1/1       Running   0          4mnode-exporter-c828w                   1/1       Running   0          4mnode-exporter-r2qq2                   1/1       Running   0          4mnode-exporter-s25x6                   1/1       Running   0          4mnode-exporter-xpgh7                   1/1       Running   0          4mprometheus-k8s-0                      1/2       Running   0          10sprometheus-k8s-1                      2/2       Running   0          10sprometheus-operator-f596c68cf-wrpqc   1/1       Running   0          4m</code></pre><p>查看 Kubernetes CRDs 與 SM：</p><pre><code class="sh">$ kubectl -n monitoring get crdNAME                                          AGEalertmanagers.monitoring.coreos.com           4mprometheuses.monitoring.coreos.com            4mservicemonitors.monitoring.coreos.com         4m$ kubectl -n monitoring get servicemonitorsNAME                      AGEalertmanager              1mkube-apiserver            1mkube-controller-manager   1mkube-dns                  1mkube-scheduler            1mkube-state-metrics        1mkubelet                   1mnode-exporter             1mprometheus                1mprometheus-operator       1m</code></pre><p>接著修改 Service 的 Grafana 的 Type：</p><pre><code class="sh">$ kubectl -n monitoring edit svc grafana# 修改成 NodePort</code></pre><blockquote><p>也可以建立 Ingress 來存取 Grafana。</p><pre><code class="yaml=">apiVersion: extensions/v1beta1kind: Ingressmetadata:  namespace: monitoring  name: grfana-ingress  annotations:    ingress.kubernetes.io/rewrite-target: /spec:  rules:  - host: grafana.k8s-local.k2r2bai.com    http:      paths:      - path: /        backend:          serviceName: grafana          servicePort: 3000</code></pre></blockquote><p>這邊也可以建立 Prometheus Ingress 來使用 Web-based console。</p><pre><code class="yaml=">apiVersion: extensions/v1beta1kind: Ingressmetadata:  namespace: monitoring  name: prometheus-ingress  annotations:    ingress.kubernetes.io/rewrite-target: /spec:  rules:  - host: prometheus.k8s-local.k2r2bai.com    http:      paths:      - path: /        backend:          serviceName: prometheus-k8s          servicePort: 9090</code></pre><p>最後就可以存取 Grafana 來查看 Metric 視覺化資訊了。</p><p><img src="https://i.imgur.com/39G6Zsm.png" alt></p><h4 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h4><p>首先透過 Helm 加入 coreos 的 repo：</p><pre><code class="sh">$ helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/</code></pre><p>然後透過 kubectl 建立一個 Namespace 來管理 Prometheus，並用 Helm 部署 Prometheus Operator：</p><pre><code class="sh">$ kubectl create namespace monitoring$ helm install coreos/prometheus-operator \    --name prometheus-operator \    --set rbacEnable=true \    --namespace=monitoring</code></pre><p>接著部署 Prometheus、AlertManager 與 Grafana：</p><pre><code class="sh"># Prometheus$ helm install coreos/prometheus --name prometheus \    --set serviceMonitorsSelector.app=prometheus \    --set ruleSelector.app=prometheus \    --namespace=monitoring# Alert Manager$ helm install coreos/alertmanager --name alertmanager --namespace=monitoring# Grafana$ helm install coreos/grafana --name grafana --namespace=monitoring</code></pre><p>部署 kube-prometheus 來提供 Kubernetes 監測的 Exporter 與 ServiceMonitor：</p><pre><code class="sh">$ helm install coreos/kube-prometheus --name kube-prometheus --namespace=monitoring</code></pre><p>完成後檢查安裝結果：</p><pre><code class="sh">$ kubectl -n monitoring get po,svcNAME                                                       READY     STATUS    RESTARTS   AGEpod/alertmanager-alertmanager-0                            2/2       Running   0          1mpod/alertmanager-kube-prometheus-0                         2/2       Running   0          31spod/grafana-grafana-77cfcdff66-jwxfp                       2/2       Running   0          1mpod/kube-prometheus-exporter-kube-state-56857b596f-knt8q   1/2       Running   0          21spod/kube-prometheus-exporter-kube-state-844bb6f589-n7xfg   1/2       Running   0          31spod/kube-prometheus-exporter-node-665kc                    1/1       Running   0          31spod/kube-prometheus-exporter-node-bjvbx                    1/1       Running   0          31spod/kube-prometheus-exporter-node-j8jf8                    1/1       Running   0          31spod/kube-prometheus-exporter-node-pxn8p                    1/1       Running   0          31spod/kube-prometheus-exporter-node-vft8b                    1/1       Running   0          31spod/kube-prometheus-grafana-57d5b4d79f-lq5cr               1/2       Running   0          31spod/prometheus-kube-prometheus-0                           3/3       Running   1          29spod/prometheus-operator-d75587d6-qhz4h                     1/1       Running   0          2mpod/prometheus-prometheus-0                                3/3       Running   1          1mNAME                                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGEservice/alertmanager                          ClusterIP   10.99.170.79     &lt;none&gt;        9093/TCP            1mservice/alertmanager-operated                 ClusterIP   None             &lt;none&gt;        9093/TCP,6783/TCP   1mservice/grafana-grafana                       ClusterIP   10.100.217.27    &lt;none&gt;        80/TCP              1mservice/kube-prometheus                       ClusterIP   10.102.165.173   &lt;none&gt;        9090/TCP            31sservice/kube-prometheus-alertmanager          ClusterIP   10.99.221.122    &lt;none&gt;        9093/TCP            32sservice/kube-prometheus-exporter-kube-state   ClusterIP   10.100.233.129   &lt;none&gt;        80/TCP              32sservice/kube-prometheus-exporter-node         ClusterIP   10.97.183.222    &lt;none&gt;        9100/TCP            32sservice/kube-prometheus-grafana               ClusterIP   10.110.134.52    &lt;none&gt;        80/TCP              32sservice/prometheus                            ClusterIP   10.105.229.141   &lt;none&gt;        9090/TCP            1mservice/prometheus-operated                   ClusterIP   None             &lt;none&gt;        9090/TCP            1m</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/coreos/prometheus-operator&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Prometheus Operator&lt;/a&gt; 是 CoreOS 開源的一套用於管理在 Kubernetes 上的 Prometheus 控制器，目標當然就是簡化部署與維護 Prometheus 上的事情，其架構如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://coreos.com/sites/default/files/inline-images/p1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://readailib.github.io/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="https://readailib.github.io/tags/DevOps/"/>
    
      <category term="Monitoring" scheme="https://readailib.github.io/tags/Monitoring/"/>
    
      <category term="CNCF" scheme="https://readailib.github.io/tags/CNCF/"/>
    
      <category term="Prometheus" scheme="https://readailib.github.io/tags/Prometheus/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus 介紹與基礎入門</title>
    <link href="https://readailib.github.io/2018/06/10/devops/prometheus-intro/"/>
    <id>https://readailib.github.io/2018/06/10/devops/prometheus-intro/</id>
    <published>2018-06-10T04:23:01.000Z</published>
    <updated>2019-03-05T14:37:50.217Z</updated>
    
    <content type="html"><![CDATA[<p>Prometheus 是一套開放式原始碼的<code>系統監控警報框架</code>與<code>TSDB(Time Series Database)</code>，該專案是由 SoundCloud 的工程師(前 Google 工程師)建立，Prometheus 啟發於 Google 的 Borgmon 監控系統。目前 Prometheus 已貢獻到 CNCF 成為孵化專案(2016-)，其受歡迎程度僅次於 Kubernetes。</p><a id="more"></a><p>Prometheus 具備了以下特性：</p><ul><li>多維度資料模型<ul><li>時間序列資料透過 Metric 名稱與鍵值(Key-value)來區分。</li><li>所有 Metrics 可以設定任意的多維標籤。</li><li>資料模型彈性度高，不需要刻意設定為以特定符號(ex: ,)分割。</li><li>可對資料模型進行聚合、切割與切片操作。</li><li>支援雙精度浮點數類型，標籤可以設定成 Unicode。</li></ul></li><li>靈活的查詢語言(PromQL)，如可進行加減乘除等。</li><li>不依賴分散式儲存，因為 Prometheus Server 是一個二進制檔，可在單個服務節點自主運行。</li><li>基於 HTTP 的 Pull 方式收集時序資料。</li><li>可以透過 Push Gateway 進行資料推送。</li><li>支援多種視覺化儀表板呈現，如 Grafana。</li><li>能透過服務發現(Service discovery)或靜態組態去獲取監控的 Targets。</li></ul><h2 id="Prometheus-架構"><a href="#Prometheus-架構" class="headerlink" title="Prometheus 架構"></a>Prometheus 架構</h2><p><img src="https://i.imgur.com/iJKoxdD.png" alt></p><p>Prometheus 生態圈中是由多個元件組成，其中有些是選擇性的元件：</p><ul><li><strong>Prometheus Server</strong>：收集與儲存時間序列資料，並提供 PromQL 查詢語言支援。</li><li><strong>Client Library</strong>：客戶端函式庫，提供語言開發來開發產生 Metrics 並曝露 Prometheus Server。當 Prometheus Server 來 Pull 時，直接返回即時狀態的 Metrics。</li><li><strong>Pushgateway</strong>：主要用於臨時性 Job 推送。這類 Job 存在期間較短，有可能 Prometheus 來 Pull 時就消失，因此透過一個閘道來推送。適合用於服務層面的 Metrics。</li><li><strong>Exporter</strong>：用來曝露已有第三方服務的 Metrics 給 Prometheus Server，即以 Client Library 開發的 HTTP server。</li><li><strong>AlertManager</strong>：接收來至 Prometheus Server 的 Alert event，並依據定義的 Notification 組態發送警報，ex: E-mail、Pagerduty、OpenGenie 與 Webhook 等等。</li></ul><h2 id="Prometheus-運作流程"><a href="#Prometheus-運作流程" class="headerlink" title="Prometheus 運作流程"></a>Prometheus 運作流程</h2><ol><li>Prometheus Server 定期從組態好的 Jobs 或者 Exporters 中拉取 Metrics，或者接收來自 Pushgateway 發送的 Metrics，又或者從其他的 Prometheus Server 中拉取 Metrics。</li><li>Prometheus Server 在 Local 儲存收集到的 Metrics，並運行已定義好的 alert.rules，然後紀錄新時間序列或者像 AlertManager 發送警報。</li><li>AlertManager 根據組態檔案來對接受到的 Alert event 進行處理，然後發送警報。</li><li>在視覺化介面呈現採集資料。</li></ol><p>Prometheus Server 拉取 Exporter 資料，然後透過 PromQL 語法進行查詢，再將資料給 Web UI or Dashboard。<br><img src="https://i.imgur.com/QkwEVge.png" alt></p><p>Prometheus Server 觸發 Alert Definition 定義的事件，並發送給 AelertManager。<br><img src="https://i.imgur.com/6V3RJOh.png" alt></p><p>AlertManager 依據設定發送警報給 E-mail、Slack 等等。<br><img src="https://i.imgur.com/mB789G2.png" alt></p><h2 id="Prometheus-資料模型與-Metric-類型"><a href="#Prometheus-資料模型與-Metric-類型" class="headerlink" title="Prometheus 資料模型與 Metric 類型"></a>Prometheus 資料模型與 Metric 類型</h2><p>本節將介紹 Prometheus 的資料模型與 Metrics 類型。</p><h3 id="資料模型"><a href="#資料模型" class="headerlink" title="資料模型"></a>資料模型</h3><p>Prometheus 儲存的資料為時間序列，主要以 Metrics name 以及一系列的唯一標籤(key-value)組成，不同標籤表示不同時間序列。模型資訊如下：</p><ul><li><strong>Metrics Name</strong>：該名稱通常用來表示 Metric 功能，例如 <code>http_requests_total</code>，即表示 HTTP 請求的總數。而 Metrics Name 是以 ASCII 字元、數字、英文、底線與冒號組成，並且要滿足<code>[a-zA-Z_:][a-zA-Z0-9_:]*</code> 正規表示法。</li><li><strong>標籤</strong>：用來識別同一個時間序列不同維度。如 <code>http_request_total{method=&quot;Get&quot;}</code>表示所有 HTTP 的 Get Request 數量，因此當 <code>method=&quot;Post&quot;</code> 時又是另一個新的 Metric。標籤也需要滿足<code>[a-zA-Z_:][a-zA-Z0-9_:]*</code> 正規表示法。</li><li><strong>樣本</strong>：實際的時間序列，每個序列包含一個 float64 值與一個毫秒的時間戳。</li><li><strong>格式</strong>：一般為<code>&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;,...}</code>，例如：<code>http_requests_total{method=&quot;POST&quot;,endpoint=&quot;/api/tracks&quot;}</code>。</li></ul><h3 id="Metrics-類型"><a href="#Metrics-類型" class="headerlink" title="Metrics 類型"></a>Metrics 類型</h3><p>Prometheus Client 函式庫支援了四種主要 Metric 類型：</p><ul><li><strong>Counter</strong>: 可被累加的 Metric，比如一個 HTTP Get 錯誤的出現次數。</li><li><strong>Gauge</strong>: 屬於瞬時、與時間無關的任意更動 Metric，如記憶體使用率。</li><li><strong>Histogram</strong>: 主要使用在表示一段時間範圍內的資料採樣。</li><li><strong>Summary</strong>： 類似 Histogram，用來表示一端時間範圍內的資料採樣總結。</li></ul><h2 id="Job-與-Instance"><a href="#Job-與-Instance" class="headerlink" title="Job 與 Instance"></a>Job 與 Instance</h2><p>Prometheus 中會將任意獨立資料來源(Target)稱為 Instance。而包含多個相同 Instance 的集合稱為 Job。如以下範例：</p><pre><code class="yml">- job: api-server    - instance 1: 1.2.3.4:5670    - instance 2: 1.2.3.4:5671    - instance 3: 5.6.7.8:5670    - instance 4: 5.6.7.8:5671</code></pre><ul><li><strong>Instance</strong>: 被抓取目標 URL 的<code>&lt;host&gt;:&lt;port&gt;</code>部分。</li><li><strong>Job</strong>: 一個同類型的 Instances 集合。(主要確保可靠性與擴展性)</li></ul><h2 id="Prometheus-簡單部署與使用"><a href="#Prometheus-簡單部署與使用" class="headerlink" title="Prometheus 簡單部署與使用"></a>Prometheus 簡單部署與使用</h2><p>Prometheus 官方提供了已建構完成的二進制執行檔可以下載，只需要至 <a href="https://prometheus.io/download/" target="_blank" rel="noopener">Download</a> 頁面下載即可。首先下載符合作業系統的檔案，這邊以 Linux 為例：</p><pre><code class="sh">$ wget https://github.com/prometheus/prometheus/releases/download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz$ tar xvfz prometheus-*.tar.gz$ tree prometheus-2.3.0.linux-amd64├── console_libraries # Web console templates│   ├── menu.lib│   └── prom.lib├── consoles # Web console templates│   ├── index.html.example│   ├── node-cpu.html│   ├── node-disk.html│   ├── node.html│   ├── node-overview.html│   ├── prometheus.html│   └── prometheus-overview.html├── LICENSE├── NOTICE├── prometheus     # Prometheus 執行檔├── prometheus.yml # Prometheus 設定檔└── promtool       # 2.x+ 版本用來將一些 rules 格式轉成 YAML 用。</code></pre><p>解壓縮完成後，編輯<code>prometheus.yml</code>檔案來調整設定：</p><pre><code class="yml">global:  scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間  external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。    monitor: &#39;codelab-monitor&#39;scrape_configs: # 設定 scrape jobs  - job_name: &#39;prometheus&#39;    scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。    static_configs:      - targets: [&#39;localhost:9090&#39;]</code></pre><p>完成後，直接執行 prometheus 檔案來啟動伺服器：</p><pre><code class="sh">$ ./prometheus --config.file=prometheus.yml --storage.tsdb.path /tmp/data...level=info ts=2018-06-19T08:46:37.42756438Z caller=main.go:500 msg=&quot;Server is ready to receive web requests.&quot;</code></pre><blockquote><p><code>--storage.tsdb.path</code> 預設會直接存放在<code>./data</code>底下。</p></blockquote><p>啟動後就可以瀏覽 <code>:9090</code> 來查看 Web-based console。</p><p><img src="https://i.imgur.com/qgi39CC.png" alt></p><p>另外也可以進入 <code>:9090/metrics</code> 查看 Export metrics 資訊，並且可以在 console 來查詢指定 Metrics，並以圖表呈現。</p><p><img src="https://i.imgur.com/Rv6XW6f.png" alt></p><p>Prometheus 提供了 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank" rel="noopener">Functional Expression Language</a> 進行查詢與聚合時間序列資料，比如用<code>sum(http_requests_total{method=&quot;GET&quot;} offset 5m)</code>來查看指定時間的資訊總和。</p><p>Prometheus 提供拉取第三方或者自己開發的 Exporter metrics 作為監測資料，這邊可以透過簡單的 <a href="https://github.com/prometheus/client_golang.git" target="_blank" rel="noopener">Go Client</a> 範例來簡單部署 Exporter：</p><pre><code class="sh">$ git clone https://github.com/prometheus/client_golang.git$ cd client_golang/examples/random$ go get -d$ go build</code></pre><p>完成後，開啟三個 Terminals 分別啟動以下 Exporter：</p><pre><code class="sh"># terminal 1$ ./random -listen-address=:8081# terminal 2$ ./random -listen-address=:8082# terminal 3$ ./random -listen-address=:8083</code></pre><blockquote><p>啟動後可以在<code>:8081</code>等 Ports 中查看 Metrics 資訊。</p></blockquote><p>確定沒問題後，修改<code>prometheus.yml</code>來新增 target，並重新啟動 Prometheus Server：</p><pre><code class="yaml">global:  scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間  external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。    monitor: &#39;codelab-monitor&#39;scrape_configs: # 設定 scrape jobs  - job_name: &#39;prometheus&#39;    scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。    static_configs:      - targets: [&#39;localhost:9090&#39;]  - job_name: &#39;example-random&#39;    scrape_interval: 5s    static_configs:      - targets: [&#39;localhost:8080&#39;, &#39;localhost:8081&#39;]        labels:          group: &#39;production&#39;      - targets: [&#39;localhost:8082&#39;]        labels:          group: &#39;canary&#39;</code></pre><p>啟動完成後，就可以 Web-console 的 Execute 執行以下來查詢：</p><pre><code class="sh">avg(rate(rpc_durations_seconds_count[5m])) by (job, service)</code></pre><p><img src="https://i.imgur.com/Bo7YGo5.png" alt></p><p>另外 Prometheus 也提供自定義 Group rules 來將指定的 Expression query 當作一個 Metric，這邊建立一個檔案<code>prometheus.rules.yml</code>，並新增以下內容：</p><pre><code class="yaml">groups:- name: example  rules:  - record: job_service:rpc_durations_seconds_count:avg_rate5m    expr: avg(rate(rpc_durations_seconds_count[5m])) by (job, service)</code></pre><p>接著修改<code>prometheus.yml</code>加入以下內容，並重新啟動 Prometheus Server：</p><pre><code class="yaml">global:  ...scrape_configs:  ...rule_files:  - &#39;prometheus.rules.yml&#39;</code></pre><blockquote><p><code>global</code> 與 <code>scrape_configs</code> 不做任何修改，只需加入<code>rule_files</code>即可，另外注意檔案路徑位置。</p></blockquote><p>正常啟動後，就可以看到新的 Metric 被加入。<br><img src="https://i.imgur.com/LhKcGVK.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Prometheus 是一套開放式原始碼的&lt;code&gt;系統監控警報框架&lt;/code&gt;與&lt;code&gt;TSDB(Time Series Database)&lt;/code&gt;，該專案是由 SoundCloud 的工程師(前 Google 工程師)建立，Prometheus 啟發於 Google 的 Borgmon 監控系統。目前 Prometheus 已貢獻到 CNCF 成為孵化專案(2016-)，其受歡迎程度僅次於 Kubernetes。&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://readailib.github.io/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="https://readailib.github.io/tags/DevOps/"/>
    
      <category term="Monitoring" scheme="https://readailib.github.io/tags/Monitoring/"/>
    
      <category term="CNCF" scheme="https://readailib.github.io/tags/CNCF/"/>
    
      <category term="Prometheus" scheme="https://readailib.github.io/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>以 Keystone 作為 Kubernetes 使用者認證</title>
    <link href="https://readailib.github.io/2018/05/30/kubernetes/k8s-integration-keystone/"/>
    <id>https://readailib.github.io/2018/05/30/kubernetes/k8s-integration-keystone/</id>
    <published>2018-05-30T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.224Z</updated>
    
    <content type="html"><![CDATA[<p>本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(<code>--experimental-keystone-url</code>, <code>--experimental-keystone-ca-file</code>)，並轉而使用 <a href="https://github.com/kubernetes/cloud-provider-openstack" target="_blank" rel="noopener">cloud-provider-openstack</a> 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。</p><a id="more"></a><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統以<code>Ubuntu 16.x</code>進行測試：</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.22.132.20</td><td>k8s</td><td>4</td><td>8G</td></tr><tr><td>172.22.132.21</td><td>keystone</td><td>4</td><td>8G</td></tr></tbody></table><blockquote><ul><li><code>k8s</code>為 all-in-one Kubernetes 節點(就只是個執行 kubeadm init 的節點)。</li><li><code>keystone</code>利用 DevStack 部署一台 all-in-one OpenStack。</li></ul></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p><ul><li><p><code>k8s</code>節點以 kubeadm 部署成 Kubernetes v1.9+ all-in-one 環境。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</p></li><li><p>在<code>k8s</code>節點安裝 openstack-client：</p></li></ul><pre><code class="sh">$ sudo apt-get update &amp;&amp; sudo  apt-get install -y python-pip$ export LC_ALL=C; sudo pip install python-openstackclient</code></pre><ul><li><code>keystone</code>節點部署成 OpenStack all-in-one 環境。請參考 <a href="https://docs.openstack.org/devstack/latest/" target="_blank" rel="noopener">DevStack</a>。</li></ul><h2 id="Kubernetes-與-Keystone-整合"><a href="#Kubernetes-與-Keystone-整合" class="headerlink" title="Kubernetes 與 Keystone 整合"></a>Kubernetes 與 Keystone 整合</h2><p>本節將逐節說明如何設定以整合 Keystone。</p><h3 id="建立-Keystone-User-與-Roles"><a href="#建立-Keystone-User-與-Roles" class="headerlink" title="建立 Keystone User 與 Roles"></a>建立 Keystone User 與 Roles</h3><p>當<code>keystone</code>節點的 OpenStack 部署完成後，進入到節點建立測試用 User 與 Roles：</p><pre><code class="sh">$ sudo su - stack$ cd devstack$ source openrc admin admin# 建立 Roles$ for role in &quot;k8s-admin&quot; &quot;k8s-viewer&quot; &quot;k8s-editor&quot;; do    openstack role create $role;  done# 建立 User$ openstack user create demo_editor --project demo --password secret$ openstack user create demo_admin --project demo --password secret# 加入 User 至 Roles$ openstack role add --user demo --project demo k8s-viewer$ openstack role add --user demo_editor --project demo k8s-editor$ openstack role add --user demo_admin --project demo k8s-admin</code></pre><h3 id="在-Kubernetes-安裝-Keystone-Webhook"><a href="#在-Kubernetes-安裝-Keystone-Webhook" class="headerlink" title="在 Kubernetes 安裝 Keystone Webhook"></a>在 Kubernetes 安裝 Keystone Webhook</h3><p>進入<code>k8s</code>節點，首先導入下載的檔案來源：</p><pre><code class="sh">$ export URL=&quot;https://kairen.github.io/files/openstack/keystone&quot;</code></pre><p>新增一些腳本，來提供導入不同使用者環境變數給 OpenStack Client 使用：</p><pre><code class="sh">$ export KEYSTONE_HOST=&quot;172.22.132.21&quot;$ export USER_PASSWORD=&quot;secret&quot;$ for n in &quot;admin&quot; &quot;demo&quot; &quot;demoadmin&quot; &quot;demoeditor&quot; &quot;altdemo&quot;; do    wget ${URL}/openrc-${n} -O ~/openrc-${n}    sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; ~/openrc-${n}    sed -i &quot;s/USER_PASSWORD/${USER_PASSWORD}/g&quot; ~/openrc-${n}  done</code></pre><p>下載 Keystone Webhook Policy 檔案，然後執行指令修改內容：</p><pre><code class="sh">$ sudo wget ${URL}/webhook-policy.json -O /etc/kubernetes/webhook-policy.json$ source ~/openrc-demo$ PROJECT_ID=$(openstack project list | awk &#39;/demo/ {print$2}&#39;)$ sudo sed -i &quot;s/PROJECT_ID/${PROJECT_ID}/g&quot; /etc/kubernetes/webhook-policy.json</code></pre><p>然後下載與部署 Keystone Webhook YAML 檔：</p><pre><code class="sh">$ wget ${URL}/keystone-webhook-ds.conf -O keystone-webhook-ds.yml$ KEYSTONE_HOST=&quot;172.22.132.21&quot;$ sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; keystone-webhook-ds.yml$ kubectl create -f keystone-webhook-ds.ymlconfigmap &quot;keystone-webhook-kubeconfig&quot; createddaemonset.apps &quot;keystone-auth-webhook&quot; created</code></pre><p>透過 kubectl 確認 Keystone Webhook 是否部署成功：</p><pre><code class="sh">$ kubectl -n kube-system get po -l component=k8s-keystoneNAME                          READY     STATUS    RESTARTS   AGEkeystone-auth-webhook-5qqwn   1/1       Running   0          1m</code></pre><p>透過 cURL 確認是否能夠正確存取：</p><pre><code class="sh">$ source ~/openrc-demo$ TOKEN=$(openstack token issue -f yaml -c id | awk &#39;{print $2}&#39;)$ cat &lt;&lt; EOF | curl -kvs -XPOST -d @- https://localhost:8443/webhook | python -mjson.tool{  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,  &quot;kind&quot;: &quot;TokenReview&quot;,  &quot;metadata&quot;: {    &quot;creationTimestamp&quot;: null  },  &quot;spec&quot;: {    &quot;token&quot;: &quot;$TOKEN&quot;  }}EOF# output{    &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,    &quot;kind&quot;: &quot;TokenReview&quot;,    &quot;metadata&quot;: {        &quot;creationTimestamp&quot;: null    },    &quot;spec&quot;: {        &quot;token&quot;: &quot;gAAAAABbFi1SacEPNstSuSuiBXiBG0Y_DikfbiR75j3P-CJ8CeaSKXa5kDQvun4LZUq8U6ehuW_RrQwi-N7j8t086uN6a4hLnPPGmvc6K_Iw0BZHZps7G1R5WniHZ8-WTUxtkMJROSz9eG7m33Bp18mvgx-P179QiwNYxLivf_rjnxePmvujNow&quot;    },    &quot;status&quot;: {        &quot;authenticated&quot;: true,        &quot;user&quot;: {            &quot;extra&quot;: {                &quot;alpha.kubernetes.io/identity/project/id&quot;: [                    &quot;3ebcb1da142d427db04b8df43f6cb76a&quot;                ],                &quot;alpha.kubernetes.io/identity/project/name&quot;: [                    &quot;demo&quot;                ],                &quot;alpha.kubernetes.io/identity/roles&quot;: [                    &quot;k8s-viewer&quot;,                    &quot;Member&quot;,                    &quot;anotherrole&quot;                ]            },            &quot;groups&quot;: [                &quot;3ebcb1da142d427db04b8df43f6cb76a&quot;            ],            &quot;uid&quot;: &quot;19748c0131504b87a4117e49c67383c6&quot;,            &quot;username&quot;: &quot;demo&quot;        }    }}</code></pre><h3 id="設定-kube-apiserver-使用-Webhook"><a href="#設定-kube-apiserver-使用-Webhook" class="headerlink" title="設定 kube-apiserver 使用 Webhook"></a>設定 kube-apiserver 使用 Webhook</h3><p>進入<code>k8s</code>節點，然後修改<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>檔案，加入以下內容：</p><pre><code class="yml">...spec:  containers:  - command:    ...    # authorization-mode 加入 Webhook    - --authorization-mode=Node,RBAC,Webhook    - --runtime-config=authentication.k8s.io/v1beta1=true    - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-auth    - --authorization-webhook-config-file=/srv/kubernetes/webhook-auth    - --authentication-token-webhook-cache-ttl=5m    volumeMounts:    ...    - mountPath: /srv/kubernetes/webhook-auth      name: webhook-auth-file      readOnly: true  volumes:  ...  - hostPath:      path: /srv/kubernetes/webhook-auth      type: File    name: webhook-auth-file</code></pre><p>完成後重新啟動 kubelet(或者等待 static pod 自己更新)：</p><pre><code class="sh">$ sudo systemctl restart kubelet</code></pre><h2 id="驗證部署結果"><a href="#驗證部署結果" class="headerlink" title="驗證部署結果"></a>驗證部署結果</h2><p>進入<code>k8s</code>節點，然後設定 kubectl context 並使用 openstack provider：</p><pre><code class="sh">$ kubectl config set-credentials openstack --auth-provider=openstack$ kubectl config \    set-context --cluster=kubernetes \    --user=openstack \    openstack@kubernetes \    --namespace=default$ kubectl config use-context openstack@kubernetes</code></pre><p>測試 demo 使用者的存取權限是否有被限制：</p><pre><code class="sh">$ source ~/openrc-demo$ kubectl get podsNo resources found.$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:  name: nginx-podspec:  restartPolicy: Never  containers:  - image: nginx    name: nginx-appEOF# outputError from server (Forbidden): error when creating &quot;STDIN&quot;: pods is forbidden: User &quot;demo&quot; cannot create pods in the namespace &quot;default&quot;</code></pre><blockquote><p>由於 demo 只擁有 k8s-viewer role，因此只能進行 get, list 與 watch API。</p></blockquote><p>測試 demo_editor 使用者是否能夠建立 Pod：</p><pre><code class="sh">$ source ~/openrc-demoeditor$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:  name: nginx-podspec:  restartPolicy: Never  containers:  - image: nginx    name: nginx-appEOF# outputpod &quot;nginx-pod&quot; created</code></pre><blockquote><p>這邊可以看到 demo_editor 因為擁有 k8s-editor role，因此能夠執行 create API。</p></blockquote><p>測試 alt_demo 是否被禁止存取任何 API：</p><pre><code class="sh">$ source ~/openrc-altdemo$ kubectl get poError from server (Forbidden): pods is forbidden: User &quot;alt_demo&quot; cannot list pods in the namespace &quot;default&quot;</code></pre><blockquote><p>由於 alt_demo 不具備任何 roles，因此無法存取任何 API。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(&lt;code&gt;--experimental-keystone-url&lt;/code&gt;, &lt;code&gt;--experimental-keystone-ca-file&lt;/code&gt;)，並轉而使用 &lt;a href=&quot;https://github.com/kubernetes/cloud-provider-openstack&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cloud-provider-openstack&lt;/a&gt; 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Keystone" scheme="https://readailib.github.io/tags/Keystone/"/>
    
  </entry>
  
  <entry>
    <title>在 AWS 上建立跨地區的 Kubernetes Federation 叢集</title>
    <link href="https://readailib.github.io/2018/04/21/kubernetes/aws-k8s-federation/"/>
    <id>https://readailib.github.io/2018/04/21/kubernetes/aws-k8s-federation/</id>
    <published>2018-04-21T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.219Z</updated>
    
    <content type="html"><![CDATA[<p>本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示：</p><p><img src="/images/kops-fed/fed-clusters.png" alt></p><a id="more"></a><p>本次安裝的軟體版本：</p><ul><li>Kubernetes v1.9.3</li><li>kops v1.9.0</li><li>kubefed v1.10</li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>測試環境為 AWS EC2 虛擬機器，共有三組叢集：</p><p>US West(Oregon) 叢集，也是 Federation 控制平面叢集：</p><table><thead><tr><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>us-west-m1</td><td>1</td><td>2G</td></tr><tr><td>us-west-n1</td><td>1</td><td>2G</td></tr><tr><td>us-west-n2</td><td>1</td><td>2G</td></tr></tbody></table><p>US East(Ohio) 叢集:</p><table><thead><tr><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>us-east-m1</td><td>1</td><td>2G</td></tr><tr><td>us-east-n1</td><td>1</td><td>2G</td></tr><tr><td>us-east-n2</td><td>1</td><td>2G</td></tr></tbody></table><p>Asia Pacific(Tokyo) 叢集:</p><table><thead><tr><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>ap-northeast-m1</td><td>1</td><td>2G</td></tr><tr><td>ap-northeast-n1</td><td>1</td><td>2G</td></tr><tr><td>ap-northeast-n2</td><td>1</td><td>2G</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始前，需要先安裝下列工具到操作機器上來提供使用：</p><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a>：用來操作部署完成的 Kubernetes 叢集。</li><li><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">kops</a>：用來部署與管理公有雲上的 Kubernetes 叢集。</li></ul><p>Mac OS X：</p><pre><code class="sh">$ brew update &amp;&amp; brew install kops</code></pre><p>Linux distro：</p><pre><code class="sh">$ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops</code></pre><ul><li><a href="https://github.com/kubernetes/federation" target="_blank" rel="noopener">kubefed</a>：用來建立 Federation 控制平面與管理 Federation 叢集的工具。</li></ul><p>Mac OS X：</p><pre><code class="sh">$ git clone https://github.com/kubernetes/federation.git $GOPATH/src/k8s.io/federation$ cd $GOPATH/src/k8s.io/federation$ make quick-release$ cp _output/dockerized/bin/linux/amd64/kubefed /usr/local/bin/kubefed</code></pre><p>Linux distro：</p><pre><code class="sh">$ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz$ tar xvf federation-client-linux-amd64.tar.gz$ cp federation/client/bin/kubefed /usr/local/bin/$ kubefed versionClient Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}</code></pre><ul><li><a href="https://aws.amazon.com/cli/?nc1=h_ls" target="_blank" rel="noopener">AWS CLI</a>：用來操作 AWS 服務的工具。</li></ul><pre><code class="sh">$ sudo pip install awscli$ aws --versionaws-cli/1.15.4</code></pre><p>上述工具完成後，我們還要準備一下資訊：</p><ul><li>申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。<blockquote><p>一般來說只需開啟 S3、Route53、EC2、EBS、ELB 與 VPC 就好，但由於偷懶就全開。以下為各 AWS 服務在本次安裝的用意：</p><ul><li>IAM: 提供身份認證與存取管理。</li><li>EC2: Kubernetes 叢集部署的虛擬機環境。</li><li>ELB: Kubernetes 元件與 Service 負載平衡。</li><li>Route53: 提供 Public domain 存取 Kubernetes 環境。</li><li>S3: 儲存 Kops 狀態。</li><li>VPC: 提供 Kubernetes 與 EC2 的網路環境。</li></ul></blockquote></li></ul><p><img src="/images/kops/iam-user2.png" alt></p><ul><li>擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。</li></ul><h2 id="部署-Kubernetes-Federation-叢集"><a href="#部署-Kubernetes-Federation-叢集" class="headerlink" title="部署 Kubernetes Federation 叢集"></a>部署 Kubernetes Federation 叢集</h2><p>本節將說明如何利用自己撰寫好的腳本 <a href="https://github.com/kairen/aws-k8s-federation" target="_blank" rel="noopener">aws-k8s-federation</a> 來部署 Kubernetes 叢集與 Federation 叢集。首先在操作節點下載：</p><pre><code class="sh">$ git clone https://github.com/kairen/aws-k8s-federation$ cd aws-k8s-federation$ cp .env.sample .env</code></pre><p>編輯<code>.env</code>檔案來提供後續腳本的環境變數：</p><pre><code class="sh"># 你的 Domain Name(這邊為 &lt;hoste_dzone_name&gt;.&lt;domain_name&gt;)export DOMAIN_NAME=&quot;k8s.example.com&quot;# Regions and zonesexport US_WEST_REGION=&quot;us-west-2&quot;export US_EAST_REGION=&quot;us-east-2&quot;export AP_NORTHEAST_REGION=&quot;ap-northeast-1&quot;export ZONE=&quot;a&quot;# Cluster contexts nameexport FED_CONTEXT=&quot;aws-fed&quot;export US_WEST_CONTEXT=&quot;us-west.${DOMAIN_NAME}&quot;export US_EAST_CONTEXT=&quot;us-east.${DOMAIN_NAME}&quot;export AP_NORTHEAST_CONTEXT=&quot;ap-northeast.${DOMAIN_NAME}&quot;# S3 buckets nameexport US_WEST_BUCKET_NAME=&quot;us-west-k8s&quot;export US_EAST_BUCKET_NAME=&quot;us-east-k8s&quot;export AP_NORTHEAST_BUCKET_NAME=&quot;ap-northeast-k8s&quot;# Get domain name idexport HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \       | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \       | sed &#39;s/\/hostedzone\///&#39;)# Kubernetes master and node size, and node count.export MASTER_SIZE=&quot;t2.micro&quot;export NODE_SIZE=&quot;t2.micro&quot;export NODE_COUNT=&quot;2&quot;# Federation simple apps deploy and service nameexport DNS_RECORD_PREFIX=&quot;nginx&quot;export SERVICE_NAME=&quot;nginx&quot;</code></pre><h3 id="建立-Route53-Hosted-Zone"><a href="#建立-Route53-Hosted-Zone" class="headerlink" title="建立 Route53 Hosted Zone"></a>建立 Route53 Hosted Zone</h3><p>首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey：</p><pre><code class="sh">$ aws configureAWS Access Key ID [****************QGEA]:AWS Secret Access Key [****************zJ+w]:Default region name [None]:Default output format [None]:</code></pre><blockquote><p>設定的 Keys 可以在<code>~/.aws/credentials</code>找到。</p></blockquote><p>接著需要在 Route53 建立一個 Hosted Zone，並在 Domain Name 供應商上設定 <code>NameServers</code>：</p><pre><code class="sh">$ ./0-create-hosted-domain.sh# output...{    &quot;HostedZone&quot;: {        &quot;ResourceRecordSetCount&quot;: 2,        &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;,        &quot;Config&quot;: {            &quot;PrivateZone&quot;: false        },        &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;,        &quot;Name&quot;: &quot;k8s.example.com.&quot;    },    &quot;DelegationSet&quot;: {        &quot;NameServers&quot;: [            &quot;ns-1547.awsdns-01.co.uk&quot;,            &quot;ns-1052.awsdns-03.org&quot;,            &quot;ns-886.awsdns-46.net&quot;,            &quot;ns-164.awsdns-20.com&quot;        ]    },    &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;,    &quot;ChangeInfo&quot;: {        &quot;Status&quot;: &quot;PENDING&quot;,        &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;,        &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot;    }}</code></pre><p>之後將上述<code>NameServers</code>新增至自己的 Domain name 的 record 中，如 Godaddy：</p><p><img src="/images/kops-fed/godday-ns.png" alt></p><h3 id="在每個-Region-建立-Kubernetes-叢集"><a href="#在每個-Region-建立-Kubernetes-叢集" class="headerlink" title="在每個 Region 建立 Kubernetes 叢集"></a>在每個 Region 建立 Kubernetes 叢集</h3><p>當 Hosted Zone 建立完成後，就可以接著建立每個 Region 的 Kubernetes 叢集，這邊腳本已包含建立叢集與 S3 Bucket 指令，因此只需要執行以下腳本即可：</p><pre><code class="sh">$ ./1-create-clusters.sh....Cluster is starting.  It should be ready in a few minutes....</code></pre><blockquote><p>這邊會需要等待一點時間進行初始化與部署，也可以到 AWS Console 查看狀態。</p></blockquote><p>完成後，即可透過 kubectl 來操作叢集：</p><pre><code class="sh">$ ./us-east/kc get no+ kubectl --context=us-east.k8s.example.com get noNAME                                          STATUS    ROLES     AGE       VERSIONip-172-20-43-26.us-east-2.compute.internal    Ready     node      1m        v1.9.3ip-172-20-56-167.us-east-2.compute.internal   Ready     master    3m        v1.9.3ip-172-20-63-133.us-east-2.compute.internal   Ready     node      2m        v1.9.3$ ./ap-northeast/kc get no+ kubectl --context=ap-northeast.k8s.example.com get noNAME                                               STATUS    ROLES     AGE       VERSIONip-172-20-42-184.ap-northeast-1.compute.internal   Ready     master    2m        v1.9.3ip-172-20-52-176.ap-northeast-1.compute.internal   Ready     node      20s       v1.9.3ip-172-20-56-88.ap-northeast-1.compute.internal    Ready     node      22s       v1.9.3$ ./us-west/kc get no+ kubectl --context=us-west.k8s.example.com get noNAME                                          STATUS    ROLES     AGE       VERSIONip-172-20-33-22.us-west-2.compute.internal    Ready     node      1m        v1.9.3ip-172-20-55-237.us-west-2.compute.internal   Ready     master    2m        v1.9.3ip-172-20-63-77.us-west-2.compute.internal    Ready     node      35s       v1.9.3</code></pre><h3 id="建立-Kubernetes-Federation-叢集"><a href="#建立-Kubernetes-Federation-叢集" class="headerlink" title="建立 Kubernetes Federation 叢集"></a>建立 Kubernetes Federation 叢集</h3><p>當三個地區的叢集建立完成後，接著要在 US West 的叢集上部署 Federation 控制平面元件：</p><pre><code class="sh">$ ./2-init-federation.sh...Federation API server is running at: abba6864f490111e8b4bd028106a7a79-793027324.us-west-2.elb.amazonaws.com$ ./us-west/kc -n federation-system get po+ kubectl --context=us-west.k8s.example.com -n federation-system get poNAME                                  READY     STATUS    RESTARTS   AGEapiserver-5d46898995-tmzvl            2/2       Running   0          1mcontroller-manager-6cc78c68d5-2pbg5   0/1       Error     3          1m</code></pre><p>這邊會發現<code>controller-manager</code>會一直掛掉，這是因為它需要取得 AWS 相關權限，因此需要透過 Patch 方式來把 AccessKey 與 SecretKey 注入到 Deployment 中：</p><pre><code class="sh">$ ./3-path-federation.shSwitched to context &quot;us-west.k8s.example.com&quot;.deployment &quot;controller-manager&quot; patched$ ./us-west/kc -n federation-system get po+ kubectl --context=us-west.k8s.example.com -n federation-system get poNAME                                  READY     STATUS        RESTARTS   AGEapiserver-5d46898995-tmzvl            2/2       Running       0          3mcontroller-manager-769bd95fbc-dkssr   1/1       Running       0          21s</code></pre><p>確認上述沒問題後，透過 kubectl 確認 contexts：</p><pre><code class="sh">$ kubectl config get-contextsCURRENT   NAME                           CLUSTER                        AUTHINFO                       NAMESPACE          ap-northeast.k8s.example.com   ap-northeast.k8s.example.com   ap-northeast.k8s.example.com          aws-fed                        aws-fed                        aws-fed          us-east.k8s.example.com        us-east.k8s.example.com        us-east.k8s.example.com*         us-west.k8s.example.com        us-west.k8s.example.com        us-west.k8s.example.com</code></pre><p>接著透過以下腳本來加入<code>us-west</code>叢集至 aws-fed 的 Federation 中：</p><pre><code class="sh">$ ./4-join-us-west.sh+ kubectl config use-context aws-fedSwitched to context &quot;aws-fed&quot;.+ kubefed join us-west --host-cluster-context=us-west.k8s.example.com --cluster-context=us-west.k8s.example.comcluster &quot;us-west&quot; created</code></pre><p>加入<code>ap-northeast</code>叢集至 aws-fed 的 Federation 中：</p><pre><code class="sh">$ ./5-join-ap-northeast.sh+ kubectl config use-context aws-fedSwitched to context &quot;aws-fed&quot;.+ kubefed join ap-northeast --host-cluster-context=us-west.k8s.example.com --cluster-context=ap-northeast.k8s.example.comcluster &quot;ap-northeast&quot; created</code></pre><p>加入<code>us-east</code>叢集至 aws-fed 的 Federation 中：</p><pre><code class="sh">$ ./6-join-us-east.sh+ kubectl config use-context aws-fedSwitched to context &quot;aws-fed&quot;.+ kubefed join us-east --host-cluster-context=us-west.k8s.example.com --cluster-context=us-east.k8s.example.comcluster &quot;us-east&quot; created</code></pre><p>完成後，在 Federation 建立 Federated Namespace，並列出叢集：</p><pre><code class="sh">$ ./7-create-fed-ns.sh+ kubectl --context=aws-fed create namespace defaultnamespace &quot;default&quot; created+ kubectl --context=aws-fed get clustersNAME           AGEap-northeast   2mus-east        1mus-west        2m</code></pre><p>完成這些過程表示你已經建立了一套 Kubernetes Federation 叢集了，接下來就可以進行測試。</p><h2 id="測試叢集"><a href="#測試叢集" class="headerlink" title="測試叢集"></a>測試叢集</h2><p>首先建立一個簡單的 Nginx 來提供服務的測試，這邊可以透過以下腳本達成：</p><pre><code class="sh">$ ./8-deploy-fed-nginx.sh+ cat+ kubectl --context=aws-fed apply -f -deployment &quot;nginx&quot; created+ cat+ kubectl --context=aws-fed apply -f -service &quot;nginx&quot; created$ kubectl get deploy,svcNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEdeploy/nginx   3         3         3            3           3mNAME        TYPE           CLUSTER-IP   EXTERNAL-IP        PORT(S)   AGEsvc/nginx   LoadBalancer   &lt;none&gt;       a4d86547a4903...   80/TCP    2m</code></pre><blockquote><p>這裡的 nginx deployment 有設定<code>deployment-preferences</code>，因此在 scale 時會依據下面資訊來分配：</p><pre><code class="sh">{       &quot;rebalance&quot;: true,       &quot;clusters&quot;: {         &quot;us-west&quot;: {           &quot;minReplicas&quot;: 2,           &quot;maxReplicas&quot;: 10,           &quot;weight&quot;: 200         },         &quot;us-east&quot;: {           &quot;minReplicas&quot;: 0,           &quot;maxReplicas&quot;: 2,           &quot;weight&quot;: 150         },         &quot;ap-northeast&quot;: {           &quot;minReplicas&quot;: 1,           &quot;maxReplicas&quot;: 5,           &quot;weight&quot;: 150         }       }     }</code></pre></blockquote><p>檢查每個叢集的 Pod：</p><pre><code class="sh"># us-west context(這邊策略為 2 - 10)$ ./us-west/kc get po+ kubectl --context=us-west.k8s.example.com get poNAME                     READY     STATUS    RESTARTS   AGEnginx-679dc9c764-4x78c   1/1       Running   0          3mnginx-679dc9c764-fzv9z   1/1       Running   0          3m# us-east context(這邊策略為 0 - 2)$ ./us-east/kc get po+ kubectl --context=us-east.k8s.example.com get poNo resources found.# ap-northeast context(這邊策略為 1 - 5)$ ./ap-northeast/kc get po+ kubectl --context=ap-northeast.k8s.example.com get poNAME                     READY     STATUS    RESTARTS   AGEnginx-679dc9c764-hmwzq   1/1       Running   0          4m</code></pre><p>透過擴展副本數來查看分配狀況：</p><pre><code class="sh">$ ./9-scale-fed-nginx.sh+ kubectl --context=aws-fed scale deploy nginx --replicas=10deployment &quot;nginx&quot; scaled$ kubectl get deployNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx     10        10        10           10          8m</code></pre><p>再次檢查每個叢集的 Pod：</p><pre><code class="sh"># us-west context(這邊策略為 2 - 10)$ ./us-west/kc get po+ kubectl --context=us-west.k8s.example.com get poNAME                     READY     STATUS    RESTARTS   AGEnginx-679dc9c764-4x78c   1/1       Running   0          8mnginx-679dc9c764-7958k   1/1       Running   0          50snginx-679dc9c764-fzv9z   1/1       Running   0          8mnginx-679dc9c764-j6kc9   1/1       Running   0          50snginx-679dc9c764-t6rvj   1/1       Running   0          50s# us-east context(這邊策略為 0 - 2)$ ./us-east/kc get po+ kubectl --context=us-east.k8s.example.com get poNAME                     READY     STATUS    RESTARTS   AGEnginx-679dc9c764-8t7qz   1/1       Running   0          1mnginx-679dc9c764-zvqmx   1/1       Running   0          1m# ap-northeast context(這邊策略為 1 - 5)$ ./ap-northeast/kc get po+ kubectl --context=ap-northeast.k8s.example.com get poNAME                     READY     STATUS    RESTARTS   AGEnginx-679dc9c764-f79v7   1/1       Running   0          1mnginx-679dc9c764-hmwzq   1/1       Running   0          9mnginx-679dc9c764-vj7hb   1/1       Running   0          1m</code></pre><blockquote><p>可以看到結果符合我們預期範圍內。</p></blockquote><p>最後因為服務是透過 ELB 來提供，為了統一透過 Domain name 存取相同服務，這邊更新 Hosted Zone Record 來轉發：</p><pre><code class="sh">$ ./10-update-fed-nginx-record.sh</code></pre><p>完成後透過 cURL 工作來測試：</p><pre><code class="sh">$ curl nginx.k8s.example.com...&lt;title&gt;Welcome to nginx!&lt;/title&gt;...</code></pre><p>最後透過該腳本來清楚叢集與 AWS 服務上建立的東西：</p><pre><code class="sh">$ ./99-purge.sh</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/kops-fed/fed-clusters.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="AWS" scheme="https://readailib.github.io/tags/AWS/"/>
    
      <category term="Kops" scheme="https://readailib.github.io/tags/Kops/"/>
    
      <category term="Federation" scheme="https://readailib.github.io/tags/Federation/"/>
    
  </entry>
  
  <entry>
    <title>使用 Kops 部署 Kubernetes 至公有雲(AWS)</title>
    <link href="https://readailib.github.io/2018/04/18/kubernetes/deploy/kops-aws/"/>
    <id>https://readailib.github.io/2018/04/18/kubernetes/deploy/kops-aws/</id>
    <published>2018-04-18T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.220Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">Kops</a> 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。</p><p>本次安裝的軟體版本：</p><ul><li>Kubernetes v1.9.3</li><li>Kops v1.9.0</li></ul><a id="more"></a><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始使用 Kops 前，需要先安裝下列工具到操作機器上來提供使用：</p><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a>：用來操作部署完成的 Kubernetes 叢集。</li><li><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">kops</a>：本次使用工具，用來部署與管理公有雲上的 Kubernetes 叢集。</li></ul><p>Mac OS X：</p><pre><code class="sh">$ brew update &amp;&amp; brew install kops</code></pre><p>Linux distro：</p><pre><code class="sh">$ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops</code></pre><ul><li><a href="https://aws.amazon.com/cli/?nc1=h_ls" target="_blank" rel="noopener">AWS CLI</a>：用來操作 AWS 服務的工具。</li></ul><pre><code class="sh">$ sudo pip install awscli$ aws --versionaws-cli/1.15.4</code></pre><p>上述工具完成後，我們還要準備一下資訊：</p><ul><li>申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。<blockquote><p>一般來說只需開啟 S3、Route53、EC2、EBS 與 ELB 就好，但由於偷懶就全開。</p></blockquote></li></ul><p><img src="/images/kops/iam-user2.png" alt></p><ul><li>擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。</li></ul><h2 id="建立-S3-Bucket-與-Route53-Hosted-Zone"><a href="#建立-S3-Bucket-與-Route53-Hosted-Zone" class="headerlink" title="建立 S3 Bucket 與 Route53 Hosted Zone"></a>建立 S3 Bucket 與 Route53 Hosted Zone</h2><p>首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey：</p><pre><code class="sh">$ aws configureAWS Access Key ID [****************QGEA]:AWS Secret Access Key [****************zJ+w]:Default region name [None]:Default output format [None]:</code></pre><blockquote><p>設定的 Keys 可以在<code>~/.aws/credentials</code>找到。</p></blockquote><p>完成後建立一個 S3 bucket 用來儲存 Kops 狀態：</p><pre><code class="sh">$ aws s3 mb s3://kops-k8s-1 --region us-west-2make_bucket: kops-k8s-1</code></pre><blockquote><p>這邊 region 可自行選擇，這邊選用 Oregon。</p></blockquote><p>接著建立一個 Route53 Hosted Zone：</p><pre><code class="sh">$ aws route53 create-hosted-zone \    --name k8s.example.com \    --caller-reference $(date &#39;+%Y-%m-%d-%H:%M&#39;)# output{    &quot;HostedZone&quot;: {        &quot;ResourceRecordSetCount&quot;: 2,        &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;,        &quot;Config&quot;: {            &quot;PrivateZone&quot;: false        },        &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;,        &quot;Name&quot;: &quot;k8s.example.com.&quot;    },    &quot;DelegationSet&quot;: {        &quot;NameServers&quot;: [            &quot;ns-1547.awsdns-01.co.uk&quot;,            &quot;ns-1052.awsdns-03.org&quot;,            &quot;ns-886.awsdns-46.net&quot;,            &quot;ns-164.awsdns-20.com&quot;        ]    },    &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;,    &quot;ChangeInfo&quot;: {        &quot;Status&quot;: &quot;PENDING&quot;,        &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;,        &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot;    }}</code></pre><blockquote><p>請修改<code>--name</code>為自己所擁有的 domain name。</p></blockquote><p>之後將上述<code>NameServers</code>新增至自己的 Domain name 的 record 中，如 Godaddy：</p><p><img src="/images/kops/route53-hostedzone.png" alt></p><h2 id="部署-Kubernetes-叢集"><a href="#部署-Kubernetes-叢集" class="headerlink" title="部署 Kubernetes 叢集"></a>部署 Kubernetes 叢集</h2><p>當上述階段完成後，在自己機器建立 SSH key，就可以使用 Kops 來建立 Kubernetes 叢集：</p><pre><code class="sh">$ ssh-keygen -t rsa$ kops create cluster \    --name=k8s.example.com \    --state=s3://kops-k8s-1 \    --zones=us-west-2a \    --master-size=t2.micro \    --node-size=t2.micro \    --node-count=2 \    --dns-zone=k8s.example.com# output...Finally configure your cluster with: kops update cluster k8s.example.com --yes</code></pre><p>若過程沒有發生錯誤的話，最後會提示再執行 update 來正式進行部署：</p><pre><code class="sh">$ kops update cluster k8s.example.com --state=s3://kops-k8s-1 --yes# output...Cluster is starting.  It should be ready in a few minutes.</code></pre><p>當看到上述資訊時，表示叢集已建立，這時候等待環境初始化完成後就可以使用 kubectl 來操作：</p><pre><code class="sh">$ kubectl get nodeNAME                                          STATUS    ROLES     AGE       VERSIONip-172-20-32-194.us-west-2.compute.internal   Ready     master    1m        v1.9.3ip-172-20-32-21.us-west-2.compute.internal    Ready     node      22s       v1.9.3ip-172-20-54-100.us-west-2.compute.internal   Ready     node      28s       v1.9.3</code></pre><h2 id="測試"><a href="#測試" class="headerlink" title="測試"></a>測試</h2><p>完成後就可以進行功能測試，這邊簡單建立 Nginx app：</p><pre><code class="sh">$ kubectl run nginx --image nginx --port 80$ kubectl expose deploy nginx --type=LoadBalancer --port 80$ kubectl get po,svcNAME                        READY     STATUS    RESTARTS   AGEpo/nginx-7587c6fdb6-7qtlr   1/1       Running   0          50sNAME             TYPE           CLUSTER-IP    EXTERNAL-IP        PORT(S)        AGEsvc/kubernetes   ClusterIP      100.64.0.1    &lt;none&gt;             443/TCP        8msvc/nginx        LoadBalancer   100.68.96.3   ad99f206f486e...   80:30174/TCP   28s</code></pre><p>這邊會看到<code>EXTERNAL-IP</code>會直接透過 AWS ELB 建立一個 Load Balancer，這時只要更新 Route53 的 record set 就可以存取到服務：</p><pre><code class="sh">$ export DOMAIN_NAME=k8s.example.com$ export NGINX_LB=$(kubectl get svc/nginx \  --template=&quot;{{range .status.loadBalancer.ingress}} {{.hostname}} {{end}}&quot;)$ cat &lt;&lt;EOF &gt; dns-record.json{  &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;,  &quot;Changes&quot;: [    {      &quot;Action&quot;: &quot;UPSERT&quot;,      &quot;ResourceRecordSet&quot;: {        &quot;Name&quot;: &quot;nginx.${DOMAIN_NAME}&quot;,        &quot;Type&quot;: &quot;CNAME&quot;,        &quot;Region&quot;: &quot;us-west-2&quot;,        &quot;TTL&quot;: 300,        &quot;SetIdentifier&quot;: &quot;us-west-2&quot;,        &quot;ResourceRecords&quot;: [          {            &quot;Value&quot;: &quot;${NGINX_LB}&quot;          }        ]      }    }  ]}EOF$ export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \       | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \       | sed &#39;s/\/hostedzone\///&#39;)$ aws route53 change-resource-record-sets \    --hosted-zone-id ${HOSTED_ZONE_ID} \    --change-batch file://dns-record.json# output{    &quot;ChangeInfo&quot;: {        &quot;Status&quot;: &quot;PENDING&quot;,        &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;,        &quot;SubmittedAt&quot;: &quot;2018-04-25T10:06:02.545Z&quot;,        &quot;Id&quot;: &quot;/change/C79MFJRHCF05R&quot;    }}</code></pre><p>完成後透過 cURL 工作來測試：</p><pre><code class="sh">$ curl nginx.k8s.example.com...&lt;title&gt;Welcome to nginx!&lt;/title&gt;...</code></pre><h2 id="刪除節點"><a href="#刪除節點" class="headerlink" title="刪除節點"></a>刪除節點</h2><p>當叢集測完後，可以利用以下指令來刪除：</p><pre><code class="sh">$ kops delete cluster \ --name=k8s.example.com \ --state=s3://kops-k8s-1 --yesDeleted cluster: &quot;k8s.k2r2bai.com&quot;$ aws s3 rb s3://kops-k8s-1 --forceremove_bucket: kops-k8s-1</code></pre><p>接著清除 Route53 所有 record 並刪除 hosted zone：</p><pre><code class="sh">$ aws route53 list-resource-record-sets \  --hosted-zone-id ${HOSTED_ZONE_ID} |jq -c &#39;.ResourceRecordSets[]&#39; |while read -r resourcerecordset; do  read -r name type &lt;&lt;&lt;$(echo $(jq -r &#39;.Name,.Type&#39; &lt;&lt;&lt;&quot;$resourcerecordset&quot;))  if [ $type != &quot;NS&quot; -a $type != &quot;SOA&quot; ]; then    aws route53 change-resource-record-sets \      --hosted-zone-id ${HOSTED_ZONE_ID} \      --change-batch &#39;{&quot;Changes&quot;:[{&quot;Action&quot;:&quot;DELETE&quot;,&quot;ResourceRecordSet&quot;:          &#39;&quot;$resourcerecordset&quot;&#39;        }]}&#39; \      --output text --query &#39;ChangeInfo.Id&#39;  fidone$ aws route53 delete-hosted-zone --id ${HOSTED_ZONE_ID}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/kops&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kops&lt;/a&gt; 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。&lt;/p&gt;
&lt;p&gt;本次安裝的軟體版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.9.3&lt;/li&gt;
&lt;li&gt;Kops v1.9.0&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="AWS" scheme="https://readailib.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>整合 Open LDAP 進行 Kubernetes 身份認證</title>
    <link href="https://readailib.github.io/2018/04/15/kubernetes/k8s-integration-ldap/"/>
    <id>https://readailib.github.io/2018/04/15/kubernetes/k8s-integration-ldap/</id>
    <published>2018-04-15T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.224Z</updated>
    
    <content type="html"><![CDATA[<p>本文將說明如何整合 OpenLDAP 來提供給 Kubernetes 進行使用者認證。Kubernetes 官方並沒有提供針對 LDAP 與 AD 的整合，但是可以藉由 <a href="https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication" target="_blank" rel="noopener">Webhook Token Authentication</a> 以及 <a href="https://kubernetes.io/docs/admin/authentication/#authenticating-proxy" target="_blank" rel="noopener">Authenticating Proxy</a> 來達到整合功能。概念是開發一個 HTTP Server 提供 POST Method 來塞入 Bearer Token，而該 HTTP Server 利用 LDAP library 檢索對應 Token 的 User 進行認證，成功後回傳該 User 的所有 Group 等資訊，而這時可以利用 Kubernetes 針對該 User 的 Group 設定對應的 RBAC role 進行權限控管。</p><a id="more"></a><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>192.16.35.11</td><td>k8s-m1</td><td>1</td><td>2G</td></tr><tr><td>192.16.35.12</td><td>k8s-n1</td><td>1</td><td>2G</td></tr><tr><td>192.16.35.13</td><td>k8s-n2</td><td>1</td><td>2G</td></tr><tr><td>192.16.35.20</td><td>ldap-server</td><td>1</td><td>1G</td></tr></tbody></table><blockquote><ul><li>這邊<code>m</code>為 K8s master，<code>n</code>為 K8s node。</li><li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li><li>可以下載 <a href="https://kairen.github.io/files/k8s-ldap/Vagrantfile" target="_blank" rel="noopener">Vagrantfile</a> 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。</li></ul></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p><ul><li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li></ul><pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><ul><li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h2 id="OpenLDAP-與-phpLDAPadmin"><a href="#OpenLDAP-與-phpLDAPadmin" class="headerlink" title="OpenLDAP 與 phpLDAPadmin"></a>OpenLDAP 與 phpLDAPadmin</h2><p>本節將說明如何部署、設定與操作 OpenLDAP。</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>進入<code>ldap-server</code>節點透過 Docker 來進行部署：</p><pre><code class="sh">$ docker run -d \    -p 389:389 -p 636:636 \    --env LDAP_ORGANISATION=&quot;Kubernetes LDAP&quot; \    --env LDAP_DOMAIN=&quot;k8s.com&quot; \    --env LDAP_ADMIN_PASSWORD=&quot;password&quot; \    --env LDAP_CONFIG_PASSWORD=&quot;password&quot; \    --name openldap-server \    osixia/openldap:1.2.0$ docker run -d \    -p 443:443 \    --env PHPLDAPADMIN_LDAP_HOSTS=192.16.35.20 \    --name phpldapadmin \    osixia/phpldapadmin:0.7.1</code></pre><blockquote><p>這邊為<code>cn=admin,dc=k8s,dc=com</code>為<code>admin</code> DN ，而<code>cn=admin,cn=config</code>為<code>config</code>。另外這邊僅做測試用，故不使用 Persistent Volumes，需要可以參考 <a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">Docker OpenLDAP</a>。</p></blockquote><p>完成後就可以透過瀏覽器來 <a href="https://192.16.35.20/" target="_blank" rel="noopener">phpLDAPadmin website</a>。這邊點選<code>Login</code>輸入 DN 與 Password。<br><img src="/images/k8s-ldap/ldap-login.png" alt></p><p>成功登入後畫面，這時可以自行新增其他資訊。<br><img src="/images/k8s-ldap/ldap-logined.png" alt></p><h3 id="建立-Kubenretes-Token-Schema"><a href="#建立-Kubenretes-Token-Schema" class="headerlink" title="建立 Kubenretes Token Schema"></a>建立 Kubenretes Token Schema</h3><p>進入<code>openldap-server 容器</code>，接著建立 Kubernetes token schema 物件的設定檔：</p><pre><code class="sh">$ docker exec -ti openldap-server sh$ mkdir ~/kubernetes_tokens$ cat &lt;&lt;EOF &gt; ~/kubernetes_tokens/kubernetesToken.schemaattributeType ( 1.3.6.1.4.1.18171.2.1.8        NAME &#39;kubernetesToken&#39;        DESC &#39;Kubernetes authentication token&#39;        EQUALITY caseExactIA5Match        SUBSTR caseExactIA5SubstringsMatch        SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )objectClass ( 1.3.6.1.4.1.18171.2.3        NAME &#39;kubernetesAuthenticationObject&#39;        DESC &#39;Object that may authenticate to a Kubernetes cluster&#39;        AUXILIARY        MUST kubernetesToken )EOF$ echo &quot;include /root/kubernetes_tokens/kubernetesToken.schema&quot; &gt; ~/kubernetes_tokens/schema_convert.conf$ slaptest -f ~/kubernetes_tokens/schema_convert.conf -F ~/kubernetes_tokensconfig file testing succeeded</code></pre><p>修改以下檔案內容，如以下所示：</p><pre><code class="sh">$ vim ~/kubernetes_tokens/cn=config/cn=schema/cn\=\{0\}kubernetestoken.ldif# AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.# CRC32 e502306edn: cn=kubernetestoken,cn=schema,cn=configobjectClass: olcSchemaConfigcn: kubernetestokenolcAttributeTypes: {0}( 1.3.6.1.4.1.18171.2.1.8 NAME &#39;kubernetesToken&#39; DESC &#39;Kubernetes authentication token&#39; EQUALITY caseExactIA5Match SUBSTR caseExa ctIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )olcObjectClasses: {0}( 1.3.6.1.4.1.18171.2.3 NAME &#39;kubernetesAuthenticationO bject&#39; DESC &#39;Object that may authenticate to a Kubernetes cluster&#39; AUXILIAR Y MUST kubernetesToken )</code></pre><p>新增 Schema 物件至 LDAP Server 中：</p><pre><code class="sh">$ cd ~/kubernetes_tokens/cn=config/cn=schema$ ldapadd -c -Y EXTERNAL -H ldapi:/// -f cn\=\{0\}kubernetestoken.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry &quot;cn=kubernetestoken,cn=schema,cn=config&quot;</code></pre><p>完成後查詢是否成功新增 Entry：</p><pre><code class="sh">$ ldapsearch -x -H ldap:/// -LLL -D &quot;cn=admin,cn=config&quot; -w password -b &quot;cn=schema,cn=config&quot; &quot;(objectClass=olcSchemaConfig)&quot; dn -ZEnter LDAP Password:dn: cn=schema,cn=config...dn: cn={14}kubernetestoken,cn=schema,cn=config</code></pre><h3 id="新增測試用-LDAP-Groups-與-Users"><a href="#新增測試用-LDAP-Groups-與-Users" class="headerlink" title="新增測試用 LDAP Groups 與 Users"></a>新增測試用 LDAP Groups 與 Users</h3><p>當上面 Schema 建立完成後，這邊需要新增一些測試用 Groups：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; groups.ldifdn: ou=People,dc=k8s,dc=comou: PeopleobjectClass: topobjectClass: organizationalUnitdescription: Parent object of all UNIX accountsdn: ou=Groups,dc=k8s,dc=comou: GroupsobjectClass: topobjectClass: organizationalUnitdescription: Parent object of all UNIX groupsdn: cn=kubernetes,ou=Groups,dc=k8s,dc=comcn: kubernetesgidnumber: 100memberuid: user1memberuid: user2objectclass: posixGroupobjectclass: topEOF$ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f groups.ldifadding new entry &quot;ou=People,dc=k8s,dc=com&quot;adding new entry &quot;ou=Groups,dc=k8s,dc=com&quot;adding new entry &quot;cn=kubernetes,ou=Groups,dc=k8s,dc=com&quot;</code></pre><p>Group 建立完成後再接著建立 User：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; users.ldifdn: uid=user1,ou=People,dc=k8s,dc=comcn: user1gidnumber: 100givenname: user1homedirectory: /home/users/user1loginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: user1uid: user1uidnumber: 1000userpassword: user1dn: uid=user2,ou=People,dc=k8s,dc=comhomedirectory: /home/users/user2loginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersoncn: user2givenname: user2sn: user2uid: user2uidnumber: 1001gidnumber: 100userpassword: user2EOF$ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f users.ldifadding new entry &quot;uid=user1,ou=People,dc=k8s,dc=com&quot;adding new entry &quot;uid=user2,ou=People,dc=k8s,dc=com&quot;</code></pre><p>這邊可以登入 phpLDAPadmin 查看，結果如以下所示：<br><img src="/images/k8s-ldap/ldap-entry.png" alt></p><p>確認沒問題後，將 User dump 至一個文字檔案中：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; users.txtdn: uid=user1,ou=People,dc=k8s,dc=comdn: uid=user2,ou=People,dc=k8s,dc=comEOF</code></pre><blockquote><p>這邊偷懶直接用 cat。</p></blockquote><p>執行以下腳本來更新每個 LDAP User 的 kubernetesToken：</p><pre><code class="sh">$ while read -r user; dofname=$(echo $user | grep -E -o &quot;uid=[a-z0-9]+&quot; | cut -d&quot;=&quot; -f2)token=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d &quot;=+/&quot; | dd bs=32 count=1 2&gt;/dev/null)cat &lt;&lt; EOF &gt; &quot;${fname}.ldif&quot;$userchangetype: modifyadd: objectClassobjectclass: kubernetesAuthenticationObject-add: kubernetesTokenkubernetesToken: $tokenEOFldapmodify -a -H ldapi:/// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password  -f &quot;${fname}.ldif&quot;done &lt; users.txt# outputEnter LDAP Password:modifying entry &quot;uid=user1,ou=Users,dc=k8s,dc=com&quot;Enter LDAP Password:modifying entry &quot;uid=user2,ou=Users,dc=k8s,dc=com&quot;</code></pre><h2 id="部署-Kubernetes-LDAP"><a href="#部署-Kubernetes-LDAP" class="headerlink" title="部署 Kubernetes LDAP"></a>部署 Kubernetes LDAP</h2><p>當 Kubernetes 環境建立完成後，首先進入<code>k8s-m1</code>節點，透過 git 取得 kube-ldap-authn 原始碼專案：</p><pre><code class="sh">$ git clone https://github.com/kairen/kube-ldap-authn.git$ cd kube-ldap-authn</code></pre><blockquote><p>若想使用 Go 語言實作的版本，可以參考 <a href="https://github.com/kairen/kube-ldap-webhook" target="_blank" rel="noopener">kube-ldap-webhook</a>.</p></blockquote><p>新增一個<code>config.py</code>檔案來提供相關設定內容：</p><pre><code class="sh">LDAP_URL=&#39;ldap://192.16.35.20/ ldap://192.16.35.20&#39;LDAP_START_TLS = FalseLDAP_BIND_DN = &#39;cn=admin,dc=k8s,dc=com&#39;LDAP_BIND_PASSWORD = &#39;password&#39;LDAP_USER_NAME_ATTRIBUTE = &#39;uid&#39;LDAP_USER_UID_ATTRIBUTE = &#39;uidNumber&#39;LDAP_USER_SEARCH_BASE = &#39;ou=People,dc=k8s,dc=com&#39;LDAP_USER_SEARCH_FILTER = &quot;(&amp;(kubernetesToken={token}))&quot;LDAP_GROUP_NAME_ATTRIBUTE = &#39;cn&#39;LDAP_GROUP_SEARCH_BASE = &#39;ou=Groups,dc=k8s,dc=com&#39;LDAP_GROUP_SEARCH_FILTER = &#39;(|(&amp;(objectClass=posixGroup)(memberUid={username}))(&amp;(member={dn})(objectClass=groupOfNames)))&#39;</code></pre><blockquote><p>變數詳細說明可以參考 <a href="https://github.com/kairen/kube-ldap-authn/blob/master/config.py.example" target="_blank" rel="noopener">Config example</a></p></blockquote><p>建立 kube-ldap-authn secret 來提供給 pod 使用，並部署 kube-ldap-authn pod 到所有 master 節點上：</p><pre><code class="sh">$ kubectl -n kube-system create secret generic ldap-authn-config --from-file=config.py=config.py$ kubectl create -f daemonset.yaml$ kubectl -n kube-system get po -l app=kube-ldap-authn -o wideNAME                    READY     STATUS    RESTARTS   AGE       IP             NODEkube-ldap-authn-sx994   1/1       Running   0          13s       192.16.35.11   k8s-m1</code></pre><p>這邊若成功部署的話，可以用 curl 進行測試：</p><pre><code class="sh">$ curl -X POST -H &quot;Content-Type: application/json&quot; \    -d &#39;{&quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;,  &quot;spec&quot;: {&quot;token&quot;: &quot;&lt;LDAP_K8S_TOKEN&gt;&quot;}}&#39; \    http://localhost:8087/authn# output{  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,  &quot;kind&quot;: &quot;TokenReview&quot;,  &quot;status&quot;: {    &quot;authenticated&quot;: true,    &quot;user&quot;: {      &quot;groups&quot;: [        &quot;kubernetes&quot;      ],      &quot;uid&quot;: &quot;1000&quot;,      &quot;username&quot;: &quot;user1&quot;    }  }}</code></pre><p>在所有<code>master</code>節點上新增一個名稱為<code>/srv/kubernetes/webhook-authn</code>的檔案，並加入以下內容：</p><pre><code class="sh">$ mkdir /srv/kubernetes$ cat &lt;&lt;EOF &gt; /srv/kubernetes/webhook-authnclusters:  - name: ldap-authn    cluster:      server: http://localhost:8087/authnusers:  - name: apiservercurrent-context: webhookcontexts:- context:    cluster: ldap-authn    user: apiserver  name: webhookEOF</code></pre><p>修改所有<code>master</code>節點上的<code>kube-apiserver.yaml</code> Static Pod 檔案，該檔案會存在於<code>/etc/kubernetes/manifests</code>目錄中，請修改加入以下內容：</p><pre><code class="yaml">...spec:  containers:  - command:    ...    - --runtime-config=authentication.k8s.io/v1beta1=true    - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-authn    - --authentication-token-webhook-cache-ttl=5m    volumeMounts:      ...    - mountPath: /srv/kubernetes/webhook-authn      name: webhook-authn      readOnly: true  volumes:    ...  - hostPath:      path: /srv/kubernetes/webhook-authn      type: File    name: webhook-authn</code></pre><blockquote><p>這邊<code>...</code>表示已存在的內容，請不要刪除與變更。這邊也可以用 kubeadmconfig 來設定，請參考 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file" target="_blank" rel="noopener">Using kubeadm init with a configuration file</a>。</p></blockquote><h2 id="測試功能"><a href="#測試功能" class="headerlink" title="測試功能"></a>測試功能</h2><p>首先進入<code>k8s-m1</code>，建立一個綁定在 user1 namespace 的唯讀 Role 與 RoleBinding：</p><pre><code class="sh">$ kubectl create ns user1# 建立 Role$ cat &lt;&lt;EOF | kubectl create -f -kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: readonly-role  namespace: user1rules:- apiGroups: [&quot;&quot;]  resources: [&quot;pods&quot;]  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]EOF# 建立 RoleBinding$ cat &lt;&lt;EOF | kubectl create -f -kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: readonly-role-binding  namespace: user1subjects:- kind: Group  name: kubernetes  apiGroup: &quot;&quot;roleRef:  kind: Role  name: readonly-role  apiGroup: &quot;&quot;EOF</code></pre><blockquote><p>注意!!這邊的<code>Group</code>是 LDAP 中的 Group。</p></blockquote><p>在任意台 Kubernetes client 端設定 Kubeconfig 來存取叢集，這邊直接在<code>k8s-m1</code>進行：</p><pre><code class="sh">$ cd$ kubectl config set-credentials user1 --kubeconfig=.kube/config --token=&lt;user-ldap-token&gt;$ kubectl config set-context user1-context \    --kubeconfig=.kube/config \    --cluster=kubernetes \    --namespace=user1 --user=user1</code></pre><p>接著透過 kubeclt 來測試權限是否正確設定：</p><pre><code class="sh">$ kubectl --context=user1-context get poNo resources found$ kubectl --context=user1-context run nginx --image nginx --port 80Error from server (Forbidden): deployments.extensions is forbidden: User &quot;user1&quot; cannot create deployments.extensions in the namespace &quot;user1&quot;$ kubectl --context=user1-context get po -n defaultError from server (Forbidden): pods is forbidden: User &quot;user1&quot; cannot list pods in the namespace &quot;default&quot;</code></pre><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">https://github.com/osixia/docker-openldap</a></li><li><a href="https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/" target="_blank" rel="noopener">https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/</a></li><li><a href="https://github.com/torchbox/kube-ldap-authn" target="_blank" rel="noopener">https://github.com/torchbox/kube-ldap-authn</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文將說明如何整合 OpenLDAP 來提供給 Kubernetes 進行使用者認證。Kubernetes 官方並沒有提供針對 LDAP 與 AD 的整合，但是可以藉由 &lt;a href=&quot;https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Webhook Token Authentication&lt;/a&gt; 以及 &lt;a href=&quot;https://kubernetes.io/docs/admin/authentication/#authenticating-proxy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Authenticating Proxy&lt;/a&gt; 來達到整合功能。概念是開發一個 HTTP Server 提供 POST Method 來塞入 Bearer Token，而該 HTTP Server 利用 LDAP library 檢索對應 Token 的 User 進行認證，成功後回傳該 User 的所有 Group 等資訊，而這時可以利用 Kubernetes 針對該 User 的 Group 設定對應的 RBAC role 進行權限控管。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="LDAP" scheme="https://readailib.github.io/tags/LDAP/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes v1.10.x HA 全手動苦工安裝教學(TL;DR)</title>
    <link href="https://readailib.github.io/2018/04/05/kubernetes/deploy/manual-v1.10/"/>
    <id>https://readailib.github.io/2018/04/05/kubernetes/deploy/manual-v1.10/</id>
    <published>2018-04-05T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.222Z</updated>
    
    <content type="html"><![CDATA[<p>本篇延續過往<code>手動安裝方式</code>來部署 Kubernetes v1.10.x 版本的 High Availability 叢集，主要目的是學習 Kubernetes 安裝的一些元件關析與流程。若不想這麼累的話，可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a> 來選擇自己最喜歡的方式。</p><p>本次安裝的軟體版本：</p><ul><li>Kubernetes v1.10.0</li><li>CNI v0.6.0</li><li>Etcd v3.1.13</li><li>Calico v3.0.4</li><li>Docker CE latest version</li></ul><a id="more"></a><p><img src="/images/kube/kubernetes-aa-ha.png" alt></p><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>192.16.35.11</td><td>k8s-m1</td><td>1</td><td>4G</td></tr><tr><td>192.16.35.12</td><td>k8s-m2</td><td>1</td><td>4G</td></tr><tr><td>192.16.35.13</td><td>k8s-m3</td><td>1</td><td>4G</td></tr><tr><td>192.16.35.14</td><td>k8s-n1</td><td>1</td><td>4G</td></tr><tr><td>192.16.35.15</td><td>k8s-n2</td><td>1</td><td>4G</td></tr><tr><td>192.16.35.16</td><td>k8s-n2</td><td>1</td><td>4G</td></tr></tbody></table><p>另外由所有 master 節點提供一組 VIP <code>192.16.35.10</code>。</p><blockquote><ul><li>這邊<code>m</code>為 K8s Master 節點，<code>n</code>為 K8s Node 節點。</li><li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li><li>可以下載 <a href="https://kairen.github.io/files/manual-v1.10/Vagrantfile" target="_blank" rel="noopener">Vagrantfile</a> 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。</li></ul></blockquote><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p><ul><li><code>所有節點</code>彼此網路互通，並且<code>k8s-m1</code> SSH 登入其他節點為 passwdless。</li><li>所有防火牆與 SELinux 已關閉。如 CentOS：</li></ul><pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled</code></pre><ul><li><code>所有節點</code>需要設定<code>/etc/hosts</code>解析到所有叢集主機。</li></ul><pre><code>...192.16.35.11 k8s-m1192.16.35.12 k8s-m2192.16.35.13 k8s-m3192.16.35.14 k8s-n1192.16.35.15 k8s-n2192.16.35.16 k8s-n3</code></pre><ul><li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li></ul><pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><ul><li><code>所有節點</code>需要設定<code>/etc/sysctl.d/k8s.conf</code>的系統參數。</li></ul><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf</code></pre><ul><li>Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，在<code>所有節點</code>利用以下指令關閉：</li></ul><pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</code></pre><blockquote><p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p></blockquote><ul><li>在<code>所有節點</code>下載 Kubernetes 二進制執行檔：</li></ul><pre><code class="sh">$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64&quot;$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet$ chmod +x /usr/local/bin/kubelet# node 請忽略下載 kubectl$ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl$ chmod +x /usr/local/bin/kubectl</code></pre><ul><li>在<code>所有節點</code>下載 Kubernetes CNI 二進制檔案：</li></ul><pre><code class="sh">$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx</code></pre><ul><li>在<code>k8s-m1</code>需要安裝<code>CFSSL</code>工具，這將會用來建立 TLS Certificates。</li></ul><pre><code class="sh">$ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot;$ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl$ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson</code></pre><h2 id="建立叢集-CA-keys-與-Certificates"><a href="#建立叢集-CA-keys-與-Certificates" class="headerlink" title="建立叢集 CA keys 與 Certificates"></a>建立叢集 CA keys 與 Certificates</h2><p>在這個部分，將需要產生多個元件的 Certificates，這包含 Etcd、Kubernetes 元件等，並且每個叢集都會有一個根數位憑證認證機構(Root Certificate Authority)被用在認證 API Server 與 Kubelet 端的憑證。</p><blockquote><p>P.S. 這邊要注意 CA JSON 檔的<code>CN(Common Name)</code>與<code>O(Organization)</code>等內容是會影響 Kubernetes 元件認證的。</p></blockquote><h3 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h3><p>首先在<code>k8s-m1</code>建立<code>/etc/etcd/ssl</code>資料夾，然後進入目錄完成以下操作。</p><pre><code class="sh">$ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot;</code></pre><p>下載<code>ca-config.json</code>與<code>etcd-ca-csr.json</code>檔案，並從 CSR json 產生 CA keys 與 Certificate：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot;$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca</code></pre><p>下載<code>etcd-csr.json</code>檔案，並產生 Etcd 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/etcd-csr.json&quot;$ cfssl gencert \  -ca=etcd-ca.pem \  -ca-key=etcd-ca-key.pem \  -config=ca-config.json \  -hostname=127.0.0.1,192.16.35.11,192.16.35.12,192.16.35.13 \  -profile=kubernetes \  etcd-csr.json | cfssljson -bare etcd</code></pre><blockquote><p><code>-hostname</code>需修改成所有 masters 節點。</p></blockquote><p>完成後刪除不必要檔案：</p><pre><code class="sh">$ rm -rf *.json *.csr</code></pre><p>確認<code>/etc/etcd/ssl</code>有以下檔案：</p><pre><code class="sh">$ ls /etc/etcd/ssletcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem</code></pre><p>複製相關檔案至其他 Etcd 節點，這邊為所有<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;    for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}    done  done</code></pre><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>在<code>k8s-m1</code>建立<code>pki</code>資料夾，然後進入目錄完成以下章節操作。</p><pre><code class="sh">$ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot;$ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot;</code></pre><p>下載<code>ca-config.json</code>與<code>ca-csr.json</code>檔案，並產生 CA 金鑰：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot;$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca$ ls ca*.pemca-key.pem  ca.pem</code></pre><h4 id="API-Server-Certificate"><a href="#API-Server-Certificate" class="headerlink" title="API Server Certificate"></a>API Server Certificate</h4><p>下載<code>apiserver-csr.json</code>檔案，並產生 kube-apiserver 憑證：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/apiserver-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -hostname=10.96.0.1,192.16.35.10,127.0.0.1,kubernetes.default \  -profile=kubernetes \  apiserver-csr.json | cfssljson -bare apiserver$ ls apiserver*.pemapiserver-key.pem  apiserver.pem</code></pre><blockquote><ul><li>這邊<code>-hostname</code>的<code>10.96.0.1</code>是 Cluster IP 的 Kubernetes 端點;</li><li><code>192.16.35.10</code>為虛擬 IP 位址(VIP);</li><li><code>kubernetes.default</code>為 Kubernetes DN。</li></ul></blockquote><h4 id="Front-Proxy-Certificate"><a href="#Front-Proxy-Certificate" class="headerlink" title="Front Proxy Certificate"></a>Front Proxy Certificate</h4><p>下載<code>front-proxy-ca-csr.json</code>檔案，並產生 Front Proxy CA 金鑰，Front Proxy 主要是用在 API aggregator 上:</p><pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot;$ cfssl gencert \  -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca$ ls front-proxy-ca*.pemfront-proxy-ca-key.pem  front-proxy-ca.pem</code></pre><p>下載<code>front-proxy-client-csr.json</code>檔案，並產生 front-proxy-client 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot;$ cfssl gencert \  -ca=front-proxy-ca.pem \  -ca-key=front-proxy-ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  front-proxy-client-csr.json | cfssljson -bare front-proxy-client$ ls front-proxy-client*.pemfront-proxy-client-key.pem  front-proxy-client.pem</code></pre><h4 id="Admin-Certificate"><a href="#Admin-Certificate" class="headerlink" title="Admin Certificate"></a>Admin Certificate</h4><p>下載<code>admin-csr.json</code>檔案，並產生 admin certificate 憑證：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/admin-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  admin-csr.json | cfssljson -bare admin$ ls admin*.pemadmin-key.pem  admin.pem</code></pre><p>接著透過以下指令產生名稱為 <code>admin.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># admin set cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../admin.conf# admin set credentials$ kubectl config set-credentials kubernetes-admin \    --client-certificate=admin.pem \    --client-key=admin-key.pem \    --embed-certs=true \    --kubeconfig=../admin.conf# admin set context$ kubectl config set-context kubernetes-admin@kubernetes \    --cluster=kubernetes \    --user=kubernetes-admin \    --kubeconfig=../admin.conf# admin set default context$ kubectl config use-context kubernetes-admin@kubernetes \    --kubeconfig=../admin.conf</code></pre><h4 id="Controller-Manager-Certificate"><a href="#Controller-Manager-Certificate" class="headerlink" title="Controller Manager Certificate"></a>Controller Manager Certificate</h4><p>下載<code>manager-csr.json</code>檔案，並產生 kube-controller-manager certificate 憑證：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/manager-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  manager-csr.json | cfssljson -bare controller-manager$ ls controller-manager*.pemcontroller-manager-key.pem  controller-manager.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>manager-csr.json</code>的<code>hosts</code>。</p></blockquote><p>接著透過以下指令產生名稱為<code>controller-manager.conf</code>的 kubeconfig 檔：</p><pre><code class="sh"># controller-manager set cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../controller-manager.conf# controller-manager set credentials$ kubectl config set-credentials system:kube-controller-manager \    --client-certificate=controller-manager.pem \    --client-key=controller-manager-key.pem \    --embed-certs=true \    --kubeconfig=../controller-manager.conf# controller-manager set context$ kubectl config set-context system:kube-controller-manager@kubernetes \    --cluster=kubernetes \    --user=system:kube-controller-manager \    --kubeconfig=../controller-manager.conf# controller-manager set default context$ kubectl config use-context system:kube-controller-manager@kubernetes \    --kubeconfig=../controller-manager.conf</code></pre><h4 id="Scheduler-Certificate"><a href="#Scheduler-Certificate" class="headerlink" title="Scheduler Certificate"></a>Scheduler Certificate</h4><p>下載<code>scheduler-csr.json</code>檔案，並產生 kube-scheduler certificate 憑證：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/scheduler-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  scheduler-csr.json | cfssljson -bare scheduler$ ls scheduler*.pemscheduler-key.pem  scheduler.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>scheduler-csr.json</code>的<code>hosts</code>。</p></blockquote><p>接著透過以下指令產生名稱為 <code>scheduler.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># scheduler set cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../scheduler.conf# scheduler set credentials$ kubectl config set-credentials system:kube-scheduler \    --client-certificate=scheduler.pem \    --client-key=scheduler-key.pem \    --embed-certs=true \    --kubeconfig=../scheduler.conf# scheduler set context$ kubectl config set-context system:kube-scheduler@kubernetes \    --cluster=kubernetes \    --user=system:kube-scheduler \    --kubeconfig=../scheduler.conf# scheduler use default context$ kubectl config use-context system:kube-scheduler@kubernetes \    --kubeconfig=../scheduler.conf</code></pre><h4 id="Master-Kubelet-Certificate"><a href="#Master-Kubelet-Certificate" class="headerlink" title="Master Kubelet Certificate"></a>Master Kubelet Certificate</h4><p>接著在<code>k8s-m1</code>節點下載<code>kubelet-csr.json</code>檔案，並產生所有<code>master</code>節點的憑證：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/kubelet-csr.json&quot;$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    cp kubelet-csr.json kubelet-$NODE-csr.json;    sed -i &quot;s/\$NODE/$NODE/g&quot; kubelet-$NODE-csr.json;    cfssl gencert \      -ca=ca.pem \      -ca-key=ca-key.pem \      -config=ca-config.json \      -hostname=$NODE \      -profile=kubernetes \      kubelet-$NODE-csr.json | cfssljson -bare kubelet-$NODE  done$ ls kubelet*.pemkubelet-k8s-m1-key.pem  kubelet-k8s-m1.pem  kubelet-k8s-m2-key.pem  kubelet-k8s-m2.pem  kubelet-k8s-m3-key.pem  kubelet-k8s-m3.pem</code></pre><blockquote><p>這邊需要依據節點修改<code>-hostname</code>與<code>$NODE</code>。</p></blockquote><p>完成後複製 kubelet 憑證至其他<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki&quot;    for FILE in kubelet-$NODE-key.pem kubelet-$NODE.pem ca.pem; do      scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE}    done  done</code></pre><p>接著執行以下指令產生名稱為<code>kubelet.conf</code>的 kubeconfig 檔：</p><pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;cd /etc/kubernetes/pki &amp;&amp; \      kubectl config set-cluster kubernetes \        --certificate-authority=ca.pem \        --embed-certs=true \        --server=${KUBE_APISERVER} \        --kubeconfig=../kubelet.conf &amp;&amp; \      kubectl config set-credentials system:node:${NODE} \        --client-certificate=kubelet-${NODE}.pem \        --client-key=kubelet-${NODE}-key.pem \        --embed-certs=true \        --kubeconfig=../kubelet.conf &amp;&amp; \      kubectl config set-context system:node:${NODE}@kubernetes \        --cluster=kubernetes \        --user=system:node:${NODE} \        --kubeconfig=../kubelet.conf &amp;&amp; \      kubectl config use-context system:node:${NODE}@kubernetes \        --kubeconfig=../kubelet.conf &amp;&amp; \      rm kubelet-${NODE}.pem kubelet-${NODE}-key.pem&quot;  done</code></pre><h4 id="Service-Account-Key"><a href="#Service-Account-Key" class="headerlink" title="Service Account Key"></a>Service Account Key</h4><p>Service account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用：</p><pre><code class="sh">$ openssl genrsa -out sa.key 2048$ openssl rsa -in sa.key -pubout -out sa.pub$ ls sa.*sa.key  sa.pub</code></pre><h4 id="刪除不必要檔案"><a href="#刪除不必要檔案" class="headerlink" title="刪除不必要檔案"></a>刪除不必要檔案</h4><p>所有資訊準備完成後，就可以將一些不必要檔案刪除：</p><pre><code class="sh">$ rm -rf *.json *.csr scheduler*.pem controller-manager*.pem admin*.pem kubelet*.pem</code></pre><h4 id="複製檔案至其他節點"><a href="#複製檔案至其他節點" class="headerlink" title="複製檔案至其他節點"></a>複製檔案至其他節點</h4><p>複製憑證檔案至其他<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    for FILE in $(ls /etc/kubernetes/pki/); do      scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE}    done  done</code></pre><p>複製 Kubernetes config 檔案至其他<code>master</code>節點：</p><pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do    echo &quot;--- $NODE ---&quot;    for FILE in admin.conf controller-manager.conf scheduler.conf; do      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}    done  done</code></pre><h2 id="Kubernetes-Masters"><a href="#Kubernetes-Masters" class="headerlink" title="Kubernetes Masters"></a>Kubernetes Masters</h2><p>本部分將說明如何建立與設定 Kubernetes Master 角色，過程中會部署以下元件：</p><ul><li><strong>kube-apiserver</strong>：提供 REST APIs，包含授權、認證與狀態儲存等。</li><li><strong>kube-controller-manager</strong>：負責維護叢集的狀態，如自動擴展，滾動更新等。</li><li><strong>kube-scheduler</strong>：負責資源排程，依據預定的排程策略將 Pod 分配到對應節點上。</li><li><strong>Etcd</strong>：儲存叢集所有狀態的 Key/Value 儲存系統。</li><li><strong>HAProxy</strong>：提供負載平衡器。</li><li><strong>Keepalived</strong>：提供虛擬網路位址(VIP)。</li></ul><h3 id="部署與設定"><a href="#部署與設定" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>首先在<code>所有 master 節點</code>下載部署元件的 YAML 檔案，這邊不採用二進制執行檔與 Systemd 來管理這些元件，全部採用 <a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/" target="_blank" rel="noopener">Static Pod</a> 來達成。這邊將檔案下載至<code>/etc/kubernetes/manifests</code>目錄：</p><pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/master&quot;$ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests$ for FILE in kube-apiserver kube-controller-manager kube-scheduler haproxy keepalived etcd etcd.config; do    wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml    if [ ${FILE} == &quot;etcd.config&quot; ]; then      mv etcd.config.yml /etc/etcd/etcd.config.yml      sed -i &quot;s/\${HOSTNAME}/${HOSTNAME}/g&quot; /etc/etcd/etcd.config.yml      sed -i &quot;s/\${PUBLIC_IP}/$(hostname -i)/g&quot; /etc/etcd/etcd.config.yml    fi  done$ ls /etc/kubernetes/manifestsetcd.yml  haproxy.yml  keepalived.yml  kube-apiserver.yml  kube-controller-manager.yml  kube-scheduler.yml</code></pre><blockquote><ul><li>若<code>IP</code>與教學設定不同的話，請記得修改 YAML 檔案。</li><li>kube-apiserver 中的 <code>NodeRestriction</code> 請參考 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Using Node Authorization</a>。</li></ul></blockquote><p>產生一個用來加密 Etcd 的 Key：</p><pre><code class="sh">$ head -c 32 /dev/urandom | base64SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=</code></pre><blockquote><p>注意每台<code>master</code>節點需要用一樣的 Key。</p></blockquote><p>在<code>/etc/kubernetes/</code>目錄下，建立<code>encryption.yml</code>的加密 YAML 檔案：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.ymlkind: EncryptionConfigapiVersion: v1resources:  - resources:      - secrets    providers:      - aescbc:          keys:            - name: key1              secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=      - identity: {}EOF</code></pre><blockquote><p>Etcd 資料加密可參考這篇 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/" target="_blank" rel="noopener">Encrypting data at rest</a>。</p></blockquote><p>在<code>/etc/kubernetes/</code>目錄下，建立<code>audit-policy.yml</code>的進階稽核策略 YAML 檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.ymlapiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF</code></pre><blockquote><p>Audit Policy 請參考這篇 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/" target="_blank" rel="noopener">Auditing</a>。</p></blockquote><p>下載<code>haproxy.cfg</code>檔案來提供給 HAProxy 容器使用：</p><pre><code class="sh">$ mkdir -p /etc/haproxy/$ wget &quot;${CORE_URL}/haproxy.cfg&quot; -O /etc/haproxy/haproxy.cfg</code></pre><blockquote><p>若與本教學 IP 不同的話，請記得修改設定檔。</p></blockquote><p>下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p><pre><code class="sh">$ mkdir -p /etc/systemd/system/kubelet.service.d$ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service$ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf</code></pre><blockquote><p>若 cluster <code>dns</code>或<code>domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p></blockquote><p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service</code></pre><p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p><pre><code class="sh">$ watch netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      10344/kubelettcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      11324/kube-scheduletcp        0      0 0.0.0.0:6443            0.0.0.0:*               LISTEN      11416/haproxytcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      11235/kube-controlltcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      11416/haproxytcp6       0      0 :::2379                 :::*                    LISTEN      10479/etcdtcp6       0      0 :::2380                 :::*                    LISTEN      10479/etcdtcp6       0      0 :::10255                :::*                    LISTEN      10344/kubelettcp6       0      0 :::5443                 :::*                    LISTEN      11295/kube-apiserve</code></pre><blockquote><p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker</code>指令來查看。</p></blockquote><h3 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h3><p>完成後，在任意一台<code>master</code>節點複製 admin kubeconfig 檔案，並透過簡單指令驗證：</p><pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config$ kubectl get csNAME                 STATUS    MESSAGE              ERRORcontroller-manager   Healthy   okscheduler            Healthy   oketcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}$ kubectl get nodeNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    52s       v1.10.0k8s-m2    NotReady   master    51s       v1.10.0k8s-m3    NotReady   master    50s       v1.10.0$ kubectl -n kube-system get poNAME                             READY     STATUS    RESTARTS   AGEetcd-k8s-m1                      1/1       Running   0          7setcd-k8s-m2                      1/1       Running   0          57shaproxy-k8s-m3                   1/1       Running   0          1m...</code></pre><p>接著確認服務能夠執行 logs 等指令：</p><pre><code class="sh">$ kubectl -n kube-system logs -f kube-scheduler-k8s-m2Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-scheduler-k8s-m2)</code></pre><blockquote><p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源存取權限，屬於正常。</p></blockquote><p>由於上述權限問題，必需建立一個<code>apiserver-to-kubelet-rbac.yml</code>來定義權限，以供對 Nodes 容器執行 logs、exec 等指令。在任意一台<code>master</code>節點執行以下指令：</p><pre><code class="sh">$ kubectl apply -f &quot;${CORE_URL}/apiserver-to-kubelet-rbac.yml.conf&quot;clusterrole.rbac.authorization.k8s.io &quot;system:kube-apiserver-to-kubelet&quot; configuredclusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-apiserver&quot; configured# 測試 logs$ kubectl -n kube-system logs -f kube-scheduler-k8s-m2...I0403 02:30:36.375935       1 server.go:555] Version: v1.10.0I0403 02:30:36.378208       1 server.go:574] starting healthz server on 127.0.0.1:10251</code></pre><p>設定<code>master</code>節點允許 Taint：</p><pre><code class="sh">$ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --allnode &quot;k8s-m1&quot; taintednode &quot;k8s-m2&quot; taintednode &quot;k8s-m3&quot; tainted</code></pre><blockquote><p><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">Taints and Tolerations</a>。</p></blockquote><h3 id="建立-TLS-Bootstrapping-RBAC-與-Secret"><a href="#建立-TLS-Bootstrapping-RBAC-與-Secret" class="headerlink" title="建立 TLS Bootstrapping RBAC 與 Secret"></a>建立 TLS Bootstrapping RBAC 與 Secret</h3><p>由於本次安裝啟用了 TLS 認證，因此每個節點的 kubelet 都必須使用 kube-apiserver 的 CA 的憑證後，才能與 kube-apiserver 進行溝通，而該過程需要手動針對每台節點單獨簽署憑證是一件繁瑣的事情，且一旦節點增加會延伸出管理不易問題; 而 TLS bootstrapping 目標就是解決該問題，透過讓 kubelet 先使用一個預定低權限使用者連接到 kube-apiserver，然後在對 kube-apiserver 申請憑證簽署，當授權 Token 一致時，Node 節點的 kubelet 憑證將由 kube-apiserver 動態簽署提供。具體作法可以參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a> 與 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank" rel="noopener">Authenticating with Bootstrap Tokens</a>。</p><p>首先在<code>k8s-m1</code>建立一個變數來產生<code>BOOTSTRAP_TOKEN</code>，並建立<code>bootstrap-kubelet.conf</code>的 Kubernetes config 檔：</p><pre><code class="sh">$ cd /etc/kubernetes/pki$ export TOKEN_ID=$(openssl rand 3 -hex)$ export TOKEN_SECRET=$(openssl rand 8 -hex)$ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET}$ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot;# bootstrap set cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../bootstrap-kubelet.conf# bootstrap set credentials$ kubectl config set-credentials tls-bootstrap-token-user \    --token=${BOOTSTRAP_TOKEN} \    --kubeconfig=../bootstrap-kubelet.conf# bootstrap set context$ kubectl config set-context tls-bootstrap-token-user@kubernetes \    --cluster=kubernetes \    --user=tls-bootstrap-token-user \    --kubeconfig=../bootstrap-kubelet.conf# bootstrap use default context$ kubectl config use-context tls-bootstrap-token-user@kubernetes \    --kubeconfig=../bootstrap-kubelet.conf</code></pre><blockquote><p>若想要用手動簽署憑證來進行授權的話，可以參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/" target="_blank" rel="noopener">Certificate</a>。</p></blockquote><p>接著在<code>k8s-m1</code>建立 TLS bootstrap secret 來提供自動簽證使用：</p><pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Secretmetadata:  name: bootstrap-token-${TOKEN_ID}  namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData:  token-id: ${TOKEN_ID}  token-secret: ${TOKEN_SECRET}  usage-bootstrap-authentication: &quot;true&quot;  usage-bootstrap-signing: &quot;true&quot;  auth-extra-groups: system:bootstrappers:default-node-tokenEOFsecret &quot;bootstrap-token-65a3a9&quot; created</code></pre><p>在<code>k8s-m1</code>建立 TLS Bootstrap Autoapprove RBAC：</p><pre><code class="sh">$ kubectl apply -f &quot;${CORE_URL}/kubelet-bootstrap-rbac.yml.conf&quot;clusterrolebinding.rbac.authorization.k8s.io &quot;kubelet-bootstrap&quot; createdclusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-bootstrap&quot; createdclusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-certificate-rotation&quot; created</code></pre><h2 id="Kubernetes-Nodes"><a href="#Kubernetes-Nodes" class="headerlink" title="Kubernetes Nodes"></a>Kubernetes Nodes</h2><p>本部分將說明如何建立與設定 Kubernetes Node 角色，Node 是主要執行容器實例(Pod)的工作節點。</p><p>在開始部署前，先在<code>k8-m1</code>將需要用到的檔案複製到所有<code>node</code>節點上：</p><pre><code class="sh">$ cd /etc/kubernetes/pki$ for NODE in k8s-n1 k8s-n2 k8s-n3; do    echo &quot;--- $NODE ---&quot;    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;    # Etcd    for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}    done    # Kubernetes    for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}    done  done</code></pre><h3 id="部署與設定-1"><a href="#部署與設定-1" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>在每台<code>node</code>節點下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p><pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/node&quot;$ mkdir -p /etc/systemd/system/kubelet.service.d$ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service$ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf</code></pre><blockquote><p>若 cluster <code>dns</code>或<code>domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p></blockquote><p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service</code></pre><h3 id="驗證叢集-1"><a href="#驗證叢集-1" class="headerlink" title="驗證叢集"></a>驗證叢集</h3><p>完成後，在任意一台<code>master</code>節點並透過簡單指令驗證：</p><pre><code class="sh">$ kubectl get csrNAME                                                   AGE       REQUESTOR                 CONDITIONcsr-bvz9l                                              11m       system:node:k8s-m1        Approved,Issuedcsr-jwr8k                                              11m       system:node:k8s-m2        Approved,Issuedcsr-q867w                                              11m       system:node:k8s-m3        Approved,Issuednode-csr-Y-FGvxZWJqI-8RIK_IrpgdsvjGQVGW0E4UJOuaU8ogk   17s       system:bootstrap:dca3e1   Approved,Issuednode-csr-cnX9T1xp1LdxVDc9QW43W0pYkhEigjwgceRshKuI82c   19s       system:bootstrap:dca3e1   Approved,Issuednode-csr-m7SBA9RAGCnsgYWJB-u2HoB2qLSfiQZeAxWFI2WYN7Y   18s       system:bootstrap:dca3e1   Approved,Issued$ kubectl get nodesNAME      STATUS     ROLES     AGE       VERSIONk8s-m1    NotReady   master    12m       v1.10.0k8s-m2    NotReady   master    11m       v1.10.0k8s-m3    NotReady   master    11m       v1.10.0k8s-n1    NotReady   node      32s       v1.10.0k8s-n2    NotReady   node      31s       v1.10.0k8s-n3    NotReady   node      29s       v1.10.0</code></pre><h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成上面所有步驟後，接著需要部署一些插件，其中如<code>Kubernetes DNS</code>與<code>Kubernetes Proxy</code>等這種 Addons 是非常重要的。</p><h3 id="Kubernetes-Proxy"><a href="#Kubernetes-Proxy" class="headerlink" title="Kubernetes Proxy"></a>Kubernetes Proxy</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">Kube-proxy</a> 是實現 Service 的關鍵插件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。</p><p>在<code>k8s-m1</code>下載<code>kube-proxy.yml</code>來建立 Kubernetes Proxy Addon：</p><pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-proxy.yml.conf&quot;serviceaccount &quot;kube-proxy&quot; createdclusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-proxy&quot; createdconfigmap &quot;kube-proxy&quot; createddaemonset.apps &quot;kube-proxy&quot; created$ kubectl -n kube-system get po -o wide -l k8s-app=kube-proxyNAME               READY     STATUS    RESTARTS   AGE       IP             NODEkube-proxy-8j5w8   1/1       Running   0          29s       192.16.35.16   k8s-n3kube-proxy-c4zvt   1/1       Running   0          29s       192.16.35.11   k8s-m1kube-proxy-clpl6   1/1       Running   0          29s       192.16.35.12   k8s-m2...</code></pre><h3 id="Kubernetes-DNS"><a href="#Kubernetes-DNS" class="headerlink" title="Kubernetes DNS"></a>Kubernetes DNS</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。</p><p>在<code>k8s-m1</code>下載<code>kube-proxy.yml</code>來建立 Kubernetes Proxy Addon：</p><pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-dns.yml.conf&quot;serviceaccount &quot;kube-dns&quot; createdservice &quot;kube-dns&quot; createddeployment.extensions &quot;kube-dns&quot; created$ kubectl -n kube-system get po -l k8s-app=kube-dnsNAME                        READY     STATUS    RESTARTS   AGEkube-dns-654684d656-zq5t8   0/3       Pending   0          1m</code></pre><p>這邊會發現處於<code>Pending</code>狀態，是由於 Kubernetes Pod Network 還未建立完成，因此所有節點會處於<code>NotReady</code>狀態，而造成 Pod 無法被排程分配到指定節點上啟動，由於為了解決該問題，下節將說明如何建立 Pod Network。</p><h2 id="Calico-Network-安裝與設定"><a href="#Calico-Network-安裝與設定" class="headerlink" title="Calico Network 安裝與設定"></a>Calico Network 安裝與設定</h2><p>Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是它整合了各種雲原生平台，且 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。</p><blockquote><p>本次不採用手動方式來建立 Calico 網路，若想了解可以參考 <a href="https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/integration" target="_blank" rel="noopener">Integration Guide</a>。</p></blockquote><p>在<code>k8s-m1</code>下載<code>calico.yaml</code>來建立 Calico Network：</p><pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/network/calico.yml.conf&quot;configmap &quot;calico-config&quot; createddaemonset &quot;calico-node&quot; createddeployment &quot;calico-kube-controllers&quot; createdclusterrolebinding &quot;calico-cni-plugin&quot; createdclusterrole &quot;calico-cni-plugin&quot; createdserviceaccount &quot;calico-cni-plugin&quot; createdclusterrolebinding &quot;calico-kube-controllers&quot; createdclusterrole &quot;calico-kube-controllers&quot; createdserviceaccount &quot;calico-kube-controllers&quot; created$ kubectl -n kube-system get po -l k8s-app=calico-node -o wideNAME                READY     STATUS    RESTARTS   AGE       IP             NODEcalico-node-22mbb   2/2       Running   0          1m        192.16.35.12   k8s-m2calico-node-2qwf5   2/2       Running   0          1m        192.16.35.11   k8s-m1calico-node-g2sp8   2/2       Running   0          1m        192.16.35.13   k8s-m3calico-node-hghp4   2/2       Running   0          1m        192.16.35.14   k8s-n1calico-node-qp6gf   2/2       Running   0          1m        192.16.35.15   k8s-n2calico-node-zfx4n   2/2       Running   0          1m        192.16.35.16   k8s-n3</code></pre><blockquote><p>這邊若節點 IP 與網卡不同的話，請修改<code>calico.yml</code>檔案。</p></blockquote><p>在<code>k8s-m1</code>下載 Calico CLI 來查看 Calico nodes:</p><pre><code class="sh">$ wget https://github.com/projectcalico/calicoctl/releases/download/v3.1.0/calicoctl -O /usr/local/bin/calicoctl$ chmod u+x /usr/local/bin/calicoctl$ cat &lt;&lt;EOF &gt; ~/calico-rcexport ETCD_ENDPOINTS=&quot;https://192.16.35.11:2379,https://192.16.35.12:2379,https://192.16.35.13:2379&quot;export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot;export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;EOF$ . ~/calico-rc$ calicoctl node statusCalico process is running.IPv4 BGP status+--------------+-------------------+-------+----------+-------------+| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |+--------------+-------------------+-------+----------+-------------+| 192.16.35.12 | node-to-node mesh | up    | 04:42:37 | Established || 192.16.35.13 | node-to-node mesh | up    | 04:42:42 | Established || 192.16.35.14 | node-to-node mesh | up    | 04:42:37 | Established || 192.16.35.15 | node-to-node mesh | up    | 04:42:41 | Established || 192.16.35.16 | node-to-node mesh | up    | 04:42:36 | Established |+--------------+-------------------+-------+----------+-------------+...</code></pre><p>查看 pending 的 pod 是否已執行：</p><pre><code class="sh">$ kubectl -n kube-system get po -l k8s-app=kube-dnskubectl -n kube-system get po -l k8s-app=kube-dnsNAME                        READY     STATUS    RESTARTS   AGEkube-dns-654684d656-j8xzx   3/3       Running   0          10m</code></pre><h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。</p><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。</p><p>在<code>k8s-m1</code>透過 kubectl 來建立 kubernetes dashboard 即可：</p><pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboardNAME                                    READY     STATUS    RESTARTS   AGEkubernetes-dashboard-7d5dcdb6d9-j492l   1/1       Running   0          12sNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGEkubernetes-dashboard   ClusterIP   10.111.22.111   &lt;none&gt;        443/TCP   12s</code></pre><p>這邊會額外建立一個名稱為<code>open-api</code> Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API:</p><pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: open-api  namespace: &quot;&quot;roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:  - apiGroup: rbac.authorization.k8s.io    kind: User    name: system:anonymousEOF</code></pre><blockquote><p>注意!管理者可以針對特定使用者來開放 API 存取權限，但這邊方便使用直接綁在 cluster-admin cluster role。</p></blockquote><p>完成後，就可以透過瀏覽器存取 <a href="https://192.16.35.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p><p>在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role：</p><pre><code class="sh">$ kubectl -n kube-system create sa dashboard$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;)$ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw</code></pre><blockquote><p>複製<code>token</code>，然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。</p></blockquote><p><img src="/images/kube/kubernetes-dashboard.png" alt></p><h3 id="Heapster"><a href="#Heapster" class="headerlink" title="Heapster"></a>Heapster</h3><p><a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">Heapster</a> 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。</p><p>在<code>k8s-m1</code>透過 kubectl 來建立 kubernetes monitor  即可：</p><pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-monitor.yml.conf&quot;$ kubectl -n kube-system get po,svcNAME                                           READY     STATUS    RESTARTS   AGE...po/heapster-74fb5c8cdc-62xzc                   4/4       Running   0          7mpo/influxdb-grafana-55bd7df44-nw4nc            2/2       Running   0          7mNAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE...svc/heapster               ClusterIP   10.100.242.225   &lt;none&gt;        80/TCP              7msvc/monitoring-grafana     ClusterIP   10.101.106.180   &lt;none&gt;        80/TCP              7msvc/monitoring-influxdb    ClusterIP   10.109.245.142   &lt;none&gt;        8083/TCP,8086/TCP   7m···</code></pre><p>完成後，就可以透過瀏覽器存取 <a href="https://192.16.35.10:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/" target="_blank" rel="noopener">Grafana Dashboard</a>。</p><p><img src="/images/kube/monitoring-grafana.png" alt></p><h3 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h3><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">Ingress</a>是利用 Nginx 或 HAProxy 等負載平衡器來曝露叢集內服務的元件，Ingress 主要透過設定 Ingress 規則來定義 Domain Name 映射 Kubernetes 內部 Service，這種方式可以避免掉使用過多的 NodePort 問題。</p><p>在<code>k8s-m1</code>透過 kubectl 來建立 Ingress Controller 即可：</p><pre><code class="sh">$ kubectl create ns ingress-nginx$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf&quot;$ kubectl -n ingress-nginx get poNAME                                       READY     STATUS    RESTARTS   AGEdefault-http-backend-5c6d95c48-rzxfb       1/1       Running   0          7mnginx-ingress-controller-699cdf846-982n4   1/1       Running   0          7m</code></pre><blockquote><p>這裡也可以選擇 <a href="https://github.com/containous/traefik" target="_blank" rel="noopener">Traefik</a> 的 Ingress Controller。</p></blockquote><h4 id="測試-Ingress-功能"><a href="#測試-Ingress-功能" class="headerlink" title="測試 Ingress 功能"></a>測試 Ingress 功能</h4><p>這邊先建立一個 Nginx HTTP server Deployment 與 Service：</p><pre><code class="sh">$ kubectl run nginx-dp --image nginx --port 80$ kubectl expose deploy nginx-dp --port 80$ kubectl get po,svc$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: test-nginx-ingress  annotations:    ingress.kubernetes.io/rewrite-target: /spec:  rules:  - host: test.nginx.com    http:      paths:      - path: /        backend:          serviceName: nginx-dp          servicePort: 80EOF</code></pre><p>透過 curl 來進行測試：</p><pre><code class="sh">$ curl 192.16.35.10 -H &#39;Host: test.nginx.com&#39;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...# 測試其他 domain name 是否會回傳 404$ curl 192.16.35.10 -H &#39;Host: test.nginx.com1&#39;default backend - 404</code></pre><h3 id="Helm-Tiller-Server"><a href="#Helm-Tiller-Server" class="headerlink" title="Helm Tiller Server"></a>Helm Tiller Server</h3><p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。其中<code>Tiller Server</code>主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</p><p>首先在<code>k8s-m1</code>安裝 Helm tool：</p><pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/</code></pre><p>另外在所有<code>node</code>節點安裝 socat：</p><pre><code class="sh">$ sudo apt-get install -y socat</code></pre><p>接著初始化 Helm(這邊會安裝 Tiller Server)：</p><pre><code class="sh">$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller...Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Happy Helming!$ kubectl -n kube-system get po -l app=helmNAME                             READY     STATUS    RESTARTS   AGEtiller-deploy-5f789bd9f7-tzss6   1/1       Running   0          29s$ helm versionClient: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}</code></pre><h4 id="測試-Helm-功能"><a href="#測試-Helm-功能" class="headerlink" title="測試 Helm 功能"></a>測試 Helm 功能</h4><p>這邊部署簡單 Jenkins 來進行功能測試：</p><pre><code class="sh">$ helm install --name demo --set Persistence.Enabled=false stable/jenkins$ kubectl get po,svc  -l app=demo-jenkinsNAME                           READY     STATUS    RESTARTS   AGEdemo-jenkins-7bf4bfcff-q74nt   1/1       Running   0          2mNAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEdemo-jenkins         LoadBalancer   10.103.15.129    &lt;pending&gt;     8080:31161/TCP   2mdemo-jenkins-agent   ClusterIP      10.103.160.126   &lt;none&gt;        50000/TCP        2m# 取得 admin 帳號的密碼$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echor6y9FMuF2u</code></pre><p>完成後，就可以透過瀏覽器存取 <a href="http://192.16.35.10:31161" target="_blank" rel="noopener">Jenkins Web</a>。</p><p><img src="/images/kube/helm-jenkins-v1.10.png" alt></p><p>測試完成後，即可刪除：</p><pre><code class="sh">$ helm lsNAME    REVISION    UPDATED                     STATUS      CHART             NAMESPACEdemo    1           Tue Apr 10 07:29:51 2018    DEPLOYED    jenkins-0.14.4    default$ helm delete demo --purgerelease &quot;demo&quot; deleted</code></pre><p>更多 Helm Apps 可以到 <a href="https://hub.kubeapps.com/" target="_blank" rel="noopener">Kubeapps Hub</a> 尋找。</p><h2 id="測試叢集"><a href="#測試叢集" class="headerlink" title="測試叢集"></a>測試叢集</h2><p>SSH 進入<code>k8s-m1</code>節點，然後關閉該節點：</p><pre><code class="sh">$ sudo poweroff</code></pre><p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p><pre><code class="sh"># 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷$ kubectl get csNAME                 STATUS      MESSAGE                                                                                                                                          ERRORscheduler            Healthy     okcontroller-manager   Healthy     oketcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}etcd-0               Unhealthy   Get https://192.16.35.11:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl get poNAME      READY     STATUS    RESTARTS   AGEnginx     1/1       Running   0          22s</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇延續過往&lt;code&gt;手動安裝方式&lt;/code&gt;來部署 Kubernetes v1.10.x 版本的 High Availability 叢集，主要目的是學習 Kubernetes 安裝的一些元件關析與流程。若不想這麼累的話，可以參考 &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Picking the Right Solution&lt;/a&gt; 來選擇自己最喜歡的方式。&lt;/p&gt;
&lt;p&gt;本次安裝的軟體版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.10.0&lt;/li&gt;
&lt;li&gt;CNI v0.6.0&lt;/li&gt;
&lt;li&gt;Etcd v3.1.13&lt;/li&gt;
&lt;li&gt;Calico v3.0.4&lt;/li&gt;
&lt;li&gt;Docker CE latest version&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Calico" scheme="https://readailib.github.io/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>使用 kubefed 建立 Kubernetes Federation(On-premises)</title>
    <link href="https://readailib.github.io/2018/03/21/kubernetes/k8s-federation/"/>
    <id>https://readailib.github.io/2018/03/21/kubernetes/k8s-federation/</id>
    <published>2018-03-21T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.224Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/" target="_blank" rel="noopener">Kubernetes Federation(聯邦)</a> 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。</p><a id="more"></a><p>Federation 使管理多個叢集更為簡單，這主要是透過兩個模型來實現：</p><ol><li><strong>跨叢集的資源同步(Sync resources across clusters)</strong>：提供在多個叢集中保持資源同步的功能，如確保一個 Deployment 可以存在於多個叢集中。</li><li><strong>跨叢集的服務發現(Cross cluster discovery:)</strong>：提供自動配置 DNS 服務以及在所有叢集後端上進行負載平衡功能，如提供全域 VIP 或 DNS record，並透過此存取多個叢集後端。</li></ol><p><img src="/images/kube/federation-api.png" alt></p><p>Federation 有以下幾個好處：</p><ol><li>跨叢集的資源排程，能讓 Pod 分配至不同叢集的不同節點上執行，如果當前叢集超出負荷，能夠將額外附載分配到空閒叢集上。</li><li>叢集的高可靠，能夠做到 Pod 故障自動遷移。</li><li>可管理多個 Kubernetes 叢集。</li><li>跨叢集的服務發現。</li></ol><blockquote><p>雖然 Federation 能夠降低管理多叢集門檻，但是目前依據不建議放到生產環境。以下幾個原因：</p><ul><li><strong>成熟度問題</strong>，目前還處與 Alpha 階段，故很多功能都還處於實現性質，或者不太穩定。</li><li><strong>提升網路頻寬與成本</strong>，由於 Federation 需要監控所有叢集以確保當前狀態符合預期，因是會增加額外效能開銷。</li><li><strong>跨叢集隔離差</strong>，Federation 的子叢集git有可能因為 Bug 的引發而影響其他叢集運行狀況。</li><li>個人用起來不是很穩定，例如建立的 Deployment 刪除很常會 Timeout。</li><li>支援的物件資源有限，如不支援 StatefulSets。可參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#api-resources" target="_blank" rel="noopener">API resources</a>。</li></ul></blockquote><p>Federation 主要包含三個元件：</p><ul><li><strong>federation-apiserver</strong>：主要提供跨叢集的 REST API 伺服器，類似 kube-apiserver。</li><li><strong>federation-controller-manager</strong>：提供多個叢集之間的狀態同步，類似 kube-controller-manager。</li><li><strong>kubefed</strong>：Federation CLI 工具，用來初始化 Federation 元件與加入子叢集。</li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器，共有三組叢集：</p><p>Federation 控制平面叢集(簡稱 F):</p><table><thead><tr><th>IP Address</th><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>172.22.132.31</td><td>k8s-f-m1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.32</td><td>k8s-f-n1</td><td>4</td><td>16G</td></tr></tbody></table><p>叢集 A:</p><table><thead><tr><th>IP Address</th><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>172.22.132.41</td><td>k8s-a-m1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.42</td><td>k8s-a-n1</td><td>8</td><td>16G</td></tr></tbody></table><p>叢集 B:</p><table><thead><tr><th>IP Address</th><th>Host</th><th>vCPU</th><th>RAM</th></tr></thead><tbody><tr><td>172.22.132.51</td><td>k8s-b-m1</td><td>8</td><td>16G</td></tr><tr><td>172.22.132.52</td><td>k8s-b-n1</td><td>8</td><td>16G</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝與進行 Federation 之前，需要確保以下條件達成：</p><ul><li>所有叢集的節點各自部署成一個 Kubernetes 叢集，請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li><li>修改 F、A 與 B 叢集的 Kubernetes config，並將 A 與 B 複製到 F 節點，如修改成以下：</li></ul><pre><code class="yaml">......  name: k8s-a-clustercontexts:- context:    cluster: k8s-a-cluster    user: a-cluster-admin  name: a-cluster-contextcurrent-context: a-cluster-contextkind: Configpreferences: {}users:- name: a-cluster-admin  user:...</code></pre><blockquote><p>這邊需要修改每個叢集 config。</p></blockquote><ul><li>接著在 F 叢集合併 F、A 與 B 三個 config，透過以下方式進行：</li></ul><pre><code class="sh">$ lsa-cluster.conf  b-cluster.conf  f-cluster.conf$ KUBECONFIG=f-cluster.conf:a-cluster.conf:b-cluster.conf kubectl config view --flatten &gt; ~/.kube/config$ kubectl config get-contextsCURRENT   NAME                CLUSTER         AUTHINFO          NAMESPACE          a-cluster-context   k8s-a-cluster   a-cluster-admin          b-cluster-context   k8s-b-cluster   b-cluster-admin*         f-cluster-context   k8s-f-cluster   f-cluster-admin</code></pre><ul><li>在 F 叢集安裝 kubefed 工具：</li></ul><pre><code class="sh">$ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz$ tar xvf federation-client-linux-amd64.tar.gz$ cp federation/client/bin/kubefed /usr/local/bin/$ kubefed versionClient Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}</code></pre><ul><li>在 F 叢集安裝 Helm 工具，並進行初始化：</li></ul><pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zxf$ sudo mv linux-amd64/helm /usr/local/bin/$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller# wait for a few minutes$ helm versionClient: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}</code></pre><h2 id="部署-Kubernetes-Federation"><a href="#部署-Kubernetes-Federation" class="headerlink" title="部署 Kubernetes Federation"></a>部署 Kubernetes Federation</h2><p>由於本篇是使用實體機器部署 Kubernetes 叢集，因此無法像是 GCP 可以提供 DNS 服務來給 Federation 使用，故這邊要用 CoreDNS 建立自定義 DNS 服務。</p><h3 id="CoreDNS-安裝"><a href="#CoreDNS-安裝" class="headerlink" title="CoreDNS 安裝"></a>CoreDNS 安裝</h3><p>首先透過 Helm 來安裝 CoreDNS 使用到的 Etcd：</p><pre><code class="sh">$ helm install --namespace federation --name etcd-operator stable/etcd-operator$ helm upgrade --namespace federation --set cluster.enabled=true etcd-operator stable/etcd-operator$ kubectl -n federation get poNAME                                                              READY     STATUS    RESTARTS   AGEetcd-operator-etcd-operator-etcd-backup-operator-577d56449zqkj2   1/1       Running   0          1metcd-operator-etcd-operator-etcd-operator-56679fb56-fpgmm         1/1       Running   0          1metcd-operator-etcd-operator-etcd-restore-operator-65b6cbccl7kzr   1/1       Running   0          1m</code></pre><p>完成後就可以安裝 CoreDNS 來提供自定義 DNS 服務了：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; Values.yamlisClusterService: falseserviceType: NodePortmiddleware:  kubernetes:    enabled: false  etcd:    enabled: true    zones:    - &quot;kairen.com.&quot;    endpoint: &quot;http://etcd-cluster.federation:2379&quot;EOF$ kubectl create clusterrolebinding federation-admin --clusterrole=cluster-admin --user=system:serviceaccount:federation:default$ helm install --namespace federation --name coredns -f Values.yaml stable/coredns# 測試 CoreDNS 可以查詢 Domain Name$ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstoolsdnstools# host kuberneteskubernetes.default.svc.cluster.local has address 10.96.0.1</code></pre><h3 id="安裝與初始化-Federation-控制平面元件"><a href="#安裝與初始化-Federation-控制平面元件" class="headerlink" title="安裝與初始化 Federation 控制平面元件"></a>安裝與初始化 Federation 控制平面元件</h3><p>完成 CoreDNS 後，接著透過 kubefed 安裝控制平面元件，由於使用到 CoreDNS，因此這邊要傳入相關 conf 檔，首先建立<code>coredns-provider.conf</code>檔案，加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; coredns-provider.conf[Global]etcd-endpoints = http://etcd-cluster.federation:2379zones = kairen.com.EOF</code></pre><blockquote><p>請自行修改<code>etcd-endpoints</code>與<code>zones</code>。</p></blockquote><p>檔案建立並確認沒問題後，透過 kubefed 工具來初始化主叢集：</p><pre><code class="sh">$ kubefed init federation \  --host-cluster-context=f-cluster-context \  --dns-provider=&quot;coredns&quot; \  --dns-zone-name=&quot;kairen.com.&quot; \  --apiserver-enable-basic-auth=true \  --apiserver-enable-token-auth=true \  --dns-provider-config=&quot;coredns-provider.conf&quot; \  --apiserver-arg-overrides=&quot;--anonymous-auth=false,--v=4&quot; \  --api-server-service-type=&quot;NodePort&quot; \  --api-server-advertise-address=&quot;172.22.132.31&quot; \  --etcd-persistent-storage=true$ kubectl -n federation-system get poNAME                                  READY     STATUS    RESTARTS   AGEapiserver-848d584b5d-cwxdh            2/2       Running   0          1mcontroller-manager-5846c555c6-mw2jz   1/1       Running   1          1m</code></pre><blockquote><p>這邊可以改變<code>--etcd-persistent-storage</code>來選擇使用或不使用 PV，若使用請先建立一個 PV 來提供給 Federation Pod 的 PVC 索取使用，可以參考 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">Persistent Volumes</a>。</p></blockquote><h3 id="加入-Federation-的-Kubernetes-子叢集"><a href="#加入-Federation-的-Kubernetes-子叢集" class="headerlink" title="加入 Federation 的 Kubernetes 子叢集"></a>加入 Federation 的 Kubernetes 子叢集</h3><pre><code class="sh">$ kubectl config use-context federation# 加入 k8s-a-cluster$ kubefed join f-a-cluster \  --cluster-context=a-cluster-context \  --host-cluster-context=f-cluster-context# 加入 k8s-b-cluster$ kubefed join f-b-cluster \  --cluster-context=b-cluster-context \  --host-cluster-context=f-cluster-context$ kubectl get clusterNAME          AGEf-a-cluster   57sf-b-cluster   53s</code></pre><h2 id="測試-Federation-叢集"><a href="#測試-Federation-叢集" class="headerlink" title="測試 Federation 叢集"></a>測試 Federation 叢集</h2><p>這邊利用 Nginx Deployment 來進行測試，先簡單建立一個副本為 4 的 Nginx：</p><pre><code class="sh">$ kubectl config use-context federation$ kubectl create ns default$ kubectl run nginx --image nginx --port 80 --replicas=4</code></pre><p>查看 Cluster A：</p><pre><code class="sh">$ kubectl --context=a-cluster-context get poNAME                     READY     STATUS    RESTARTS   AGEnginx-7587c6fdb6-dpjv5   1/1       Running   0          25snginx-7587c6fdb6-sjv8v   1/1       Running   0          25s</code></pre><p>查看 Cluster B：</p><pre><code class="sh">$ kubectl --context=b-cluster-context get poNAME                     READY     STATUS    RESTARTS   AGEnginx-7587c6fdb6-dv45v   1/1       Running   0          1mnginx-7587c6fdb6-wxsmq   1/1       Running   0          1m</code></pre><p>其他可測試功能：</p><ul><li>設定 Replica set preferences，參考 <a href="https://kubernetes.io/docs/tasks/administer-federation/replicaset/#spreading-replicas-in-underlying-clusters" target="_blank" rel="noopener">Spreading Replicas in Underlying Clusters</a>。</li><li>Federation 在 v1.7+ 加入了 <a href="https://kubernetes.io/docs/tasks/administer-federation/cluster/#clusterselector-annotation" target="_blank" rel="noopener">ClusterSelector Annotation</a></li><li><a href="https://kubernetes.io/docs/tasks/federation/set-up-placement-policies-federation/#deploying-federation-and-configuring-an-external-policy-engine" target="_blank" rel="noopener">Scheduling Policy</a>。</li></ul><h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul><li><a href="https://github.com/emaildanwilson/minikube-federation" target="_blank" rel="noopener">Minikube Federation</a></li><li><a href="http://cgrant.io/tutorials/gcp/compute/gke/global-kubernetes-three-steps/" target="_blank" rel="noopener">Global Kubernetes in 3 Steps</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/cluster-administration/federation/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes Federation(聯邦)&lt;/a&gt; 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="Federation" scheme="https://readailib.github.io/tags/Federation/"/>
    
  </entry>
  
  <entry>
    <title>利用 Kubeflow 來管理 TensorFlow 應用程式</title>
    <link href="https://readailib.github.io/2018/03/15/tensorflow/kubeflow/"/>
    <id>https://readailib.github.io/2018/03/15/tensorflow/kubeflow/</id>
    <published>2018-03-15T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.231Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/kubeflow/kubeflow" target="_blank" rel="noopener">Kubeflow</a> 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。</p><p><img src="/images/kubeflow/logo.png" alt></p><a id="more"></a><p>該工具能夠建立以下幾項功能：</p><ul><li>用於建議與管理互動式 Jupyter notebook 的 JupyterHub。</li><li>可以設定使用 CPU 或 GPU，並透過單一設定調整單個叢集大小的 Tensorflow Training Controller。</li><li>用 TensorFlow Serving 容器來提供模型服務。</li></ul><p>Kubeflow 目標是透過 Kubernetes 的特性使機器學習更加簡單與快速：</p><ul><li>在不同基礎設施上實現簡單、可重複的攜帶性部署(Laptop <-> ML rig <-> Training cluster <-> Production cluster)。</-></-></-></li><li>部署與管理松耦合的微服務。</li><li>根據需求進行縮放。</li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>vCPU</th><th>RAM</th><th>Extra Device</th></tr></thead><tbody><tr><td>172.22.132.51</td><td>gpu-node1</td><td>8</td><td>16G</td><td>GTX 1060 3G</td></tr><tr><td>172.22.132.52</td><td>gpu-node2</td><td>8</td><td>16G</td><td>GTX 1060 3G</td></tr><tr><td>172.22.132.53</td><td>master1</td><td>8</td><td>16G</td><td>無</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>使用 Kubeflow 之前，需要確保以下條件達成：</p><ul><li>所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker，請參考 <a href="https://kairen.github.io/2018/02/17/container/docker-nvidia-install/" target="_blank" rel="noopener">安裝 Nvidia Docker 2</a>。</li><li>(option)所有 GPU 節點安裝 cuDNN v7.1.2 for CUDA 9.1，請至 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">NVIDIA cuDNN</a> 下載。</li></ul><pre><code class="sh">$ tar xvf cudnn-9.1-linux-x64-v7.1.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</code></pre><ul><li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集，請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li><li>Kubernetes 叢集需要安裝 NVIDIA Device Plugins，請參考 <a href="https://kairen.github.io/2018/03/01/kubernetes/k8s-device-plugin/" target="_blank" rel="noopener">安裝 Kubernetes NVIDIA Device Plugins</a>。</li><li>建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用：</li></ul><pre><code class="sh"># 在 master 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data$ echo &quot;/nfs-data *(rw,sync,no_root_squash,no_subtree_check)&quot; | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common</code></pre><ul><li>安裝<code>ksonnet 0.9.2</code>，請參考以下：</li></ul><pre><code class="sh">$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.9.2/ks_0.9.2_linux_amd64.tar.gz$ tar xvf ks_0.9.2_linux_amd64.tar.gz$ sudo cp ks_0.9.2_linux_amd64/ks /usr/local/bin/$ ks versionksonnet version: 0.9.2jsonnet version: v0.9.5client-go version: 1.8</code></pre><h2 id="部署-Kubeflow"><a href="#部署-Kubeflow" class="headerlink" title="部署 Kubeflow"></a>部署 Kubeflow</h2><p>本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。首先在<code>master</code>節點初始化 ksonnet 應用程式目錄：</p><pre><code class="sh">$ ks init my-kubeflow</code></pre><blockquote><p>如果遇到以下問題的話，可以自己建立 GitHub Token 來存取 GitHub API，請參考 <a href="https://ksonnet.io/docs/tutorial#troubleshooting-github-rate-limiting-errors" target="_blank" rel="noopener">Github rate limiting errors</a>。</p><pre><code class="sh">ERROR GET https://api.github.com/repos/ksonnet/parts/commits/master: 403 API rate limit exceeded for 122.146.93.152.</code></pre></blockquote><p>接著安裝 Kubeflow 套件至應用程式目錄：</p><pre><code class="sh">$ cd my-kubeflow$ ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow$ ks pkg install kubeflow/core$ ks pkg install kubeflow/tf-serving$ ks pkg install kubeflow/tf-job</code></pre><p>然後建立 Kubeflow 核心元件，該元件包含 JupyterHub 與 TensorFlow job controller：</p><pre><code class="sh">$ kubectl create namespace kubeflow$ kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator$ ks generate core kubeflow-core --name=kubeflow-core --namespace=kubeflow# 啟動收集匿名使用者使用量資訊，如果不想開啟則忽略$ ks param set kubeflow-core reportUsage true$ ks param set kubeflow-core usageId $(uuidgen)# 部署 Kubeflow$ ks param set kubeflow-core jupyterHubServiceType LoadBalancer$ ks apply default -c kubeflow-core</code></pre><blockquote><p>詳細使用量資訊請參考 <a href="https://github.com/kubeflow/kubeflow/blob/master/user_guide.md#usage-reporting" target="_blank" rel="noopener">Usage Reporting</a>。</p></blockquote><p>完成後檢查 Kubeflow 元件部署結果：</p><pre><code class="sh">$ kubectl -n kubeflow get po -o wideNAME                                  READY     STATUS    RESTARTS   AGE       IP               NODEambassador-7956cf5c7f-6hngq           2/2       Running   0          34m       10.244.41.132    kube-gpu-node1ambassador-7956cf5c7f-jgxnd           2/2       Running   0          34m       10.244.152.134   kube-gpu-node2ambassador-7956cf5c7f-jww2d           2/2       Running   0          34m       10.244.41.133    kube-gpu-node1spartakus-volunteer-8c659d4f5-bg7kn   1/1       Running   0          34m       10.244.152.135   kube-gpu-node2tf-hub-0                              1/1       Running   0          34m       10.244.152.133   kube-gpu-node2tf-job-operator-78757955b-2jbdh       1/1       Running   0          34m       10.244.41.131    kube-gpu-node1</code></pre><p>這時候就可以登入 Jupyter Notebook，但這邊需要修改 Kubernetes Service，透過以下指令進行：</p><pre><code class="sh">$ kubectl -n kubeflow get svc -o wideNAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE       SELECTORambassador         ClusterIP   10.101.157.91    &lt;none&gt;        80/TCP     45m       service=ambassadorambassador-admin   ClusterIP   10.107.24.138    &lt;none&gt;        8877/TCP   45m       service=ambassadork8s-dashboard      ClusterIP   10.111.128.104   &lt;none&gt;        443/TCP    45m       k8s-app=kubernetes-dashboardtf-hub-0           ClusterIP   None             &lt;none&gt;        8000/TCP   45m       app=tf-hubtf-hub-lb          ClusterIP   10.105.47.253    &lt;none&gt;        80/TCP     45m       app=tf-hub# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc tf-hub-lb...spec:  type: LoadBalancer  externalIPs:  - 172.22.132.41...</code></pre><h2 id="測試-Kubeflow"><a href="#測試-Kubeflow" class="headerlink" title="測試 Kubeflow"></a>測試 Kubeflow</h2><p>開始測試前先建立一個 NFS PV 來提供給 Kubeflow Jupyter 使用：</p><pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata:  name: nfs-pvspec:  capacity:    storage: 20Gi  accessModes:    - ReadWriteOnce  nfs:    server: 172.22.132.41    path: /nfs-dataEOF</code></pre><p>完成後連接 <code>http://Master_IP</code>，並輸入<code>任意帳號密碼</code>進行登入。</p><p><img src="/images/kubeflow/1.png" alt></p><p>登入後點選<code>Start My Server</code>按鈕來建立 Server 的 Spawner options，預設會有多種映像檔可以使用：</p><ul><li>CPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-cpu。</li><li>GPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-gpu。</li></ul><p>這邊也使用以下 GCP 建構的映像檔做測試使用(GPU 當前為 CUDA 8)：</p><ul><li>gcr.io/kubeflow/tensorflow-notebook-cpu:latest</li><li>gcr.io/kubeflow/tensorflow-notebook-gpu:latest</li></ul><p>若 CUDA 版本不同，請自行修改 <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/blob/master/example/tensorflow-notebook-image" target="_blank" rel="noopener">GCP Tensorflow Notebook image</a> 或是 <a href="https://github.com/kubeflow/kubeflow/tree/master/components/k8s-model-server/images" target="_blank" rel="noopener">Kubeflow Tensorflow Notebook image </a>重新建構。</p><p>如果使用 GPU 請執行以下指令確認是否可被分配資源：</p><pre><code class="sh">$ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu&quot;NAME               GPUkube-gpu-master1   &lt;none&gt;kube-gpu-node1     1kube-gpu-node2     1</code></pre><p>最後點選<code>Spawn</code>來完成建立 Server，如下圖所示：</p><p><img src="/images/kubeflow/2.png" alt></p><blockquote><p>這邊先用 CPU 進行測試，由於本篇是安裝 CUDA 9.1 + cuDNN 7，因此要自己建構映像檔。</p></blockquote><p>接著等 Kubernetes 下載映像檔後，就會正常啟動，如下圖所示：</p><p><img src="/images/kubeflow/3.png" alt></p><p>當正常啟動後，點選<code>New &gt; Python 3</code>建立一個 Notebook 並貼上以下範例程式：</p><pre><code class="python">from __future__ import print_functionimport tensorflow as tfhello = tf.constant(&#39;Hello TensorFlow!&#39;)s = tf.Session()print(s.run(hello))</code></pre><p>正確執行會如以下圖所示：</p><p><img src="/images/kubeflow/4.png" alt></p><blockquote><p>若想關閉叢集的話，可以點選<code>Control Plane</code>。</p></blockquote><p>另外由於 Kubeflow 會安裝 TF Operator 來管理 TFJob，這邊可以透過 Kubernetes 來手動建立 Job：</p><pre><code class="sh">$ kubectl create -f https://raw.githubusercontent.com/kubeflow/tf-operator/master/examples/tf_job.yaml$ kubectl get poNAME                              READY     STATUS    RESTARTS   AGEexample-job-ps-qq6x-0-pdx7v       1/1       Running   0          5mexample-job-ps-qq6x-1-2mpfp       1/1       Running   0          5mexample-job-worker-qq6x-0-m5fm5   1/1       Running   0          5m</code></pre><p>若想從 Kubernetes 叢集刪除 Kubeflow 相關元件的話，可執行下列指令達成：</p><pre><code class="sh">$ ks delete default -c kubeflow-core</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubeflow&lt;/a&gt; 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/kubeflow/logo.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://readailib.github.io/categories/TensorFlow/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
      <category term="TensorFlow" scheme="https://readailib.github.io/tags/TensorFlow/"/>
    
      <category term="GPU" scheme="https://readailib.github.io/tags/GPU/"/>
    
      <category term="DL/ML" scheme="https://readailib.github.io/tags/DL-ML/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes NVIDIA Device Plugins</title>
    <link href="https://readailib.github.io/2018/03/01/kubernetes/nvidia-device-plugin/"/>
    <id>https://readailib.github.io/2018/03/01/kubernetes/nvidia-device-plugin/</id>
    <published>2018-03-01T09:08:54.000Z</published>
    <updated>2019-03-05T14:37:50.225Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a> 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">Device Plugins 介面</a>來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。</p><p>P.S. 傳統的<code>alpha.kubernetes.io/nvidia-gpu</code>將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。<br><a id="more"></a></p><h2 id="Device-Plugins-原理"><a href="#Device-Plugins-原理" class="headerlink" title="Device Plugins 原理"></a>Device Plugins 原理</h2><p>Device  Plugins 主要提供了一個 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">gRPC 介面</a>來給廠商實現<code>ListAndWatch()</code>與<code>Allocate()</code>等 gRPC 方法，並監聽節點的<code>/var/lib/kubelet/device-plugins/</code>目錄中的 gRPC Server Unix Socket，這邊可以參考官方文件 <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a>。一旦啟動 Device Plugins 時，透過 Kubelet Unix Socket 註冊，並提供該 plugin 的 Unix Socket 名稱、API 版本號與插件資源名稱(vendor-domain/resource，例如 nvidia.com/gpu)，接著 Kubelet 會將這些曝露到 Node 狀態以便 Scheduler 使用。</p><p>Unix Socket 範例：</p><pre><code class="sh">$ ls /var/lib/kubelet/device-plugins/kubelet_internal_checkpoint  kubelet.sock  nvidia.sock</code></pre><p>一些 Device Plugins 列表：</p><ul><li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noopener">NVIDIA GPU</a></li><li><a href="https://github.com/hustcat/k8s-rdma-device-plugin" target="_blank" rel="noopener">RDMA</a></li><li><a href="https://github.com/kubevirt/kubernetes-device-plugins" target="_blank" rel="noopener">Kubevirt</a></li><li><a href="https://github.com/vikaschoudhary16/sfc-device-plugin" target="_blank" rel="noopener">SFC</a></li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>vCPU</th><th>RAM</th><th>Extra Device</th></tr></thead><tbody><tr><td>172.22.132.51</td><td>gpu-node1</td><td>8</td><td>16G</td><td>GTX 1060 3G</td></tr><tr><td>172.22.132.52</td><td>gpu-node2</td><td>8</td><td>16G</td><td>GTX 1060 3G</td></tr><tr><td>172.22.132.53</td><td>master1</td><td>8</td><td>16G</td><td>無</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝 Device Plugin 前，需要確保以下條件達成：</p><ul><li>所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker。請參考 <a href="https://kairen.github.io/2018/02/17/container/docker-nvidia-install/" target="_blank" rel="noopener">安裝 Nvidia Docker 2</a>。</li><li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h2 id="安裝-NVIDIA-Device-Plugin"><a href="#安裝-NVIDIA-Device-Plugin" class="headerlink" title="安裝 NVIDIA Device Plugin"></a>安裝 NVIDIA Device Plugin</h2><p>若上述要求以符合，再開始前需要在<code>每台 GPU worker 節點</code>修改<code>/lib/systemd/system/docker.service</code>檔案，將 Docker default runtime 改成 nvidia，依照以下內容來修改：</p><pre><code class="sh">...ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia...</code></pre><blockquote><p>這邊也可以修改<code>/etc/docker/daemon.json</code>檔案，請參考 <a href="https://docs.docker.com/config/daemon/" target="_blank" rel="noopener">Configure and troubleshoot the Docker daemon</a>。</p></blockquote><p>完成後儲存，並重新啟動 Docker：</p><pre><code class="sh">$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker</code></pre><p>接著由於 v1.9 版本的 Device Plugins 還是處於 Alpha 中，因此需要手動修改<code>每台 GPU worker 節點</code>的 kubelet drop-in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>檔案，這邊在<code>KUBELET_CERTIFICATE_ARGS</code>加入一行 args：</p><pre><code class="sh">...Environment=&quot;KUBELET_EXTRA_ARGS=--feature-gates=DevicePlugins=true&quot;...</code></pre><p>完成後儲存，並重新啟動 kubelet：</p><pre><code class="sh">$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet</code></pre><p>確認上述完成，接著在<code>Master</code>節點安裝 NVIDIA Device Plugins，透過以下方式來進行：</p><pre><code class="sh">$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.ymldaemonset &quot;nvidia-device-plugin-daemonset&quot; created$ kubectl -n kube-system get po -o wideNAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE...nvidia-device-plugin-daemonset-bncw2       1/1       Running   0          2m        10.244.41.135    kube-gpu-node1nvidia-device-plugin-daemonset-ddnhd       1/1       Running   0          2m        10.244.152.132   kube-gpu-node2</code></pre><h2 id="測試-GPU"><a href="#測試-GPU" class="headerlink" title="測試 GPU"></a>測試 GPU</h2><p>首先執行以下指令確認是否可被分配資源：</p><pre><code class="sh">$ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu&quot;NAME               GPUmaster1           &lt;none&gt;gpu-node1          1gpu-node2          1</code></pre><p>當 NVIDIA Device Plugins 部署完成後，即可建立一個簡單範例來進行測試：</p><pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:  name: gpu-podspec:  restartPolicy: Never  containers:  - image: nvidia/cuda    name: cuda    command: [&quot;nvidia-smi&quot;]    resources:      limits:        nvidia.com/gpu: 1EOFpod &quot;gpu-pod&quot; created$ kubectl get po -a -o wideNAME      READY     STATUS      RESTARTS   AGE       IP              NODEgpu-pod   0/1       Completed   0          50s       10.244.41.136   kube-gpu-node1$ kubectl logs gpu-podThu Mar 15 07:28:45 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.30                 Driver Version: 390.30                    ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A ||  0%   41C    P8    10W / 120W |      0MiB /  3019MiB |      1%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================||  No running processes found                                                 |+-----------------------------------------------------------------------------+</code></pre><p>從上面結果可以看到 Kubernetes Pod 正確的使用到 NVIDIA GPU，這邊也可以利用 TensorFlow 來進行測試，新增一個檔案<code>tf-gpu-dp.yml</code>加入以下內容：</p><pre><code class="yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: tf-gpuspec:  replicas: 1  selector:    matchLabels:      app: tf-gpu  template:    metadata:     labels:       app: tf-gpu    spec:      containers:      - name: tensorflow        image: tensorflow/tensorflow:latest-gpu        ports:        - containerPort: 8888        resources:          limits:            nvidia.com/gpu: 1</code></pre><p>利用 kubectl 建立 Deployment，並曝露 Jupyter port：</p><pre><code class="sh">$ kubectl create -f tf-gpu-dp.ymldeployment &quot;tf-gpu&quot; created$ kubectl expose deploy tf-gpu --type LoadBalancer --external-ip=172.22.132.53 --port 8888 --target-port 8888service &quot;tf-gpu&quot; exposed$ kubectl get po,svc -o wideNAME                         READY     STATUS    RESTARTS   AGE       IP               NODEpo/tf-gpu-6f9464f94b-pq8t9   1/1       Running   0          1m        10.244.152.133   kube-gpu-node2NAME             TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE       SELECTORsvc/kubernetes   ClusterIP      10.96.0.1        &lt;none&gt;          443/TCP          23h       &lt;none&gt;svc/tf-gpu       LoadBalancer   10.105.104.183   172.22.132.53   8888:30093/TCP   12s       app=tf-gpu</code></pre><blockquote><p>確認無誤後，透過 logs 指令取得 token，並登入<code>Jupyter Notebook</code>，這邊 IP 為 &lt;master1_ip&gt;:8888。</p></blockquote><p>這邊執行一個簡單範例，並在用 logs 指令查看就能看到 Pod 透過 NVIDIA Device Plugins 使用 GPU：</p><pre><code class="sh">$ kubectl logs -f tf-gpu-6f9464f94b-pq8t9...2018-03-15 07:37:22.022052: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-03-15 07:37:22.155254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-03-15 07:37:22.155565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845pciBusID: 0000:01:00.0totalMemory: 2.95GiB freeMemory: 2.88GiB2018-03-15 07:37:22.155586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 02018-03-15 07:37:22.346590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)</code></pre><p>最後因為目前 Pod 會綁整張 GPU 來使用，因此當無多餘顯卡時就讓 Pod 處於 Pending：</p><pre><code class="sh">$ kubectl scale deploy tf-gpu --replicas=3$ kubectl get po -o wideNAME                      READY     STATUS    RESTARTS   AGE       IP               NODEtf-gpu-6f9464f94b-42xcf   0/1       Pending   0          4s        &lt;none&gt;           &lt;none&gt;tf-gpu-6f9464f94b-nxdw5   1/1       Running   0          12s       10.244.41.138    kube-gpu-node1tf-gpu-6f9464f94b-pq8t9   1/1       Running   0          5m        10.244.152.133   kube-gpu-node2</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Device Plugins&lt;/a&gt; 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 &lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Device Plugins 介面&lt;/a&gt;來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。&lt;/p&gt;
&lt;p&gt;P.S. 傳統的&lt;code&gt;alpha.kubernetes.io/nvidia-gpu&lt;/code&gt;將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。&lt;br&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/categories/Kubernetes/"/>
    
    
      <category term="NVIDIA GPU" scheme="https://readailib.github.io/tags/NVIDIA-GPU/"/>
    
      <category term="Kubernetes" scheme="https://readailib.github.io/tags/Kubernetes/"/>
    
  </entry>
  
</feed>
