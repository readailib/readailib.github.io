<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShenH.&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/543fc8a83d9b480e5f69c3842db96518</icon>
  <subtitle>Learn Anything, Anytime, Anywhere~</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://readailib.com/"/>
  <updated>2019-03-07T07:13:29.046Z</updated>
  <id>https://readailib.com/</id>
  
  <author>
    <name>ShenHengheng</name>
    <email>shenhengheng17g@ict.ac.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在树莓派上编译安装Go版本的Tensorflow</title>
    <link href="https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/"/>
    <id>https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/</id>
    <published>2019-03-06T16:14:16.000Z</published>
    <updated>2019-03-07T07:13:29.046Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-Used-Hardwares-and-Softwares"><a href="#0-Used-Hardwares-and-Softwares" class="headerlink" title="0. Used Hardwares and Softwares"></a>0. Used Hardwares and Softwares</h1><p>All steps were taken on my <strong>Raspberry Pi 3 B</strong> model with:</p><ul><li>Minimum GPU memory allocated (16MB)</li><li><strong>1GB</strong> of swap memory</li><li>External USB HDD (as root partition)</li></ul><a id="more"></a><p>and software versions were:</p><ul><li>Raspbian (Stretch) / gcc 6.3.0</li><li>Tensorflow 1.3.0</li><li>Protobuf 3.1.0</li><li>Bazel 0.5.1</li></ul><p>Before the beginning, I had to install dependencies:</p><h3 id="for-protobuf"><a href="#for-protobuf" class="headerlink" title="for protobuf"></a>for protobuf</h3><pre><code class="bash">$ sudo apt-get install autoconf automake libtool</code></pre><h3 id="for-bazel"><a href="#for-bazel" class="headerlink" title="for bazel"></a>for bazel</h3><pre><code class="bash">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip oracle-java8-jdk</code></pre><hr><h1 id="1-Install-Protobuf"><a href="#1-Install-Protobuf" class="headerlink" title="1. Install Protobuf"></a>1. Install Protobuf</h1><p>I cloned the protobuf’s repository:</p><pre><code class="bash">$ git clone https://github.com/google/protobuf.git</code></pre><p>and started building:</p><pre><code class="bash">$ cd protobuf$ git checkout v3.1.0$ ./autogen.sh$ ./configure$ make -j 4$ sudo make install$ sudo ldconfig</code></pre><p>It took less than an hour to finish.</p><p>I could see the version of installed protobuf with:</p><pre><code class="bash">$ protoc --versionlibprotoc 3.1.0</code></pre><h1 id="2-Install-Bazel"><a href="#2-Install-Bazel" class="headerlink" title="2. Install Bazel"></a>2. Install Bazel</h1><h2 id="a-download"><a href="#a-download" class="headerlink" title="a. download"></a>a. download</h2><p>I got a zip file of bazel from <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">here</a> and unzipped it:</p><pre><code class="bash">$ wget https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip$ unzip -d bazel bazel-0.5.1-dist.zip</code></pre><h2 id="b-edit-bootstrap-files"><a href="#b-edit-bootstrap-files" class="headerlink" title="b. edit bootstrap files"></a>b. edit bootstrap files</h2><p>In the unzipped directory, I opened the <code>scripts/bootstrap/compile.sh</code> file:</p><pre><code class="bash">$ cd bazel$ vi scripts/bootstrap/compile.sh</code></pre><p>searched for lines that looked like following:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot;</code></pre><p>and appended <code>-J-Xmx500M</code> to the last line so that the whole lines would look like:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot; -J-Xmx500M</code></pre><p>It was for enlarging the max heap size of Java.</p><h2 id="c-compile"><a href="#c-compile" class="headerlink" title="c. compile"></a>c. compile</h2><p>After that, started building with:</p><pre><code class="bash">$ chmod u+w ./* -R$ ./compile.sh</code></pre><p>It also took about an hour.</p><h2 id="d-install"><a href="#d-install" class="headerlink" title="d. install"></a>d. install</h2><p>After the compilation had finished, I could find the compiled binary in <code>output</code> directory.</p><p>Copied it into <code>/usr/local/bin</code> directory:</p><pre><code class="bash">$ sudo cp output/bazel /usr/local/bin/</code></pre><h1 id="3-Build-libtensorflow-so"><a href="#3-Build-libtensorflow-so" class="headerlink" title="3. Build libtensorflow.so"></a>3. Build libtensorflow.so</h1><p>(I referenced <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md" target="_blank" rel="noopener">this document</a> for following processes)</p><h2 id="a-download-1"><a href="#a-download-1" class="headerlink" title="a. download"></a>a. download</h2><p>Got the tensorflow go code with:</p><pre><code class="bash">$ go get -d github.com/tensorflow/tensorflow/tensorflow/go</code></pre><h2 id="b-edit-files"><a href="#b-edit-files" class="headerlink" title="b. edit files"></a>b. edit files</h2><p>In the downloaded directory, I checked out the latest tag and replaced <code>lib64</code> to <code>lib</code> in the files with:</p><pre><code class="bash">$ cd ${GOPATH}/src/github.com/tensorflow/tensorflow$ git fetch --all --tags --prune$ git checkout tags/v1.3.0$ grep -Rl &#39;lib64&#39; | xargs sed -i &#39;s/lib64/lib/g&#39;</code></pre><p>Raspberry Pi still runs on 32bit OS, so they had to be changed like this.</p><p>After that, I commented <code>#define IS_MOBILE_PLATFORM</code> out in <code>tensorflow/core/platform/platform.h</code>:</p><pre><code class="c">// Since there&#39;s no macro for the Raspberry Pi, assume we&#39;re on a mobile// platform if we&#39;re compiling for the ARM CPU.//#define IS_MOBILE_PLATFORM    // &lt;= commented this line</code></pre><p>If it is not commented out, bazel will build for mobile platforms like <code>iOS</code> or <code>Android</code>, not Raspberry Pi.</p><p>To do this easily, just run:</p><pre><code class="bash">$ sed -i &quot;s|#define IS_MOBILE_PLATFORM|//#define IS_MOBILE_PLATFORM|g&quot; tensorflow/core/platform/platform.h</code></pre><p>Finally, it was time to configure and build tensorflow.</p><h2 id="c-configure-and-build"><a href="#c-configure-and-build" class="headerlink" title="c. configure and build"></a>c. configure and build</h2><pre><code class="bash">$ ./configure</code></pre><p>I had to answer to some questions here.</p><p>Then I started building <code>libtensorflow.so</code> with:</p><pre><code class="bash">$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so</code></pre><p>My Pi became unresponsive many times during this process, but I kept it going on.</p><h2 id="d-install-1"><a href="#d-install-1" class="headerlink" title="d. install"></a>d. install</h2><p>After a long time of struggle, (it took nearly 7 hours for me!)</p><p>I finally got <code>libtensorflow.so</code> compiled in <code>bazel-bin/tensorflow/</code>.</p><p>So I copied it into <code>/usr/local/lib/</code>:</p><pre><code class="bash">$ sudo cp ./bazel-bin/tensorflow/libtensorflow.so /usr/local/lib/$ sudo chmod 644 /usr/local/lib/libtensorflow.so$ sudo ldconfig</code></pre><p>All done. Time to test!</p><h1 id="4-Go-Test"><a href="#4-Go-Test" class="headerlink" title="4. Go Test"></a>4. Go Test</h1><p>I ran a test for validating the installation:</p><pre><code class="bash">$ go test github.com/tensorflow/tensorflow/tensorflow/go</code></pre><p>then I could see:</p><pre><code class="bash">ok      github.com/tensorflow/tensorflow/tensorflow/go  0.350s</code></pre><p>Ok, it works!</p><p><strong>Edit</strong>: As <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go#generate-wrapper-functions-for-ops" target="_blank" rel="noopener">this instruction</a> says, I had to regenerate operations before the test:</p><pre><code class="bash">$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op</code></pre><h1 id="5-Further-Test"><a href="#5-Further-Test" class="headerlink" title="5. Further Test"></a>5. Further Test</h1><p>I wanted to see a simple go program running, so I wrote this code:</p><pre><code class="go">// sample.gopackage mainimport (    &quot;fmt&quot;    tf &quot;github.com/tensorflow/tensorflow/tensorflow/go&quot;)// Sorry - I don&#39;t have a good example yet :-Pfunc main() {    tensor, _ := tf.NewTensor(int64(42))    if v, ok := tensor.Value().(int64); ok {        fmt.Printf(&quot;The answer to the life, universe, and everything: %v\n&quot;, v)    }}</code></pre><p>and ran it with <code>go run sample.go</code>:</p><pre><code>The answer to the life, universe, and everything: 42</code></pre><p>See the result?</p><p>From now on, I can write tensorflow applications in go, on Raspberry Pi! :-)</p><hr><h1 id="98-Trouble-shooting"><a href="#98-Trouble-shooting" class="headerlink" title="98. Trouble shooting"></a>98. Trouble shooting</h1><h2 id="Build-failure-due-to-a-problem-with-Eigen"><a href="#Build-failure-due-to-a-problem-with-Eigen" class="headerlink" title="Build failure due to a problem with Eigen"></a>Build failure due to a problem with Eigen</h2><p>Back in the day with <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.2.0" target="_blank" rel="noopener">Tensorflow 1.2.0</a>, I encountered <a href="https://github.com/tensorflow/tensorflow/issues/9697" target="_blank" rel="noopener">this issue</a> while building, but it’s still not fixed yet in <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0" target="_blank" rel="noopener">1.3.0</a>.</p><p>So I had to work around this problem again by editing <code>tensorflow/workspace.bzl</code>from:</p><pre><code class="bzl">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,    ],    sha256 = &quot;ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4&quot;,    strip_prefix = &quot;eigen-eigen-f3a22f35b044&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>to:</p><pre><code class="bash">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,    ],    sha256 = &quot;a34b208da6ec18fa8da963369e166e4a368612c14d956dd2f9d7072904675d9b&quot;,    strip_prefix = &quot;eigen-eigen-d781c1de9834&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>and starting again from the beginning:</p><pre><code class="bash">$ bazel clean$ ./configure$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so...</code></pre><p>Then I could build it without further problems.</p><p>I hope it would be fixed on future releases.</p><hr><h1 id="99-Wrap-up"><a href="#99-Wrap-up" class="headerlink" title="99. Wrap-up"></a>99. Wrap-up</h1><p>Installing TensorFlow on Raspberry Pi is not easy yet. (There’s <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi" target="_blank" rel="noopener">a kind project</a> which makes it super easy though!)</p><p>Installing <code>libtensorflow.so</code> is a lot more difficult, because it takes too much time to build it.</p><p>But it is worth trying; managing TensorFlow graphs in golang will be handy for people who don’t love python - just like me.</p><hr><h1 id="999-If-you-need-one"><a href="#999-If-you-need-one" class="headerlink" title="999. If you need one,"></a>999. If you need one,</h1><p>You don’t have time to build it yourself, but still need the compiled file?</p><p>Good, take it <a href="https://github.com/meinside/libtensorflow.so-raspberrypi/releases" target="_blank" rel="noopener">here</a>.</p><p>I cannot promise, but will try keeping it up-to-date whenever a newer version of tensorflow comes out.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-Used-Hardwares-and-Softwares&quot;&gt;&lt;a href=&quot;#0-Used-Hardwares-and-Softwares&quot; class=&quot;headerlink&quot; title=&quot;0. Used Hardwares and Softwares&quot;&gt;&lt;/a&gt;0. Used Hardwares and Softwares&lt;/h1&gt;&lt;p&gt;All steps were taken on my &lt;strong&gt;Raspberry Pi 3 B&lt;/strong&gt; model with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimum GPU memory allocated (16MB)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1GB&lt;/strong&gt; of swap memory&lt;/li&gt;
&lt;li&gt;External USB HDD (as root partition)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="Tensorflow" scheme="https://readailib.com/tags/Tensorflow/"/>
    
      <category term="ARM" scheme="https://readailib.com/tags/ARM/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
  </entry>
  
  <entry>
    <title>在树莓派kubernetes集群部署gRPC框架编写的微服务</title>
    <link href="https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/"/>
    <id>https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/</id>
    <published>2019-03-04T16:14:16.000Z</published>
    <updated>2019-03-06T00:50:11.802Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.</p><a id="more"></a><p>本次教程需要准备:</p><ul><li>一个启动好的树莓派kubernetes集群(参考: <a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a>)</li><li>protobuf工具(参考 <a href="https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可" target="_blank" rel="noopener">https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可</a>)</li><li>Docker (参考: <a href="https://docs.docker.com/engine/installation/" target="_blank" rel="noopener">https://docs.docker.com/engine/installation/</a>)</li><li>kubectl工具 (参考: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>)</li></ul><p>本文章参考: </p><ul><li><a href="https://outcrawl.com/getting-started-microservices-go-grpc-kubernetes" target="_blank" rel="noopener">Getting Started with Microservices using Go, gRPC and Kubernetes</a></li></ul><p>本文章的实例代码:</p><ul><li><a href="https://github.com/rh01/grpc-microservice-k8s" target="_blank" rel="noopener">https://github.com/rh01/grpc-microservice-k8s</a></li></ul><h2 id="定义我们的protobuf文件"><a href="#定义我们的protobuf文件" class="headerlink" title="定义我们的protobuf文件"></a>定义我们的protobuf文件</h2><p>protobuf是google的一个序列化结构化数据工具,它可以让人们定义好相关的结构,使用protoc工具自动生成对应的代码.类似的结构化工具还有thrift.</p><blockquote><p>微服务:这里我主要实现一个最大公约数的功能,输入两个数值,返回这两个数的最大公约数.</p></blockquote><p>这里既然使用gRPC来做,那么主要使用rpc来实现服务调用,因为rpc实现的是服务之间的同步调用，即客户端调用服务并等待响应。gRPC是提供RPC功能的框架之一。此时我们需要使用Protocol Buffer的接口定义语言中编写消息类型和服务的代码并进行编译。</p><p>下面就是我们使用protobuf语言定义的消息类型和服务.(具体参考: <a href="https://grpc.io/docs/quickstart/go.html)" target="_blank" rel="noopener">https://grpc.io/docs/quickstart/go.html)</a></p><pre><code class="bash">$ mkdir -pv ~/go/src/github.com/rh01/mini-deploy-app/pb$ cd ~/go/src/github.com/rh01/mini-deploy-app/pb$ vim pb.proto</code></pre><pre><code class="yaml">syntax = &quot;proto3&quot;; // protobuf 版本package pb;        // 代码生成的package名字// 定义的请求消息体message GCDRequest {    uint64 a = 1;    uint64 b = 2;}// 定义的响应消息体message GCDResponse {    uint64 result = 1;}// 调用的远程服务,这是client请求server端的远程计算服务service GCDService {    rpc Compute (GCDRequest) returns (GCDResponse) {}}</code></pre><p>接下来我们需要使用 protoc 生成对应的服务代码</p><pre><code>$ protoc -I . --go_out=plugins=grpc:. ./*.proto</code></pre><blockquote class="colorquote warning"><p><strong>提前须知</strong>:</p><p>1). 执行上面的指令需要使用安装 grpc 和 proto-gen-go 工具,使用下面的命令:</p><pre><code class="bash">$ go get -u google.golang.org/grpc$ go get -u github.com/golang/protobuf/protoc-gen-go</code></pre><p>2). 将 $GOPATH/bin 目录添加到PATH环境变量中</p><pre><code class="bash">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$GOPATH</span>/bin</code></pre></blockquote><p>这时应该生成了 gcd.pb.go 程序.</p><h2 id="最大公约数服务"><a href="#最大公约数服务" class="headerlink" title="最大公约数服务"></a>最大公约数服务</h2><h3 id="定义服务端"><a href="#定义服务端" class="headerlink" title="定义服务端"></a>定义服务端</h3><p>gcd 服务将会使用上一步生成的代码进行实现gcd计算服务.</p><pre><code class="bash">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p gcd$ vim main.go</code></pre><pre><code class="go">package mainimport (    &quot;log&quot;    &quot;net&quot;    context &quot;golang.org/x/net/context&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)</code></pre><p>在main函数中主要定义server结构,并将其注册为server端,用于处理 gcd 计算的请求.然后启动grpc服务.</p><pre><code class="go">type server struct {}func main() {    lis, err := net.Listen(&quot;tcp&quot;, &quot;:3000&quot;)    if err != nil {        log.Fatalf(&quot;Failed to listen: %v&quot;, err)    }    s := grpc.NewServer()    pb.RegisterGCDServiceServer(s, &amp;server{})    reflection.Register(s)    if err := s.Serve(lis); err != nil {        log.Fatalf(&quot;Failed to serve: %v&quot;, err)    }}</code></pre><p>实现GCDServiceServer接口的 <code>Compute</code> 方法, server 结构对象的指针作为方法接受者.</p><pre><code class="Go">// gcd.pb.go// GCDServiceServer is the server API for GCDService service.type GCDServiceServer interface {    Compute(context.Context, *GCDRequest) (*GCDResponse, error)}func (s *server) Compute(ctx context.Context, r *pb.GCDRequest) (*pb.GCDResponse, error) {    a, b := r.A, r.B    for b != 0 {        a, b = b, a%b    }    return &amp;pb.GCDResponse{Result: a}, nil}</code></pre><h2 id="定义RESTFul客户端"><a href="#定义RESTFul客户端" class="headerlink" title="定义RESTFul客户端"></a>定义RESTFul客户端</h2><p>前端使用 <a href="https://github.com/gin-gonic/gin" target="_blank" rel="noopener">gin</a> 框架,主要是提供一个REST风格的访问方式和调用我们定义的gcd服务端执行实际的计算任务.</p><pre><code class="go">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p api$ vim main.go</code></pre><pre><code>package mainimport (    fmt &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;strconv&quot;    &quot;github.com/gin-gonic/gin&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;)func main() {    conn, err := grpc.Dial(&quot;gcd-service:3000&quot;, grpc.WithInsecure())    if err != nil {        log.Fatalf(&quot;Dial failed: %v&quot;, err)    }    gcdClient := pb.NewGCDServiceClient(conn)}</code></pre><p>上面的代码主要使用rpc的方式访问我们定义的服务端,此时的 gcd-service:3000 就是我们gcd服务端的 endpoint,这个就是服务地址,需要在kubernetes中定义.</p><pre><code class="go">    r := gin.Default()    r.GET(&quot;/gcd/:a/:b&quot;, func(c *gin.Context) {        // Parse parameters        a, err := strconv.ParseUint(c.Param(&quot;a&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter A&quot;})            return        }        b, err := strconv.ParseUint(c.Param(&quot;b&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter B&quot;})            return        }        // Call GCD service        req := &amp;pb.GCDRequest{A: a, B: b}        if res, err := gcdClient.Compute(c, req); err == nil {            c.JSON(http.StatusOK, gin.H{                &quot;result&quot;: fmt.Sprint(res.Result),            })        } else {            c.JSON(http.StatusInternalServerError, gin.H{&quot;error&quot;: err.Error()})        }    })</code></pre><p>接下来处理 /gcd/:a/:b 请求,读取参数 A 和 B,然后调用GCD服务.</p><p>最后运行我们的REST API端.启动一个API server.</p><pre><code class="go">    // Run HTTP server    if err := r.Run(&quot;:3000&quot;); err != nil {        log.Fatalf(&quot;Failed to run server: %v&quot;, err)    }</code></pre><h2 id="构建Docker镜像"><a href="#构建Docker镜像" class="headerlink" title="构建Docker镜像"></a>构建Docker镜像</h2><p>下面是我定义的Dockerfile文件,因为有两个服务,所以这里分成两个Docker镜像,一个是gcd服务,另外一个是提供RESTAPI访问的客户端api.</p><blockquote><p>有关Docker的多阶段构建参考: <a href="https://www.41sh.cn/?id=61" target="_blank" rel="noopener">[实战] 将golang编写的微服务部署在树莓派搭建的kubernetes集群</a></p></blockquote><pre><code class="dockerfile"># Dockerfile.gcdFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/gcdCOPY gcd .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/gcd-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/gcd-server /usr/local/bin/gcdEXPOSE 3000CMD [ &quot;gcd&quot; ]</code></pre><pre><code class="dockerfile"># Dockerfile.apiFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/apiCOPY api .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/api-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/api-server /usr/local/bin/apiEXPOSE 3000CMD [ &quot;api&quot; ]</code></pre><p>然后构建</p><pre><code class="bash">$ docker build -t rh02/apiserver:v1.0.0 -f Dockerfile.api . $ docker build -t rh02/gcdserver:v1.0.0 -f Dockerfile.gcd .</code></pre><h2 id="部署到kubernetes"><a href="#部署到kubernetes" class="headerlink" title="部署到kubernetes"></a>部署到kubernetes</h2><p>定义两个Deployment和对应的两个Service,并且将gcd服务的名字写成我们api服务调用的名字:gcd-service.</p><p>下面是gcd的deployment和service的定义: gcd.yaml</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: gcd-deployment  labels:    app: gcdspec:  selector:    matchLabels:      app: gcd  replicas: 3  template:    metadata:      labels:        app: gcd    spec:      containers:      - name: gcd        image: rh02/gcdserver:v1.0.0        imagePullPolicy: Always      ports:      - name: gcd-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: gcd-servicespec:  selector:    app: gcd  ports:  - port: 3000    targetPort: gcd-service</code></pre><p>创建api.yaml, service类型设置为NodePort,从而可以在集群外部也可以访问,对于GCD服务,类型设置为ClusterIP即可,只需要在集群内部访问就可以.</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: api-deployment  labels:    app: apispec:  selector:    matchLabels:      app: api  replicas: 1  template:    metadata:      labels:        app: api    spec:      containers:      - name: api        image: rh02/apiserver:v1.0.0        imagePullPolicy: Always      ports:      - name: api-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: api-servicespec:  type: NodePort  selector:    app: api  ports:  - port: 3000    targetPort: api-service</code></pre><p>使用kubectl创建两个资源:</p><pre><code class="bash">$ kubectl create -f api.yaml$ kubectl create -f gcd.yaml</code></pre><p>检查所有的Pod是否正在运行, 可以指定 <code>-w</code> 标记,查看启动的过程.</p><pre><code class="bash">$ kubectl get pods -wNAME                             READY     STATUS    RESTARTS   AGEapi-deployment-778049682-3vd0z   1/1       Running   0          3sgcd-deployment-544390878-0zgc8   1/1       Running   0          2sgcd-deployment-544390878-p78g0   1/1       Running   0          2sgcd-deployment-544390878-r26nx   1/1       Running   0          2s</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771763579640.png" alt="截图_2019-03-05_15-42-18.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771802295607.png" alt="截图_2019-03-05_15-43-02.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="kubernetes" scheme="https://readailib.com/tags/kubernetes/"/>
    
      <category term="Edge computing" scheme="https://readailib.com/tags/Edge-computing/"/>
    
      <category term="gRPC" scheme="https://readailib.com/tags/gRPC/"/>
    
      <category term="REST" scheme="https://readailib.com/tags/REST/"/>
    
  </entry>
  
  <entry>
    <title>部署微服务到树莓派的kubernetes集群</title>
    <link href="https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/"/>
    <id>https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/</id>
    <published>2019-02-28T17:02:07.000Z</published>
    <updated>2019-03-07T07:14:15.319Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。</p><p>本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>那么下面就开始吧，本次的环境主要分为两台机器，分别我的本地计算机（这里使用的Mac），另外就是我需要部署的环境（有三个树莓派节点组成的kubernetes集群）。</p><p>这里你需要：</p><ul><li>熟悉go语言</li><li>熟悉微服务</li><li>熟悉Docker</li><li>熟悉kubernetes的部署与基本管理任务</li><li>熟悉Linux</li></ul><p>本章主要参考了下面的文章：</p><ul><li><a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a></li><li><a href="https://github.com/rh01/deploy-golang-applicaton-on-kubernetes" target="_blank" rel="noopener">https://github.com/rh01/deploy-golang-applicaton-on-kubernetes</a></li><li><a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></li><li><a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></li></ul><h2 id="编写golang的应用"><a href="#编写golang的应用" class="headerlink" title="编写golang的应用"></a>编写golang的应用</h2><p><strong>本小节全部在我们本地的计算机中做的！</strong></p><p>在本地创建项目文件夹goappk8s，并且将该文件夹设置成临时的 GOPATH环境变量的Value值。</p><pre><code class="bash">$ mkdir -pv k8sapp/src$ export GOPATH=/User/rh01/k8sapp/</code></pre><p>然后在<strong>src</strong>目录下面添加 golang 代码：</p><pre><code class="bash">$ mkdir -pv github.com/rh01/goappk8s &amp;&amp; cd github.com/rh01/goappk8s$ cat &lt;&lt; EOF &gt;&gt; main.gopackage mainimport (    &quot;github.com/gin-gonic/gin&quot;    &quot;net/http&quot;)func main() {    router := gin.Default()    router.GET(&quot;/ping&quot;, func(c *gin.Context) {        c.String(http.StatusOK, &quot;PONG&quot;)    })    router.Run(&quot;:8080&quot;)}EOF</code></pre><blockquote><p>注：上面的代码取自 <a href="https://github.com/cnych/goappk8s" target="_blank" rel="noopener">https://github.com/cnych/goappk8s</a></p><p>另外上面的代码主要是创建了一个微服务，主要实现PING的功能</p></blockquote><p>可以看到上面的代码中，我们导入了第三方包 github.com/gin-gonic/gin，这时可以手动将该依赖包下载下来放置到<strong>GOPATH</strong>下面，这里使用了 <strong>govendor</strong> 来进行管理，当然你可以使用其他的包管理工具，比如：dep、glid或者利用go mod 等等。</p><p>在 <strong>github.com/rh01/goappk8s</strong> 目录下面执行下面的操作：</p><blockquote class="colorquote info"><p><strong>如何安装 govendor：</strong></p><p>在Mac中可以使用brew工具或者使用下面的命令</p><pre><code class="bash">$ go get -u github.com/kardianos/govendor</code></pre></blockquote><p>下面我们使用govendor来将依赖包缓存到本地项目中，方便移植和项目管理。</p><pre><code class="bash"># 初始化本地项目文件夹为使用vendor管理包$ govendor init# 将依赖包缓存到本地$ govendor fetch github.com/gin-gonic/gin</code></pre><blockquote><p>上面 fetch 需要设置代理才能通过，还是老办法：</p><pre><code class="bash">$ export http_proxy=&quot;http://127.0.0.1:12333&quot;$ export https_proxy=&quot;http://127.0.0.1:12333&quot;</code></pre></blockquote><p>这样一个非常小的微服务应用就做完了，这时需要测试一下，看看是否正常运行。这时切换到 <strong>GOPARH</strong> 文件夹。执行下面的语句</p><pre><code class="bash">$ go install github.com/rh01/goappk8s &amp;&amp; ./bin/goappk8s[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.[GIN-debug] [WARNING] Running in &quot;debug&quot; mode. Switch to &quot;release&quot; mode in production. - using env:   export GIN_MODE=release - using code:  gin.SetMode(gin.ReleaseMode)[GIN-debug] GET    /ping                     --&gt; main.main.func1 (3 handlers)[GIN-debug] Listening and serving HTTP on :8080</code></pre><p>这时会打印出如上面所示的日志信息，这时我们可以通过 curl 来访问。</p><pre><code class="bash">$ curl localhost:8080/pingPONG</code></pre><p>这时我们的服务是已经正常可以运行的了。</p><h2 id="打包成Docker镜像（使用ARM）"><a href="#打包成Docker镜像（使用ARM）" class="headerlink" title="打包成Docker镜像（使用ARM）"></a>打包成Docker镜像（使用ARM）</h2><p>这里主要使用了 Docker 的多阶段构建来打造一个非常小的镜像，这对我们的边缘端来讲，是非常必要的，因为他们的资源是非常有限的。</p><blockquote><p>有关 Docker镜像的多阶段构建以及Docker镜像优化问题，请详见我之前的一篇文章：</p><ul><li><a href="https://www.41sh.cn/?id=25" target="_blank" rel="noopener">https://www.41sh.cn/?id=25</a></li></ul></blockquote><p>下面我们看一下Dockerfile文件吧。</p><pre><code class="dockerfile">FROM golang AS build-envADD . /go/src/appWORKDIR /go/src/appENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-serverFROM armhf/alpine:latestRUN apk add -U tzdataRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai  /etc/localtimeCOPY --from=build-env /go/src/app/app-server /usr/local/bin/app-serverEXPOSE 8080CMD [ &quot;app-server&quot; ]</code></pre><blockquote class="colorquote info"><p>这里也参考了知乎的这片文章：<a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></p><p>主要修改如下：</p><p>(1).  增加了</p><pre><code class="dockerfile"><span class="keyword">ENV</span> http_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span><span class="keyword">ENV</span> https_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span></code></pre><p>这是因为国内的网络环境，我们需要添加代理，才能拉取相关的依赖包</p><p>(2).  修改了</p><pre><code class="dockerfile"><span class="keyword">RUN</span><span class="bash"> GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-server</span></code></pre><p> 这是因为我们需要在树莓派 arm架构下去编译和运行golang程序，因此需要交叉编译。</p><p>(3). 修改了</p><pre><code class="dockerfile"><span class="keyword">FROM</span> armhf/alpine:latest</code></pre><p>这个也是因为硬件架构的不同进行相应的修改</p></blockquote><p>然后构建<code>Docker</code>镜像：</p><pre><code class="bash">$ docker build -t rh02/goappk8s:v1.0.0 ........(省略了)Successfully built 00751f94d8a9Successfully tagged cnych/goappk8s:v1.0.0$ docker push rh02/goappk8s:v1.0.0</code></pre><p>上面的操作可以将我们本地的镜像<code>rh02/goappk8s:v1.0.0</code>推送到公共的<code>dockerhub</code>上面去（前提是你得先注册了dockerhub）。</p><h2 id="将服务部署在kubernetes"><a href="#将服务部署在kubernetes" class="headerlink" title="将服务部署在kubernetes"></a>将服务部署在kubernetes</h2><p>如果要将微服务部署在kubernetes上，只需要写一个yaml文件，定义好你需要的资源对象即可，下面先给出我们的部署的yaml文件内容。</p><pre><code class="yaml">---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: goapp-deploy  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  replicas: 2  revisionHistoryLimit: 10  minReadySeconds: 5  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 1  template:    metadata:      labels:        k8s-app: goappk8s    spec:      containers:      - image: rh02/goappk8s:v1.1.0        imagePullPolicy: Always        name: goappk8s        ports:        - containerPort: 8080          protocol: TCP        resources:          limits:            cpu: 100m            memory: 100Mi          requests:            cpu: 50m            memory: 50Mi        livenessProbe:          tcpSocket:            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 3        readinessProbe:          httpGet:            path: /ping            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 2---apiVersion: v1kind: Servicemetadata:  name: goapp-svc  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  ports:    - name: api      port: 8080      protocol: TCP      targetPort: 8080  selector:    k8s-app: goappk8s---kind: IngressapiVersion: extensions/v1beta1metadata:  name: goapp-ingressspec:  rules:  - host: k8sapp1.41sh.cn    http:      paths:      - path: /        backend:          serviceName: goapp-svc          servicePort: api</code></pre><blockquote><p>这里的k8sapp1.41sh.cn 需要解析为ingress的node。详细可以参考 <a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></p></blockquote><p>因为我们编写了一个无状态的应用，因此上面主要创建了三个资源对象，分别为deployment（部署），service（服务）和ingress（主要负责负载均衡和提供一种访问的方式）对象。</p><p>使用kubectl来创建这三个资源对象。</p><pre><code class="bash">$ kubectl apply -f deployment.yamldeployment &quot;goapp-deploy&quot; createdservice &quot;goapp-svc&quot; createdingress &quot;goapp-ingress&quot; created</code></pre><p>这时我们需要创建一个traefik的ingress应用，用来处理ingress的请求。</p><pre><code class="bash">$ kubectl label node edge-node2 ingress-controller=traefik $ kubectl apply -f traefik.yaml</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>这时，我们都已经做完了，这时我们可以打开我们的dashboard看看怎么样。并且可以通过在浏览器中访问 <a href="http://k8sapp1.41sh.cn/ping" target="_blank" rel="noopener">http://k8sapp1.41sh.cn/ping</a> 来访问我们的微服务。</p><p>下面是成果截图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423865486307.png" alt="屏幕快照 2019-03-01 下午3.03.48.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423899753911.png" alt="屏幕快照 2019-03-01 下午3.04.39.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。&lt;/p&gt;
&lt;p&gt;本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://readailib.com/categories/Kubernetes/"/>
    
      <category term="Microservice" scheme="https://readailib.com/categories/Kubernetes/Microservice/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="Edge Computing" scheme="https://readailib.com/tags/Edge-Computing/"/>
    
      <category term="Gin" scheme="https://readailib.com/tags/Gin/"/>
    
  </entry>
  
  <entry>
    <title>使用minikube快速安装istio集群</title>
    <link href="https://readailib.com/2019/02/22/kubernetes/istio-minikube/"/>
    <id>https://readailib.com/2019/02/22/kubernetes/istio-minikube/</id>
    <published>2019-02-22T05:48:07.000Z</published>
    <updated>2019-03-07T07:32:46.988Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。</p><p>本节课不教：</p><ul><li>使用minikube部署多节点的kubernetes集群，详细教程请看：<a href="https://www.41sh.cn/?id=53" target="_blank" rel="noopener">https://www.41sh.cn/?id=53</a></li></ul><p>本节课的目标是：</p><ul><li><p>使用Helm或者手动方式来构建istio集群</p></li><li><p>使用istio框架来部微服务</p></li><li><p>服务治理与金丝雀发布等等</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820119606413.png" alt="23534644.png"></p></li></ul><h2 id="启动minikube集群"><a href="#启动minikube集群" class="headerlink" title="启动minikube集群"></a>启动minikube集群</h2><p>使用下面的指令在本地启动两个节点的k8s集群，分别为master和node节点，并使node节点加入到集群中。</p><pre><code class="bash"># 启动master节点，名字为k8s-m1$ minikube --profile k8s-m1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24# 以同样的方式启动node节点，名字为k8s-n1$ minikube --profile k8s-n1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24</code></pre><p>确认两个节点已经启动，但是现在node节点并没有加入到集群中，你可能使用下面指令会看到NotReady的标示，也有可能列表中不会出现k8s-n1的条目。</p><pre><code class="bash"># 切换到master节点的配置中，这样才可以使用kubectl来查询资源信息$ kubectl config --use-context k8s-m1# 查看当前的节点列表$ kubectl get no</code></pre><p>这时候需要将node节点加入集群，使用下面指令获得TOKEN，并且使用kubeadm join指令，使得node节点加入集群。</p><pre><code class="bash"># 获取master节点的ip地址$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;# 获取当前的TOKEN列表$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 执行下面指令进入 k8s-n1$ minikube --profile k8s-n1 ssh# 下面为进入 k8s-n1 VM 內执行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下结果后，即可以在 k8s-m1 context 来操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><p>这时候我们回到我们本地的机器（非VM），通过执行下面命令确认k8s-n1已经准备完成。</p><pre><code class="bash"># 查看当前的节点列表$ kubectl get no</code></pre><h2 id="使用helm安装istio"><a href="#使用helm安装istio" class="headerlink" title="使用helm安装istio"></a>使用helm安装istio</h2><p>为了方便起见，这里我使用helm chart来安装istio，这里参考了下面的文档：</p><ul><li><a href="https://istio.io/docs/setup/kubernetes/helm-install/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/helm-install/</a></li><li><a href="https://istio.io/docs/setup/kubernetes/download-release/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/download-release/</a></li></ul><p><em>1). 使用下面的命令安装helm</em>（mac系统）</p><pre><code class="bash">$ brew install kubernetes-helm</code></pre><blockquote><p>其他系统的安装方式请参考：<br><a href="https://helm.sh/docs/using_helm/#installing-helm" target="_blank" rel="noopener">https://helm.sh/docs/using_helm/#installing-helm</a></p></blockquote><p>这里是自动安装的，这里你安装的只是一个helm client，你需要又一个helm后端来支持helm自动化部署的功能，你也可以通过下面的命令查看当前的helm的安装情况以及版本。</p><pre><code class="bash">$ helm version</code></pre><blockquote><p>如果出现Server端没有起来的情况，使用helm init –service-account tiller 就自动将Tiller端安装到kubernetes集群中  。</p></blockquote><p><em>2). 下载istio并准备安装</em></p><p>使用下面的指令自动下载istio的最新发行版本，并解压到当前的文件夹下，这里有一个istio的客户端二进制文件，这时你需要手动的将二进制文件的目录添加到PATH环境变量中。</p><pre><code class="bash">$ curl -L https://git.io/getLatestIstio | sh -$ export PATH=&quot;$PATH:/Users/rh01/istio-1.0.6/bin&quot; #这是会话环境变量的设置，临时使用，如果想永久生效，请添加到 ~/.bashrc 或者 /etc/profile中</code></pre><p><em>3). 安装istioz</em></p><p>切换到istio的文件夹下，并使用helm 安装 install 文件夹下面的yaml 来创建istio。下面的命令是我在本地测试过的：</p><pre><code class="bash">$ cd istio-1.0.6$ helm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; $HOME/istio-1.0.6/istio.yaml$ kubectl create namespace istio-system                              # 创建istio-system命名空间，之后管理istio的所有资源$ kubectl apply -f install/kubernetes/helm/helm-service-account.yaml # 创建helm服务账号，使得helm能够有权限对istio-system命名空间进行操作$ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml # 创建crd$ kubectl apply -f $HOME/istio-1.0.6/istio.yaml                      # 创建istio所有资源对象$ kubectl get po -n istio-systemNAME                                      READY     STATUS      RESTARTS   AGEistio-citadel-6f444d9999-7l2hx            1/1       Running     0          1mistio-cleanup-secrets-qjk7c               0/1       Completed   0          1mistio-egressgateway-6d79447874-v6xds      1/1       Running     0          1mistio-galley-685bb48846-pxcs2             1/1       Running     0          1mistio-ingressgateway-5b64fffc9f-4jnr9     1/1       Running     0          1mistio-pilot-8645f5655b-s6nbq              0/2       Pending     0          1mistio-policy-547d64b8d7-vtxqh             2/2       Running     0          1mistio-security-post-install-r27sv         0/1       Completed   0          1mistio-sidecar-injector-5d8dd9448d-bwcvg   1/1       Running     0          1mistio-telemetry-c5488fc49-qr7sg           2/2       Running     0          1mprometheus-76b7745b64-pm68q               1/1       Running     0          1m</code></pre><blockquote class="colorquote warning"><p><strong>遇到的坑：</strong>等等我遇到了一个istio-pilot内存不足的情况，因为我是两个节点，但是因为是在本地创建的两个虚拟机，所有内存和CPU资源都比较小，因此当出现资源不足的时候，就Pending了,这时候需要手动修改一下请求的内存资源的大小就可以了。</p><p><em>使用到的命令</em>：</p><pre><code class="bash">$ kubectl edit istio-pilot-xxx -n istio-system<span class="comment"># 修改resource的request的memory为100Mi即可</span></code></pre></blockquote><h2 id="部署一个应用"><a href="#部署一个应用" class="headerlink" title="部署一个应用"></a>部署一个应用</h2><p>这个是官方给出的一个例子 bookinfo，这个应用由四个微服务组成，下面是应用的架构图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903061551882998396723.png" alt="20190222153800_46088.png"></p><p>下面的所有命令以及说明部分来自下面的文档：</p><ul><li><a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a></li><li><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></li></ul><p>下面的指令将会在kubernetes上部署一个bookinfo应用，并由istio提供微服务的一些服务调用和路由等等功能，具体的istio特性，后期见。</p><pre><code class="bash">$ kubectl label namespace default istio-injection=enablednamespace &quot;default&quot; labeled$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlservice &quot;details&quot; createddeployment.extensions &quot;details-v1&quot; createdservice &quot;ratings&quot; createddeployment.extensions &quot;ratings-v1&quot; createdservice &quot;reviews&quot; createddeployment.extensions &quot;reviews-v1&quot; createddeployment.extensions &quot;reviews-v2&quot; createddeployment.extensions &quot;reviews-v3&quot; createdservice &quot;productpage&quot; createddeployment.extensions &quot;productpage-v1&quot; created$ kubectl get servicesNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGEdetails       ClusterIP   10.97.137.200    &lt;none&gt;        9080/TCP   7skubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    56mproductpage   ClusterIP   10.106.43.117    &lt;none&gt;        9080/TCP   6sratings       ClusterIP   10.108.175.114   &lt;none&gt;        9080/TCP   7sreviews       ClusterIP   10.96.73.150     &lt;none&gt;        9080/TCP   6s$ kubectl get pods$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yamlgateway.networking.istio.io &quot;bookinfo-gateway&quot; createdvirtualservice.networking.istio.io &quot;bookinfo&quot; created$  kubectl get gatewayNAME               AGEbookinfo-gateway   9s$ kubectl get svc istio-ingressgateway -n istio-systemNAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                   AGEistio-ingressgateway   LoadBalancer   10.101.64.224   &lt;pending&gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31643/TCP,8060:30348/TCP,853:30934/TCP,15030:32334/TCP,15031:32681/TCP   17m</code></pre><p>这时候大家会看到istio-ingressgateway服务的EXTERNAL_IP是pending状态，这是因为我们没有指定外置的负载均衡器的ip地址，这里有三种处理方式：</p><ul><li>如果处于云服务厂商的环境，并且有负载均衡器，这时候就填写负载均衡器的ip地址</li><li>如果没有，可以采取将服务的类型改为NodePort类型，或者直接使用提供的NodePort，这样就可以使用http://&lt;Ingress所在Node的IP&gt;:NodePort/productpage的方式访问</li><li>另外你也可以将ingress对象处于的Node的ip作为ExternalIP，也是可以访问的。这样就可以直接使用<a href="http://ExternalIP:80/productpage进行访问。" target="_blank" rel="noopener">http://ExternalIP:80/productpage进行访问。</a></li></ul><blockquote><p><em>具体的如何获取ingress和配置ingress，详见下面的文档</em>：<br><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></p></blockquote><h2 id="大功告成"><a href="#大功告成" class="headerlink" title="大功告成"></a>大功告成</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820787188445.png" alt="屏幕快照 2019-02-22 下午3.15.31.png"></p><p>如果继续刷新，会发现有三个版本的应用出现，就是红色的星星，黑色的星星，没有星星这三个版本，会随机出现，我们可以使用istio来管理这些不同版本的应用，通过金丝雀发布，灰度发布等一些高级特性实现切流量的功能，后面的文章我会详细介绍有关istio的高级特性。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821049797553.png" alt="屏幕快照 2019-02-22 下午3.37.12.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821077460003.png" alt="屏幕快照 2019-02-22 下午3.37.39.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。&lt;/p&gt;
&lt;p&gt;本节课不教：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用minikube部署多节点的kubernetes集群，详细教程请看：&lt;a href=&quot;https://www.41sh.c
      
    
    </summary>
    
      <category term="istio" scheme="https://readailib.com/categories/istio/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="istio" scheme="https://readailib.com/tags/istio/"/>
    
  </entry>
  
</feed>
