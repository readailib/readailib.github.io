<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShenH.&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/543fc8a83d9b480e5f69c3842db96518</icon>
  <subtitle>Learn Anything, Anytime, Anywhere~</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://readailib.com/"/>
  <updated>2019-03-07T07:13:29.046Z</updated>
  <id>https://readailib.com/</id>
  
  <author>
    <name>ShenHengheng</name>
    <email>shenhengheng17g@ict.ac.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在树莓派上编译安装Go版本的Tensorflow</title>
    <link href="https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/"/>
    <id>https://readailib.com/2019/03/07/golang/tensorflow/tensorflow-for-golang-on-raspberrypi/</id>
    <published>2019-03-06T16:14:16.000Z</published>
    <updated>2019-03-07T07:13:29.046Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-Used-Hardwares-and-Softwares"><a href="#0-Used-Hardwares-and-Softwares" class="headerlink" title="0. Used Hardwares and Softwares"></a>0. Used Hardwares and Softwares</h1><p>All steps were taken on my <strong>Raspberry Pi 3 B</strong> model with:</p><ul><li>Minimum GPU memory allocated (16MB)</li><li><strong>1GB</strong> of swap memory</li><li>External USB HDD (as root partition)</li></ul><a id="more"></a><p>and software versions were:</p><ul><li>Raspbian (Stretch) / gcc 6.3.0</li><li>Tensorflow 1.3.0</li><li>Protobuf 3.1.0</li><li>Bazel 0.5.1</li></ul><p>Before the beginning, I had to install dependencies:</p><h3 id="for-protobuf"><a href="#for-protobuf" class="headerlink" title="for protobuf"></a>for protobuf</h3><pre><code class="bash">$ sudo apt-get install autoconf automake libtool</code></pre><h3 id="for-bazel"><a href="#for-bazel" class="headerlink" title="for bazel"></a>for bazel</h3><pre><code class="bash">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip oracle-java8-jdk</code></pre><hr><h1 id="1-Install-Protobuf"><a href="#1-Install-Protobuf" class="headerlink" title="1. Install Protobuf"></a>1. Install Protobuf</h1><p>I cloned the protobuf’s repository:</p><pre><code class="bash">$ git clone https://github.com/google/protobuf.git</code></pre><p>and started building:</p><pre><code class="bash">$ cd protobuf$ git checkout v3.1.0$ ./autogen.sh$ ./configure$ make -j 4$ sudo make install$ sudo ldconfig</code></pre><p>It took less than an hour to finish.</p><p>I could see the version of installed protobuf with:</p><pre><code class="bash">$ protoc --versionlibprotoc 3.1.0</code></pre><h1 id="2-Install-Bazel"><a href="#2-Install-Bazel" class="headerlink" title="2. Install Bazel"></a>2. Install Bazel</h1><h2 id="a-download"><a href="#a-download" class="headerlink" title="a. download"></a>a. download</h2><p>I got a zip file of bazel from <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">here</a> and unzipped it:</p><pre><code class="bash">$ wget https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip$ unzip -d bazel bazel-0.5.1-dist.zip</code></pre><h2 id="b-edit-bootstrap-files"><a href="#b-edit-bootstrap-files" class="headerlink" title="b. edit bootstrap files"></a>b. edit bootstrap files</h2><p>In the unzipped directory, I opened the <code>scripts/bootstrap/compile.sh</code> file:</p><pre><code class="bash">$ cd bazel$ vi scripts/bootstrap/compile.sh</code></pre><p>searched for lines that looked like following:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot;</code></pre><p>and appended <code>-J-Xmx500M</code> to the last line so that the whole lines would look like:</p><pre><code class="bash">run &quot;${JAVAC}&quot; -classpath &quot;${classpath}&quot; -sourcepath &quot;${sourcepath}&quot; \      -d &quot;${output}/classes&quot; -source &quot;$JAVA_VERSION&quot; -target &quot;$JAVA_VERSION&quot; \      -encoding UTF-8 &quot;@${paramfile}&quot; -J-Xmx500M</code></pre><p>It was for enlarging the max heap size of Java.</p><h2 id="c-compile"><a href="#c-compile" class="headerlink" title="c. compile"></a>c. compile</h2><p>After that, started building with:</p><pre><code class="bash">$ chmod u+w ./* -R$ ./compile.sh</code></pre><p>It also took about an hour.</p><h2 id="d-install"><a href="#d-install" class="headerlink" title="d. install"></a>d. install</h2><p>After the compilation had finished, I could find the compiled binary in <code>output</code> directory.</p><p>Copied it into <code>/usr/local/bin</code> directory:</p><pre><code class="bash">$ sudo cp output/bazel /usr/local/bin/</code></pre><h1 id="3-Build-libtensorflow-so"><a href="#3-Build-libtensorflow-so" class="headerlink" title="3. Build libtensorflow.so"></a>3. Build libtensorflow.so</h1><p>(I referenced <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md" target="_blank" rel="noopener">this document</a> for following processes)</p><h2 id="a-download-1"><a href="#a-download-1" class="headerlink" title="a. download"></a>a. download</h2><p>Got the tensorflow go code with:</p><pre><code class="bash">$ go get -d github.com/tensorflow/tensorflow/tensorflow/go</code></pre><h2 id="b-edit-files"><a href="#b-edit-files" class="headerlink" title="b. edit files"></a>b. edit files</h2><p>In the downloaded directory, I checked out the latest tag and replaced <code>lib64</code> to <code>lib</code> in the files with:</p><pre><code class="bash">$ cd ${GOPATH}/src/github.com/tensorflow/tensorflow$ git fetch --all --tags --prune$ git checkout tags/v1.3.0$ grep -Rl &#39;lib64&#39; | xargs sed -i &#39;s/lib64/lib/g&#39;</code></pre><p>Raspberry Pi still runs on 32bit OS, so they had to be changed like this.</p><p>After that, I commented <code>#define IS_MOBILE_PLATFORM</code> out in <code>tensorflow/core/platform/platform.h</code>:</p><pre><code class="c">// Since there&#39;s no macro for the Raspberry Pi, assume we&#39;re on a mobile// platform if we&#39;re compiling for the ARM CPU.//#define IS_MOBILE_PLATFORM    // &lt;= commented this line</code></pre><p>If it is not commented out, bazel will build for mobile platforms like <code>iOS</code> or <code>Android</code>, not Raspberry Pi.</p><p>To do this easily, just run:</p><pre><code class="bash">$ sed -i &quot;s|#define IS_MOBILE_PLATFORM|//#define IS_MOBILE_PLATFORM|g&quot; tensorflow/core/platform/platform.h</code></pre><p>Finally, it was time to configure and build tensorflow.</p><h2 id="c-configure-and-build"><a href="#c-configure-and-build" class="headerlink" title="c. configure and build"></a>c. configure and build</h2><pre><code class="bash">$ ./configure</code></pre><p>I had to answer to some questions here.</p><p>Then I started building <code>libtensorflow.so</code> with:</p><pre><code class="bash">$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so</code></pre><p>My Pi became unresponsive many times during this process, but I kept it going on.</p><h2 id="d-install-1"><a href="#d-install-1" class="headerlink" title="d. install"></a>d. install</h2><p>After a long time of struggle, (it took nearly 7 hours for me!)</p><p>I finally got <code>libtensorflow.so</code> compiled in <code>bazel-bin/tensorflow/</code>.</p><p>So I copied it into <code>/usr/local/lib/</code>:</p><pre><code class="bash">$ sudo cp ./bazel-bin/tensorflow/libtensorflow.so /usr/local/lib/$ sudo chmod 644 /usr/local/lib/libtensorflow.so$ sudo ldconfig</code></pre><p>All done. Time to test!</p><h1 id="4-Go-Test"><a href="#4-Go-Test" class="headerlink" title="4. Go Test"></a>4. Go Test</h1><p>I ran a test for validating the installation:</p><pre><code class="bash">$ go test github.com/tensorflow/tensorflow/tensorflow/go</code></pre><p>then I could see:</p><pre><code class="bash">ok      github.com/tensorflow/tensorflow/tensorflow/go  0.350s</code></pre><p>Ok, it works!</p><p><strong>Edit</strong>: As <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go#generate-wrapper-functions-for-ops" target="_blank" rel="noopener">this instruction</a> says, I had to regenerate operations before the test:</p><pre><code class="bash">$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op</code></pre><h1 id="5-Further-Test"><a href="#5-Further-Test" class="headerlink" title="5. Further Test"></a>5. Further Test</h1><p>I wanted to see a simple go program running, so I wrote this code:</p><pre><code class="go">// sample.gopackage mainimport (    &quot;fmt&quot;    tf &quot;github.com/tensorflow/tensorflow/tensorflow/go&quot;)// Sorry - I don&#39;t have a good example yet :-Pfunc main() {    tensor, _ := tf.NewTensor(int64(42))    if v, ok := tensor.Value().(int64); ok {        fmt.Printf(&quot;The answer to the life, universe, and everything: %v\n&quot;, v)    }}</code></pre><p>and ran it with <code>go run sample.go</code>:</p><pre><code>The answer to the life, universe, and everything: 42</code></pre><p>See the result?</p><p>From now on, I can write tensorflow applications in go, on Raspberry Pi! :-)</p><hr><h1 id="98-Trouble-shooting"><a href="#98-Trouble-shooting" class="headerlink" title="98. Trouble shooting"></a>98. Trouble shooting</h1><h2 id="Build-failure-due-to-a-problem-with-Eigen"><a href="#Build-failure-due-to-a-problem-with-Eigen" class="headerlink" title="Build failure due to a problem with Eigen"></a>Build failure due to a problem with Eigen</h2><p>Back in the day with <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.2.0" target="_blank" rel="noopener">Tensorflow 1.2.0</a>, I encountered <a href="https://github.com/tensorflow/tensorflow/issues/9697" target="_blank" rel="noopener">this issue</a> while building, but it’s still not fixed yet in <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0" target="_blank" rel="noopener">1.3.0</a>.</p><p>So I had to work around this problem again by editing <code>tensorflow/workspace.bzl</code>from:</p><pre><code class="bzl">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz&quot;,    ],    sha256 = &quot;ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4&quot;,    strip_prefix = &quot;eigen-eigen-f3a22f35b044&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>to:</p><pre><code class="bash">native.new_http_archive(    name = &quot;eigen_archive&quot;,    urls = [        &quot;http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,        &quot;https://bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz&quot;,    ],    sha256 = &quot;a34b208da6ec18fa8da963369e166e4a368612c14d956dd2f9d7072904675d9b&quot;,    strip_prefix = &quot;eigen-eigen-d781c1de9834&quot;,    build_file = str(Label(&quot;//third_party:eigen.BUILD&quot;)),)</code></pre><p>and starting again from the beginning:</p><pre><code class="bash">$ bazel clean$ ./configure$ bazel build -c opt --copt=&quot;-mfpu=neon-vfpv4&quot; --copt=&quot;-funsafe-math-optimizations&quot; --copt=&quot;-ftree-vectorize&quot; --copt=&quot;-fomit-frame-pointer&quot; --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow:libtensorflow.so...</code></pre><p>Then I could build it without further problems.</p><p>I hope it would be fixed on future releases.</p><hr><h1 id="99-Wrap-up"><a href="#99-Wrap-up" class="headerlink" title="99. Wrap-up"></a>99. Wrap-up</h1><p>Installing TensorFlow on Raspberry Pi is not easy yet. (There’s <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi" target="_blank" rel="noopener">a kind project</a> which makes it super easy though!)</p><p>Installing <code>libtensorflow.so</code> is a lot more difficult, because it takes too much time to build it.</p><p>But it is worth trying; managing TensorFlow graphs in golang will be handy for people who don’t love python - just like me.</p><hr><h1 id="999-If-you-need-one"><a href="#999-If-you-need-one" class="headerlink" title="999. If you need one,"></a>999. If you need one,</h1><p>You don’t have time to build it yourself, but still need the compiled file?</p><p>Good, take it <a href="https://github.com/meinside/libtensorflow.so-raspberrypi/releases" target="_blank" rel="noopener">here</a>.</p><p>I cannot promise, but will try keeping it up-to-date whenever a newer version of tensorflow comes out.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-Used-Hardwares-and-Softwares&quot;&gt;&lt;a href=&quot;#0-Used-Hardwares-and-Softwares&quot; class=&quot;headerlink&quot; title=&quot;0. Used Hardwares and Softwares&quot;&gt;&lt;/a&gt;0. Used Hardwares and Softwares&lt;/h1&gt;&lt;p&gt;All steps were taken on my &lt;strong&gt;Raspberry Pi 3 B&lt;/strong&gt; model with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimum GPU memory allocated (16MB)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1GB&lt;/strong&gt; of swap memory&lt;/li&gt;
&lt;li&gt;External USB HDD (as root partition)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="Tensorflow" scheme="https://readailib.com/tags/Tensorflow/"/>
    
      <category term="ARM" scheme="https://readailib.com/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>在树莓派kubernetes集群部署gRPC框架编写的微服务</title>
    <link href="https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/"/>
    <id>https://readailib.com/2019/03/05/kubernetes/raspberrypi/deploy-a-microservice-on-k8s/</id>
    <published>2019-03-04T16:14:16.000Z</published>
    <updated>2019-03-06T00:50:11.802Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.</p><a id="more"></a><p>本次教程需要准备:</p><ul><li>一个启动好的树莓派kubernetes集群(参考: <a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a>)</li><li>protobuf工具(参考 <a href="https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可" target="_blank" rel="noopener">https://github.com/google/protobuf,从release页下载对应的操作系统的版本即可</a>)</li><li>Docker (参考: <a href="https://docs.docker.com/engine/installation/" target="_blank" rel="noopener">https://docs.docker.com/engine/installation/</a>)</li><li>kubectl工具 (参考: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>)</li></ul><p>本文章参考: </p><ul><li><a href="https://outcrawl.com/getting-started-microservices-go-grpc-kubernetes" target="_blank" rel="noopener">Getting Started with Microservices using Go, gRPC and Kubernetes</a></li></ul><p>本文章的实例代码:</p><ul><li><a href="https://github.com/rh01/grpc-microservice-k8s" target="_blank" rel="noopener">https://github.com/rh01/grpc-microservice-k8s</a></li></ul><h2 id="定义我们的protobuf文件"><a href="#定义我们的protobuf文件" class="headerlink" title="定义我们的protobuf文件"></a>定义我们的protobuf文件</h2><p>protobuf是google的一个序列化结构化数据工具,它可以让人们定义好相关的结构,使用protoc工具自动生成对应的代码.类似的结构化工具还有thrift.</p><blockquote><p>微服务:这里我主要实现一个最大公约数的功能,输入两个数值,返回这两个数的最大公约数.</p></blockquote><p>这里既然使用gRPC来做,那么主要使用rpc来实现服务调用,因为rpc实现的是服务之间的同步调用，即客户端调用服务并等待响应。gRPC是提供RPC功能的框架之一。此时我们需要使用Protocol Buffer的接口定义语言中编写消息类型和服务的代码并进行编译。</p><p>下面就是我们使用protobuf语言定义的消息类型和服务.(具体参考: <a href="https://grpc.io/docs/quickstart/go.html)" target="_blank" rel="noopener">https://grpc.io/docs/quickstart/go.html)</a></p><pre><code class="bash">$ mkdir -pv ~/go/src/github.com/rh01/mini-deploy-app/pb$ cd ~/go/src/github.com/rh01/mini-deploy-app/pb$ vim pb.proto</code></pre><pre><code class="yaml">syntax = &quot;proto3&quot;; // protobuf 版本package pb;        // 代码生成的package名字// 定义的请求消息体message GCDRequest {    uint64 a = 1;    uint64 b = 2;}// 定义的响应消息体message GCDResponse {    uint64 result = 1;}// 调用的远程服务,这是client请求server端的远程计算服务service GCDService {    rpc Compute (GCDRequest) returns (GCDResponse) {}}</code></pre><p>接下来我们需要使用 protoc 生成对应的服务代码</p><pre><code>$ protoc -I . --go_out=plugins=grpc:. ./*.proto</code></pre><blockquote class="colorquote warning"><p><strong>提前须知</strong>:</p><p>1). 执行上面的指令需要使用安装 grpc 和 proto-gen-go 工具,使用下面的命令:</p><pre><code class="bash">$ go get -u google.golang.org/grpc$ go get -u github.com/golang/protobuf/protoc-gen-go</code></pre><p>2). 将 $GOPATH/bin 目录添加到PATH环境变量中</p><pre><code class="bash">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$GOPATH</span>/bin</code></pre></blockquote><p>这时应该生成了 gcd.pb.go 程序.</p><h2 id="最大公约数服务"><a href="#最大公约数服务" class="headerlink" title="最大公约数服务"></a>最大公约数服务</h2><h3 id="定义服务端"><a href="#定义服务端" class="headerlink" title="定义服务端"></a>定义服务端</h3><p>gcd 服务将会使用上一步生成的代码进行实现gcd计算服务.</p><pre><code class="bash">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p gcd$ vim main.go</code></pre><pre><code class="go">package mainimport (    &quot;log&quot;    &quot;net&quot;    context &quot;golang.org/x/net/context&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)</code></pre><p>在main函数中主要定义server结构,并将其注册为server端,用于处理 gcd 计算的请求.然后启动grpc服务.</p><pre><code class="go">type server struct {}func main() {    lis, err := net.Listen(&quot;tcp&quot;, &quot;:3000&quot;)    if err != nil {        log.Fatalf(&quot;Failed to listen: %v&quot;, err)    }    s := grpc.NewServer()    pb.RegisterGCDServiceServer(s, &amp;server{})    reflection.Register(s)    if err := s.Serve(lis); err != nil {        log.Fatalf(&quot;Failed to serve: %v&quot;, err)    }}</code></pre><p>实现GCDServiceServer接口的 <code>Compute</code> 方法, server 结构对象的指针作为方法接受者.</p><pre><code class="Go">// gcd.pb.go// GCDServiceServer is the server API for GCDService service.type GCDServiceServer interface {    Compute(context.Context, *GCDRequest) (*GCDResponse, error)}func (s *server) Compute(ctx context.Context, r *pb.GCDRequest) (*pb.GCDResponse, error) {    a, b := r.A, r.B    for b != 0 {        a, b = b, a%b    }    return &amp;pb.GCDResponse{Result: a}, nil}</code></pre><h2 id="定义RESTFul客户端"><a href="#定义RESTFul客户端" class="headerlink" title="定义RESTFul客户端"></a>定义RESTFul客户端</h2><p>前端使用 <a href="https://github.com/gin-gonic/gin" target="_blank" rel="noopener">gin</a> 框架,主要是提供一个REST风格的访问方式和调用我们定义的gcd服务端执行实际的计算任务.</p><pre><code class="go">$ cd ~/go/src/github.com/rh01/mini-deploy-app/$ mkdir -p api$ vim main.go</code></pre><pre><code>package mainimport (    fmt &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;strconv&quot;    &quot;github.com/gin-gonic/gin&quot;    pb &quot;github.com/rh01/mini-deploy-app/pb&quot;    &quot;google.golang.org/grpc&quot;)func main() {    conn, err := grpc.Dial(&quot;gcd-service:3000&quot;, grpc.WithInsecure())    if err != nil {        log.Fatalf(&quot;Dial failed: %v&quot;, err)    }    gcdClient := pb.NewGCDServiceClient(conn)}</code></pre><p>上面的代码主要使用rpc的方式访问我们定义的服务端,此时的 gcd-service:3000 就是我们gcd服务端的 endpoint,这个就是服务地址,需要在kubernetes中定义.</p><pre><code class="go">    r := gin.Default()    r.GET(&quot;/gcd/:a/:b&quot;, func(c *gin.Context) {        // Parse parameters        a, err := strconv.ParseUint(c.Param(&quot;a&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter A&quot;})            return        }        b, err := strconv.ParseUint(c.Param(&quot;b&quot;), 10, 64)        if err != nil {            c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;Invalid parameter B&quot;})            return        }        // Call GCD service        req := &amp;pb.GCDRequest{A: a, B: b}        if res, err := gcdClient.Compute(c, req); err == nil {            c.JSON(http.StatusOK, gin.H{                &quot;result&quot;: fmt.Sprint(res.Result),            })        } else {            c.JSON(http.StatusInternalServerError, gin.H{&quot;error&quot;: err.Error()})        }    })</code></pre><p>接下来处理 /gcd/:a/:b 请求,读取参数 A 和 B,然后调用GCD服务.</p><p>最后运行我们的REST API端.启动一个API server.</p><pre><code class="go">    // Run HTTP server    if err := r.Run(&quot;:3000&quot;); err != nil {        log.Fatalf(&quot;Failed to run server: %v&quot;, err)    }</code></pre><h2 id="构建Docker镜像"><a href="#构建Docker镜像" class="headerlink" title="构建Docker镜像"></a>构建Docker镜像</h2><p>下面是我定义的Dockerfile文件,因为有两个服务,所以这里分成两个Docker镜像,一个是gcd服务,另外一个是提供RESTAPI访问的客户端api.</p><blockquote><p>有关Docker的多阶段构建参考: <a href="https://www.41sh.cn/?id=61" target="_blank" rel="noopener">[实战] 将golang编写的微服务部署在树莓派搭建的kubernetes集群</a></p></blockquote><pre><code class="dockerfile"># Dockerfile.gcdFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/gcdCOPY gcd .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/gcd-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/gcd-server /usr/local/bin/gcdEXPOSE 3000CMD [ &quot;gcd&quot; ]</code></pre><pre><code class="dockerfile"># Dockerfile.apiFROM golang AS build-envWORKDIR /go/src/github.com/rh01/mini-deploy-app/apiCOPY api .COPY pb ../pbCOPY vendor ../vendorENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/github.com/rh01/mini-deploy-app/api-serverFROM armhf/alpine:latestCOPY --from=build-env /go/src/github.com/rh01/mini-deploy-app/api-server /usr/local/bin/apiEXPOSE 3000CMD [ &quot;api&quot; ]</code></pre><p>然后构建</p><pre><code class="bash">$ docker build -t rh02/apiserver:v1.0.0 -f Dockerfile.api . $ docker build -t rh02/gcdserver:v1.0.0 -f Dockerfile.gcd .</code></pre><h2 id="部署到kubernetes"><a href="#部署到kubernetes" class="headerlink" title="部署到kubernetes"></a>部署到kubernetes</h2><p>定义两个Deployment和对应的两个Service,并且将gcd服务的名字写成我们api服务调用的名字:gcd-service.</p><p>下面是gcd的deployment和service的定义: gcd.yaml</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: gcd-deployment  labels:    app: gcdspec:  selector:    matchLabels:      app: gcd  replicas: 3  template:    metadata:      labels:        app: gcd    spec:      containers:      - name: gcd        image: rh02/gcdserver:v1.0.0        imagePullPolicy: Always      ports:      - name: gcd-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: gcd-servicespec:  selector:    app: gcd  ports:  - port: 3000    targetPort: gcd-service</code></pre><p>创建api.yaml, service类型设置为NodePort,从而可以在集群外部也可以访问,对于GCD服务,类型设置为ClusterIP即可,只需要在集群内部访问就可以.</p><pre><code class="yaml">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: api-deployment  labels:    app: apispec:  selector:    matchLabels:      app: api  replicas: 1  template:    metadata:      labels:        app: api    spec:      containers:      - name: api        image: rh02/apiserver:v1.0.0        imagePullPolicy: Always      ports:      - name: api-service        containerPort: 3000---apiVersion: v1kind: Servicemetadata:  name: api-servicespec:  type: NodePort  selector:    app: api  ports:  - port: 3000    targetPort: api-service</code></pre><p>使用kubectl创建两个资源:</p><pre><code class="bash">$ kubectl create -f api.yaml$ kubectl create -f gcd.yaml</code></pre><p>检查所有的Pod是否正在运行, 可以指定 <code>-w</code> 标记,查看启动的过程.</p><pre><code class="bash">$ kubectl get pods -wNAME                             READY     STATUS    RESTARTS   AGEapi-deployment-778049682-3vd0z   1/1       Running   0          3sgcd-deployment-544390878-0zgc8   1/1       Running   0          2sgcd-deployment-544390878-p78g0   1/1       Running   0          2sgcd-deployment-544390878-r26nx   1/1       Running   0          2s</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771763579640.png" alt="截图_2019-03-05_15-42-18.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903051551771802295607.png" alt="截图_2019-03-05_15-43-02.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天带着大家如何在树莓派kubernetes集群中部署微服务,这次文章和上次文章的区别就是这个涉及两个微服务,如何使两个微服务在kubernetes中实现服务之间的调用可能是与上次的不同之处,这次也使用Google的开源框架gRPC框架来加快微服务的开发.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="kubernetes" scheme="https://readailib.com/tags/kubernetes/"/>
    
      <category term="Edge computing" scheme="https://readailib.com/tags/Edge-computing/"/>
    
      <category term="gRPC" scheme="https://readailib.com/tags/gRPC/"/>
    
      <category term="REST" scheme="https://readailib.com/tags/REST/"/>
    
  </entry>
  
  <entry>
    <title>部署微服务到树莓派的kubernetes集群</title>
    <link href="https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/"/>
    <id>https://readailib.com/2019/03/01/kubernetes/raspberrypi/simple-microservice-on-pis/</id>
    <published>2019-02-28T17:02:07.000Z</published>
    <updated>2019-03-07T08:18:18.768Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。</p><p>本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>那么下面就开始吧，本次的环境主要分为两台机器，分别我的本地计算机（这里使用的Mac），另外就是我需要部署的环境（有三个树莓派节点组成的kubernetes集群）。</p><p>这里你需要：</p><ul><li>熟悉go语言</li><li>熟悉微服务</li><li>熟悉Docker</li><li>熟悉kubernetes的部署与基本管理任务</li><li>熟悉Linux</li></ul><p>本章主要参考了下面的文章：</p><ul><li><a href="https://41sh.cn/?id=16" target="_blank" rel="noopener">边缘智能-在树莓派上部署kubernetes集群</a></li><li><a href="https://github.com/rh01/deploy-golang-applicaton-on-kubernetes" target="_blank" rel="noopener">https://github.com/rh01/deploy-golang-applicaton-on-kubernetes</a></li><li><a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></li><li><a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></li></ul><h2 id="编写golang的应用"><a href="#编写golang的应用" class="headerlink" title="编写golang的应用"></a>编写golang的应用</h2><p><strong>本小节全部在我们本地的计算机中做的！</strong></p><p>在本地创建项目文件夹goappk8s，并且将该文件夹设置成临时的 GOPATH环境变量的Value值。</p><pre><code class="bash">$ mkdir -pv k8sapp/src$ export GOPATH=/User/rh01/k8sapp/</code></pre><p>然后在<strong>src</strong>目录下面添加 golang 代码：</p><pre><code class="bash">$ mkdir -pv github.com/rh01/goappk8s &amp;&amp; cd github.com/rh01/goappk8s$ cat &lt;&lt; EOF &gt;&gt; main.gopackage mainimport (    &quot;github.com/gin-gonic/gin&quot;    &quot;net/http&quot;)func main() {    router := gin.Default()    router.GET(&quot;/ping&quot;, func(c *gin.Context) {        c.String(http.StatusOK, &quot;PONG&quot;)    })    router.Run(&quot;:8080&quot;)}EOF</code></pre><blockquote><p>注：上面的代码取自 <a href="https://github.com/cnych/goappk8s" target="_blank" rel="noopener">https://github.com/cnych/goappk8s</a></p><p>另外上面的代码主要是创建了一个微服务，主要实现PING的功能</p></blockquote><p>可以看到上面的代码中，我们导入了第三方包 github.com/gin-gonic/gin，这时可以手动将该依赖包下载下来放置到<strong>GOPATH</strong>下面，这里使用了 <strong>govendor</strong> 来进行管理，当然你可以使用其他的包管理工具，比如：dep、glid或者利用go mod 等等。</p><p>在 <strong>github.com/rh01/goappk8s</strong> 目录下面执行下面的操作：</p><blockquote class="colorquote info"><p><strong>如何安装 govendor：</strong></p><p>在Mac中可以使用brew工具或者使用下面的命令</p><pre><code class="bash">$ go get -u github.com/kardianos/govendor</code></pre></blockquote><p>下面我们使用govendor来将依赖包缓存到本地项目中，方便移植和项目管理。</p><pre><code class="bash"># 初始化本地项目文件夹为使用vendor管理包$ govendor init# 将依赖包缓存到本地$ govendor fetch github.com/gin-gonic/gin</code></pre><blockquote><p>上面 fetch 需要设置代理才能通过，还是老办法：</p><pre><code class="bash">$ export http_proxy=&quot;http://127.0.0.1:12333&quot;$ export https_proxy=&quot;http://127.0.0.1:12333&quot;</code></pre></blockquote><p>这样一个非常小的微服务应用就做完了，这时需要测试一下，看看是否正常运行。这时切换到 <strong>GOPARH</strong> 文件夹。执行下面的语句</p><pre><code class="bash">$ go install github.com/rh01/goappk8s &amp;&amp; ./bin/goappk8s[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.[GIN-debug] [WARNING] Running in &quot;debug&quot; mode. Switch to &quot;release&quot; mode in production. - using env:   export GIN_MODE=release - using code:  gin.SetMode(gin.ReleaseMode)[GIN-debug] GET    /ping                     --&gt; main.main.func1 (3 handlers)[GIN-debug] Listening and serving HTTP on :8080</code></pre><p>这时会打印出如上面所示的日志信息，这时我们可以通过 curl 来访问。</p><pre><code class="bash">$ curl localhost:8080/pingPONG</code></pre><p>这时我们的服务是已经正常可以运行的了。</p><h2 id="打包成Docker镜像（使用ARM）"><a href="#打包成Docker镜像（使用ARM）" class="headerlink" title="打包成Docker镜像（使用ARM）"></a>打包成Docker镜像（使用ARM）</h2><p>这里主要使用了 Docker 的多阶段构建来打造一个非常小的镜像，这对我们的边缘端来讲，是非常必要的，因为他们的资源是非常有限的。</p><blockquote><p>有关 Docker镜像的多阶段构建以及Docker镜像优化问题，请详见我之前的一篇文章：</p><ul><li><a href="https://www.41sh.cn/?id=25" target="_blank" rel="noopener">https://www.41sh.cn/?id=25</a></li></ul></blockquote><p>下面我们看一下Dockerfile文件吧。</p><pre><code class="dockerfile">FROM golang AS build-envADD . /go/src/appWORKDIR /go/src/appENV http_proxy http://192.168.1.9:12333ENV https_proxy http://192.168.1.9:12333RUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-serverFROM armhf/alpine:latestRUN apk add -U tzdataRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai  /etc/localtimeCOPY --from=build-env /go/src/app/app-server /usr/local/bin/app-serverEXPOSE 8080CMD [ &quot;app-server&quot; ]</code></pre><blockquote class="colorquote info"><p>这里也参考了知乎的这片文章：<a href="https://zhuanlan.zhihu.com/p/33813413" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33813413</a></p><p>主要修改如下：</p><p>(1).  增加了</p><pre><code class="dockerfile"><span class="keyword">ENV</span> http_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span><span class="keyword">ENV</span> https_proxy http://<span class="number">192.168</span>.<span class="number">1.9</span>:<span class="number">12333</span></code></pre><p>这是因为国内的网络环境，我们需要添加代理，才能拉取相关的依赖包</p><p>(2).  修改了</p><pre><code class="dockerfile"><span class="keyword">RUN</span><span class="bash"> GOOS=linux GOARCH=arm GOARM=7 go build -v -o /go/src/app/app-server</span></code></pre><p> 这是因为我们需要在树莓派 arm架构下去编译和运行golang程序，因此需要交叉编译。</p><p>(3). 修改了</p><pre><code class="dockerfile"><span class="keyword">FROM</span> armhf/alpine:latest</code></pre><p>这个也是因为硬件架构的不同进行相应的修改</p></blockquote><p>然后构建<code>Docker</code>镜像：</p><pre><code class="bash">$ docker build -t rh02/goappk8s:v1.0.0 ........(省略了)Successfully built 00751f94d8a9Successfully tagged cnych/goappk8s:v1.0.0$ docker push rh02/goappk8s:v1.0.0</code></pre><p>上面的操作可以将我们本地的镜像<code>rh02/goappk8s:v1.0.0</code>推送到公共的<code>dockerhub</code>上面去（前提是你得先注册了dockerhub）。</p><h2 id="将服务部署在kubernetes"><a href="#将服务部署在kubernetes" class="headerlink" title="将服务部署在kubernetes"></a>将服务部署在kubernetes</h2><p>如果要将微服务部署在kubernetes上，只需要写一个yaml文件，定义好你需要的资源对象即可，下面先给出我们的部署的yaml文件内容。</p><pre><code class="yaml">---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: goapp-deploy  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  replicas: 2  revisionHistoryLimit: 10  minReadySeconds: 5  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 1  template:    metadata:      labels:        k8s-app: goappk8s    spec:      containers:      - image: rh02/goappk8s:v1.1.0        imagePullPolicy: Always        name: goappk8s        ports:        - containerPort: 8080          protocol: TCP        resources:          limits:            cpu: 100m            memory: 100Mi          requests:            cpu: 50m            memory: 50Mi        livenessProbe:          tcpSocket:            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 3        readinessProbe:          httpGet:            path: /ping            port: 8080          initialDelaySeconds: 10          timeoutSeconds: 2---apiVersion: v1kind: Servicemetadata:  name: goapp-svc  namespace: kube-apps  labels:    k8s-app: goappk8sspec:  ports:    - name: api      port: 8080      protocol: TCP      targetPort: 8080  selector:    k8s-app: goappk8s---kind: IngressapiVersion: extensions/v1beta1metadata:  name: goapp-ingressspec:  rules:  - host: k8sapp1.41sh.cn    http:      paths:      - path: /        backend:          serviceName: goapp-svc          servicePort: api</code></pre><blockquote><p>这里的k8sapp1.41sh.cn 需要解析为ingress的node。详细可以参考 <a href="https://github.com/rh01/traefik-for-pi" target="_blank" rel="noopener">https://github.com/rh01/traefik-for-pi</a></p></blockquote><p>因为我们编写了一个无状态的应用，因此上面主要创建了三个资源对象，分别为deployment（部署），service（服务）和ingress（主要负责负载均衡和提供一种访问的方式）对象。</p><p>使用kubectl来创建这三个资源对象。</p><pre><code class="bash">$ kubectl apply -f deployment.yamldeployment &quot;goapp-deploy&quot; createdservice &quot;goapp-svc&quot; createdingress &quot;goapp-ingress&quot; created</code></pre><p>这时我们需要创建一个traefik的ingress应用，用来处理ingress的请求。</p><pre><code class="bash">$ kubectl label node edge-node2 ingress-controller=traefik $ kubectl apply -f traefik.yaml</code></pre><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>这时，我们都已经做完了，这时我们可以打开我们的dashboard看看怎么样。并且可以通过在浏览器中访问 <a href="http://k8sapp1.41sh.cn/ping" target="_blank" rel="noopener">http://k8sapp1.41sh.cn/ping</a> 来访问我们的微服务。</p><p>下面是成果截图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423865486307.png" alt="屏幕快照 2019-03-01 下午3.03.48.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903011551423899753911.png" alt="屏幕快照 2019-03-01 下午3.04.39.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在如何将微服务部署在我的边缘机器上（树莓派），通过kubernetes管理。然后就开始了今天的项目成果，为此特别对此总结。&lt;/p&gt;
&lt;p&gt;本次的微服务我使用的golang，主要因为golang的天然适合开发云端服务的特性并且golang的轻便，包的管理方便等特点。并且golang的docker镜像稍微小，占用空间小。&lt;/p&gt;
    
    </summary>
    
      <category term="RPI kubernetes" scheme="https://readailib.com/categories/RPI-kubernetes/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/categories/RPI-kubernetes/Kubernetes/"/>
    
      <category term="Microservice" scheme="https://readailib.com/categories/RPI-kubernetes/Kubernetes/Microservice/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
      <category term="Edge Computing" scheme="https://readailib.com/tags/Edge-Computing/"/>
    
      <category term="Gin" scheme="https://readailib.com/tags/Gin/"/>
    
  </entry>
  
  <entry>
    <title>使用minikube快速安装istio集群</title>
    <link href="https://readailib.com/2019/02/22/kubernetes/istio-minikube/"/>
    <id>https://readailib.com/2019/02/22/kubernetes/istio-minikube/</id>
    <published>2019-02-22T05:48:07.000Z</published>
    <updated>2019-03-07T07:33:38.744Z</updated>
    
    <content type="html"><![CDATA[<p>今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。</p><p>本节课不教：</p><ul><li>使用minikube部署多节点的kubernetes集群，详细教程请看：<a href="https://www.41sh.cn/?id=53" target="_blank" rel="noopener">https://www.41sh.cn/?id=53</a></li></ul><p>本节课的目标是：</p><ul><li><p>使用Helm或者手动方式来构建istio集群</p></li><li><p>使用istio框架来部微服务</p></li><li><p>服务治理与金丝雀发布等等</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820119606413.png" alt="23534644.png"></p></li></ul><h2 id="启动minikube集群"><a href="#启动minikube集群" class="headerlink" title="启动minikube集群"></a>启动minikube集群</h2><p>使用下面的指令在本地启动两个节点的k8s集群，分别为master和node节点，并使node节点加入到集群中。</p><pre><code class="bash"># 启动master节点，名字为k8s-m1$ minikube --profile k8s-m1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24# 以同样的方式启动node节点，名字为k8s-n1$ minikube --profile k8s-n1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24</code></pre><p>确认两个节点已经启动，但是现在node节点并没有加入到集群中，你可能使用下面指令会看到NotReady的标示，也有可能列表中不会出现k8s-n1的条目。</p><pre><code class="bash"># 切换到master节点的配置中，这样才可以使用kubectl来查询资源信息$ kubectl config --use-context k8s-m1# 查看当前的节点列表$ kubectl get no</code></pre><p>这时候需要将node节点加入集群，使用下面指令获得TOKEN，并且使用kubeadm join指令，使得node节点加入集群。</p><pre><code class="bash"># 获取master节点的ip地址$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;# 获取当前的TOKEN列表$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 执行下面指令进入 k8s-n1$ minikube --profile k8s-n1 ssh# 下面为进入 k8s-n1 VM 內执行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下结果后，即可以在 k8s-m1 context 来操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><p>这时候我们回到我们本地的机器（非VM），通过执行下面命令确认k8s-n1已经准备完成。</p><pre><code class="bash"># 查看当前的节点列表$ kubectl get no</code></pre><h2 id="使用helm安装istio"><a href="#使用helm安装istio" class="headerlink" title="使用helm安装istio"></a>使用helm安装istio</h2><p>为了方便起见，这里我使用helm chart来安装istio，这里参考了下面的文档：</p><ul><li><a href="https://istio.io/docs/setup/kubernetes/helm-install/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/helm-install/</a></li><li><a href="https://istio.io/docs/setup/kubernetes/download-release/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/download-release/</a></li></ul><p><em>1). 使用下面的命令安装helm</em>（mac系统）</p><pre><code class="bash">$ brew install kubernetes-helm</code></pre><blockquote><p>其他系统的安装方式请参考：<br><a href="https://helm.sh/docs/using_helm/#installing-helm" target="_blank" rel="noopener">https://helm.sh/docs/using_helm/#installing-helm</a></p></blockquote><p>这里是自动安装的，这里你安装的只是一个helm client，你需要又一个helm后端来支持helm自动化部署的功能，你也可以通过下面的命令查看当前的helm的安装情况以及版本。</p><pre><code class="bash">$ helm version</code></pre><blockquote><p>如果出现Server端没有起来的情况，使用helm init –service-account tiller 就自动将Tiller端安装到kubernetes集群中  。</p></blockquote><p><em>2). 下载istio并准备安装</em></p><p>使用下面的指令自动下载istio的最新发行版本，并解压到当前的文件夹下，这里有一个istio的客户端二进制文件，这时你需要手动的将二进制文件的目录添加到PATH环境变量中。</p><pre><code class="bash">$ curl -L https://git.io/getLatestIstio | sh -$ export PATH=&quot;$PATH:/Users/rh01/istio-1.0.6/bin&quot; #这是会话环境变量的设置，临时使用，如果想永久生效，请添加到 ~/.bashrc 或者 /etc/profile中</code></pre><p><em>3). 安装istioz</em></p><p>切换到istio的文件夹下，并使用helm 安装 install 文件夹下面的yaml 来创建istio。下面的命令是我在本地测试过的：</p><pre><code class="bash">$ cd istio-1.0.6$ helm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; $HOME/istio-1.0.6/istio.yaml$ kubectl create namespace istio-system                              # 创建istio-system命名空间，之后管理istio的所有资源$ kubectl apply -f install/kubernetes/helm/helm-service-account.yaml # 创建helm服务账号，使得helm能够有权限对istio-system命名空间进行操作$ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml # 创建crd$ kubectl apply -f $HOME/istio-1.0.6/istio.yaml                      # 创建istio所有资源对象$ kubectl get po -n istio-systemNAME                                      READY     STATUS      RESTARTS   AGEistio-citadel-6f444d9999-7l2hx            1/1       Running     0          1mistio-cleanup-secrets-qjk7c               0/1       Completed   0          1mistio-egressgateway-6d79447874-v6xds      1/1       Running     0          1mistio-galley-685bb48846-pxcs2             1/1       Running     0          1mistio-ingressgateway-5b64fffc9f-4jnr9     1/1       Running     0          1mistio-pilot-8645f5655b-s6nbq              0/2       Pending     0          1mistio-policy-547d64b8d7-vtxqh             2/2       Running     0          1mistio-security-post-install-r27sv         0/1       Completed   0          1mistio-sidecar-injector-5d8dd9448d-bwcvg   1/1       Running     0          1mistio-telemetry-c5488fc49-qr7sg           2/2       Running     0          1mprometheus-76b7745b64-pm68q               1/1       Running     0          1m</code></pre><blockquote class="colorquote warning"><p><strong>遇到的坑：</strong>等等我遇到了一个istio-pilot内存不足的情况，因为我是两个节点，但是因为是在本地创建的两个虚拟机，所有内存和CPU资源都比较小，因此当出现资源不足的时候，就Pending了,这时候需要手动修改一下请求的内存资源的大小就可以了。</p><p><em>使用到的命令</em>：</p><pre><code class="bash">$ kubectl edit istio-pilot-xxx -n istio-system<span class="comment"># 修改resource的request的memory为100Mi即可</span></code></pre></blockquote><h2 id="部署一个应用"><a href="#部署一个应用" class="headerlink" title="部署一个应用"></a>部署一个应用</h2><p>这个是官方给出的一个例子 bookinfo，这个应用由四个微服务组成，下面是应用的架构图。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/03/201903061551882998396723.png" alt="20190222153800_46088.png"></p><p>下面的所有命令以及说明部分来自下面的文档：</p><ul><li><a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a></li><li><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></li></ul><p>下面的指令将会在kubernetes上部署一个bookinfo应用，并由istio提供微服务的一些服务调用和路由等等功能，具体的istio特性，后期见。</p><pre><code class="bash">$ kubectl label namespace default istio-injection=enablednamespace &quot;default&quot; labeled$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlservice &quot;details&quot; createddeployment.extensions &quot;details-v1&quot; createdservice &quot;ratings&quot; createddeployment.extensions &quot;ratings-v1&quot; createdservice &quot;reviews&quot; createddeployment.extensions &quot;reviews-v1&quot; createddeployment.extensions &quot;reviews-v2&quot; createddeployment.extensions &quot;reviews-v3&quot; createdservice &quot;productpage&quot; createddeployment.extensions &quot;productpage-v1&quot; created$ kubectl get servicesNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGEdetails       ClusterIP   10.97.137.200    &lt;none&gt;        9080/TCP   7skubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    56mproductpage   ClusterIP   10.106.43.117    &lt;none&gt;        9080/TCP   6sratings       ClusterIP   10.108.175.114   &lt;none&gt;        9080/TCP   7sreviews       ClusterIP   10.96.73.150     &lt;none&gt;        9080/TCP   6s$ kubectl get pods$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yamlgateway.networking.istio.io &quot;bookinfo-gateway&quot; createdvirtualservice.networking.istio.io &quot;bookinfo&quot; created$  kubectl get gatewayNAME               AGEbookinfo-gateway   9s$ kubectl get svc istio-ingressgateway -n istio-systemNAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                   AGEistio-ingressgateway   LoadBalancer   10.101.64.224   &lt;pending&gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31643/TCP,8060:30348/TCP,853:30934/TCP,15030:32334/TCP,15031:32681/TCP   17m</code></pre><p>这时候大家会看到istio-ingressgateway服务的EXTERNAL_IP是pending状态，这是因为我们没有指定外置的负载均衡器的ip地址，这里有三种处理方式：</p><ul><li>如果处于云服务厂商的环境，并且有负载均衡器，这时候就填写负载均衡器的ip地址</li><li>如果没有，可以采取将服务的类型改为NodePort类型，或者直接使用提供的NodePort，这样就可以使用http://&lt;Ingress所在Node的IP&gt;:NodePort/productpage的方式访问</li><li>另外你也可以将ingress对象处于的Node的ip作为ExternalIP，也是可以访问的。这样就可以直接使用<a href="http://ExternalIP:80/productpage进行访问。" target="_blank" rel="noopener">http://ExternalIP:80/productpage进行访问。</a></li></ul><blockquote><p><em>具体的如何获取ingress和配置ingress，详见下面的文档</em>：<br><a href="https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports" target="_blank" rel="noopener">https://istio.io/docs/tasks/traffic-management/ingress/#determining-the-ingress-ip-and-ports</a></p></blockquote><h2 id="大功告成"><a href="#大功告成" class="headerlink" title="大功告成"></a>大功告成</h2><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550820787188445.png" alt="屏幕快照 2019-02-22 下午3.15.31.png"></p><p>如果继续刷新，会发现有三个版本的应用出现，就是红色的星星，黑色的星星，没有星星这三个版本，会随机出现，我们可以使用istio来管理这些不同版本的应用，通过金丝雀发布，灰度发布等一些高级特性实现切流量的功能，后面的文章我会详细介绍有关istio的高级特性。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821049797553.png" alt="屏幕快照 2019-02-22 下午3.37.12.png"></p><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/201902221550821077460003.png" alt="屏幕快照 2019-02-22 下午3.37.39.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天带着大家如何在minikube本地kubernetes测试集群中安装和尝试部署一个服务。&lt;/p&gt;
&lt;p&gt;本节课不教：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用minikube部署多节点的kubernetes集群，详细教程请看：&lt;a href=&quot;https://www.41sh.c
      
    
    </summary>
    
      <category term="istio" scheme="https://readailib.com/categories/istio/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Simple Example" scheme="https://readailib.com/tags/Simple-Example/"/>
    
      <category term="istio" scheme="https://readailib.com/tags/istio/"/>
    
  </entry>
  
  <entry>
    <title>使用 Minikube 来部署本地 kubernetes 多节点集群</title>
    <link href="https://readailib.com/2019/02/18/kubernetes/multi-nodes-kubernetes-using-minikube/"/>
    <id>https://readailib.com/2019/02/18/kubernetes/multi-nodes-kubernetes-using-minikube/</id>
    <published>2019-02-18T07:57:21.000Z</published>
    <updated>2019-03-07T08:19:23.559Z</updated>
    
    <content type="html"><![CDATA[<p>一般来讲，使用minikube的目的主要用于作为本地单机测试集群，也只能构建单节点的kubernetes集群，本文章参看<a href="https://k2r2bai.com/" target="_blank" rel="noopener">凯仁兄</a>的方法使得minikube能够借助vitual box软件来实现多节点的部署，其中包括Master/Worker节点的部署与安装，下面主要针对kubernetes的最新版本kubernetes 1.13.2, 网络插件为Calico，主要为了测试Network Policy的功能。</p><a id="more"></a><p><img src="https://www.41sh.cn/zb_users/upload/2019/02/20190218164923_24529.jpg" alt="img"></p><h2 id="系统准备"><a href="#系统准备" class="headerlink" title="系统准备"></a>系统准备</h2><p>下面一定要确保以下的步骤都已经执行，所有要安装的软件包已经安转。</p><p>第一步肯定是准备minikube的执行文件，下面我列出了不同平台的二进制文件的下载地址：</p><ul><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-linux-amd64" target="_blank" rel="noopener">Linux</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-darwin-amd64" target="_blank" rel="noopener">Mac OS X</a></li><li><a href="https://github.com/kairen/minikube/releases/download/v0.33.1-multi-node/minikube-windows-amd64.exe" target="_blank" rel="noopener">Windows</a></li></ul><blockquote class="colorquote danger"><p><strong>误操作：</strong>  为了能够使 minikube 能够启动多节点的集群，一定使用上面给出的二进制执行文件链接。</p><p><em>提示：</em></p><p>如果你要使用 官方给的 minikube 的二进制文件来启动集群，此时如果你要使用网络插件Calico，请手动输入下面的命令即可生效。</p><pre><code class="bash">$ kubectl apply -f https://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml</code></pre></blockquote><p>第二步，需要将<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">Virtual Box</a>下载下来，然后提供给minikube来创建虚拟机。</p><blockquote><p>IMPORTANT: 测试机器一定要开启 VT-x or AMD-v virtualization.</p><p>虽然建议使用 VBox，但是也可以其他的虚拟化解决方案，比如使用 KVM, xhyve等虚拟机管理软件。</p></blockquote><p>第三步，下载测试机器操作系统适配的 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubeclt</a>。</p><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><p>本节主要介绍如何使用minikube来建立集群，并且相应的创建master节点和worker节点。</p><p>开始之前，确认本测试机器是否已经安装过Minikube，如果有的话，就把上面下载二进制文件放置在任意方便的位置，或者直接替代之前的然后在启动集群之前，删除Home文件夹下的.minikube文件夹。</p><pre><code class="bash">$ rm -rf $HOME/.minikube</code></pre><h3 id="Master-节点"><a href="#Master-节点" class="headerlink" title="Master 节点"></a>Master 节点</h3><p>首先通过 Minikube 执行以下指令来启动 Master节点，并通过 kubectl 检查：</p><pre><code class="bash">$ minikube --profile k8s-m1 start --network-plugin=cni --docker-env HTTP_PROXY=http://192.168.99.1:12333 --docker-env HTTPS_PROXY=http://192.168.99.1:12333 --docker-env NO_PROXY=127.0.0.1/24?  minikube v0.34.1 on darwin (amd64)?  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...?  &quot;k8s-m1&quot; IP address is 192.168.99.102?  Configuring Docker as the container runtime ...    ▪ env HTTP_PROXY=http://192.168.99.1:12333    ▪ env HTTPS_PROXY=http://192.168.99.1:12333    ▪ env NO_PROXY=127.0.0.1/24✨  Preparing Kubernetes environment ...?  Pulling images required by Kubernetes v1.13.3 ...?  Launching Kubernetes v1.13.3 using kubeadm ... ?  Configuring cluster permissions ...?  Verifying component health .....?  kubectl is now configured to use &quot;k8s-m1&quot;?  Done! Thank you for using minikube!</code></pre><blockquote><p><code>--vm-driver</code> 可以选择使用其他 VM driver 启动虚拟机，如 xhyve、hyperv、hyperkit 与 kvm2 等等。</p></blockquote><p>完成后，确认 k8s-m1 节点處处于Ready 状态：</p><pre><code class="bash">$ kubectl get noNAME     STATUS   ROLES    AGE    VERSIONk8s-m1   Ready    master   2m8s   v1.13.2</code></pre><p>下面来部署Node节点，通过minikube执行下面指令来启动Node节点。</p><pre><code>$ minikube --profile k8s-n1 start --network-plugin=cni --node...Stopping extra container runtimes...# 接着取得 Master IP 与 Token$ minikube --profile k8s-m1 ssh &quot;ifconfig eth1&quot;$ minikube --profile k8s-m1 ssh &quot;sudo kubeadm token list&quot;# 執行以下指令進入 k8s-n1$ minikube --profile k8s-n1 ssh#  下面是 在k8s-n1 VM 里执行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token ${TOKEN} ${MASTER_IP}:8443 \    --discovery-token-unsafe-skip-ca-verification \    --ignore-preflight-errors=Swap \    --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下结果后，即可以在 k8s-m1 context 操作。...Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><blockquote class="colorquote warning"><p>1). 上面的 IP 有可能不同，请手动确认 Master 节点 IP。</p><p>2).  如果上面的TOKEN失效，请使用下面的指令生成一个，或者生成TOKEN时指定tt值为-1</p><pre><code class="bash">$ minikube --profile k8s-m1 ssh <span class="string">"sudo kubeadm token new"</span></code></pre></blockquote><p>接下来，我们可以通过 kuubectl 客户端检查Node是否加入到集群：</p><pre><code>$ kubectl config use-context k8s-m1Switched to context &quot;k8s-m1&quot;.$ kubectl get noNAME     STATUS   ROLES    AGE     VERSIONk8s-m1   Ready    master   3m44s   v1.13.2k8s-n1   Ready    &lt;none&gt;   80s     v1.13.2$ kubectl get csrNAME                                                   AGE    REQUESTOR                 CONDITIONnode-csr-Ut1k5mLXpXVsyZwjn2z2-fpie9HHyTkMU7wnrjDnD3E   118s   system:bootstrap:3qeeeu   Approved,Issued$ kubectl -n kube-system get po -o wideNAME                             READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATEScalico-node-qxkw5                2/2     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;calico-node-srhlk                2/2     Running   0          3m24s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-826nz         1/1     Running   0          3m27s   10.244.0.3       k8s-m1   &lt;none&gt;           &lt;none&gt;coredns-86c58d9df4-9z7mr         1/1     Running   0          3m27s   10.244.0.2       k8s-m1   &lt;none&gt;           &lt;none&gt;etcd-k8s-m1                      1/1     Running   0          2m40s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-m1        1/1     Running   0          3m48s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-addon-manager-k8s-n1        1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-apiserver-k8s-m1            1/1     Running   0          2m36s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-controller-manager-k8s-m1   1/1     Running   0          2m50s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-proxy-768w8                 1/1     Running   0          86s     192.168.99.101   k8s-n1   &lt;none&gt;           &lt;none&gt;kube-proxy-b7ndj                 1/1     Running   0          3m27s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;kube-scheduler-k8s-m1            1/1     Running   0          2m46s   192.168.99.100   k8s-m1   &lt;none&gt;           &lt;none&gt;storage-provisioner              1/1     Running   0          2m46s   192.168.99.100   k8s-m1   &lt;none&gt;</code></pre><p>这样一个 Kubernetes 集群就完成了，速度快一点不到 10 分钟就可以建立好了。</p><h2 id="删除虚拟机"><a href="#删除虚拟机" class="headerlink" title="删除虚拟机"></a>删除虚拟机</h2><p><em>清除环境一条指令即可：</em></p><pre><code>$ minikube --profile &lt;node_name&gt; delete</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般来讲，使用minikube的目的主要用于作为本地单机测试集群，也只能构建单节点的kubernetes集群，本文章参看&lt;a href=&quot;https://k2r2bai.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;凯仁兄&lt;/a&gt;的方法使得minikube能够借助vitual box软件来实现多节点的部署，其中包括Master/Worker节点的部署与安装，下面主要针对kubernetes的最新版本kubernetes 1.13.2, 网络插件为Calico，主要为了测试Network Policy的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Minikube Multi-Node Cluster" scheme="https://readailib.com/categories/Minikube-Multi-Node-Cluster/"/>
    
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dooker" scheme="https://readailib.com/tags/Dooker/"/>
    
      <category term="Calico" scheme="https://readailib.com/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>在kubernetes API和client-go中使用 Go modules</title>
    <link href="https://readailib.com/2019/02/15/kubernetes/kubernetes-api-gomod/"/>
    <id>https://readailib.com/2019/02/15/kubernetes/kubernetes-api-gomod/</id>
    <published>2019-02-15T08:41:31.000Z</published>
    <updated>2019-03-13T07:19:30.776Z</updated>
    
    <content type="html"><![CDATA[<p>现如今kubernetes和golang的发展非常之快，Golang的依赖管理也不断的更新换代，从最初的<code>Go dep</code> 到现在<code>go mod</code>。本篇文章主要介绍如何使用<code>go mod</code>来管理项目依赖。</p><blockquote class="colorquote warning"><p>这篇文章不是 <strong><em>Go module</em></strong> 教程，网上有很多关于这个主题的资料。大家可以参考下面的：</p><ul><li><a href="https://github.com/golang/go/wiki/Modules" target="_blank" rel="noopener">https://github.com/golang/go/wiki/Modules</a></li><li><a href="https://www.youtube.com/watch?v=H_4eRD8aegk" target="_blank" rel="noopener">justforfunc #43: Migrating Go Modules to v2+</a></li></ul></blockquote><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>在开始之前，需要确认以下：</p><ul><li>Go 的版本要在1.11以上</li><li>在GOPATH之外，即 <code>$GOPATH/src</code> 之外新建了一个目录作为本地的项目文件夹</li><li>确认 <code>GO111MODULE=on</code> 环境变量是否正确</li></ul><p>为此，我在GOPATH之外创建了一个目录。我将创建一个简单的PVC控制器，它可以监控所需的PVC数量。</p><h2 id="从-Dep-工具开始-或其他工具）"><a href="#从-Dep-工具开始-或其他工具）" class="headerlink" title="从 Dep 工具开始(或其他工具）"></a>从 Dep 工具开始(或其他工具）</h2><p>对于使用<a href="https://github.com/kubernetes/client-go" target="_blank" rel="noopener">client-go</a>编写Kubernetes API工具的大多数人来说，都可能正在使用<code>Dep</code>来管理依赖。幸运的是，<code>go mod</code>工具可以从<code>Dep</code>（以及其他工具，如Godep,Govendor和Glide）导入依赖项配置。我的示例项目使用<code>dep</code>与以下<code>Gopkg.toml</code>：</p><pre><code class="toml">[[constraint]]  name = &quot;k8s.io/api&quot;  version = &quot;kubernetes-1.9.0&quot;[[constraint]]  name = &quot;k8s.io/apimachinery&quot;  version = &quot;kubernetes-1.9.0&quot;[[constraint]]  name = &quot;k8s.io/client-go&quot;  version = &quot;6.0.0&quot;</code></pre><p>可以看出，代码使用旧版本的client-go版本6.0.0和Kubernetes API版本1.9。</p><p>首先要做的是将项目初始化为模块。从代码的根目录，使用以下命令：</p><pre><code class="bash">$ cd ./pvcwatch$ go mod init github.com/vladimirvivien/pvcwatch</code></pre><p> <code>mod</code> 命令将会创建一个新的文件 <code>go.mod</code> 并复制Dep的依赖信息。</p><pre><code class="bash">go: creating new go.mod: module github.com/vladimirvivien/pvcwatchgo: copying requirements from Gopkg.lock</code></pre><p>这时，可以看一下从<code>Gopkg.toml</code>生成的<code>go.mod</code>文件的内容。</p><pre><code class="bash">module github.com/vladimirvivien/pvcwatchrequire (  github.com/PuerkitoBio/purell v1.1.0...   k8s.io/api v0.0.0-20171214033149-af4bc157c3a2  k8s.io/apimachinery v0.0.0-20171207040834-180eddb345a5  k8s.io/client-go v6.0.0+incompatible  k8s.io/kube-openapi v0.0.0-20180216212618-50ae88d24ede)</code></pre><p>生成的<code>go.mod</code>文件依然遵循<code>Gopkg.toml</code>版本限制.</p><blockquote><p><code>go mod</code> <em>will resort to the latest version of discovered packages that do not have any version information (from Dep or otherwise). If you don’t want that, you can update or downgrade to your preferred version (discussed later).</em></p></blockquote><p>此时，您可以像平常一样构建代码。除此之外，构建工具将显示包解析的进度：</p><pre><code class="bash">$ go build .go: finding github.com/golang/protobuf v1.0.0...go: downloading github.com/modern-go/reflect2 v0.0.0-20180228065516-1df9eeb2bb81go: downloading github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1ddgo: downloading golang.org/x/sys v0.0.0-20180322165403-91ee8cde4354</code></pre><p>如果解析了所有包，将构建成功，此时拥有了可以运行的二进制文件。</p><h2 id="从无到有"><a href="#从无到有" class="headerlink" title="从无到有"></a>从无到有</h2><p>假若我们此时的项目并没有依赖管理工具，这时怎么使用<code>go mod</code>来管理我们的依赖呢？</p><p>首先第一步，将我们的项目初始化为一个Go module：</p><pre><code class="bash">$ go mod init github.com/vladimirvivien/pvcwatchgo: creating new go.mod: module github.com/vladimirvivien/pvcwatch</code></pre><p>这时，生成的 <code>go.mod</code> 文件将会是空的，只有我们执行了Go的一些命令(比如<code>build</code>, <code>get</code>,<code>test</code>, 等等)之后，<code>go.mod</code>才会出现依赖项. 接下来, 在我们的项目文件夹下执行 <code>build</code> 命令。</p><pre><code class="bash">$ cd ./pvcwatch$ go build .go: finding k8s.io/apimachinery/pkg/util/runtime latest...go: downloading golang.org/x/sys v0.0.0-20181021155630-eda9bb28ed51go: finding golang.org/x/text/unicode latestgo: finding golang.org/x/text/secure latest</code></pre><p>在构建过程结束时，如果没有依赖性问题，我们应该获得构建的二进制文件。在这种情况下，<code>go mod</code>将自动提取所有已解析包的最新版本，如更新的go.mod文件中所示：</p><pre><code class="bash">$&gt; cat go.modmodule github.com/vladimirvivien/pvcwatchrequire (...    k8s.io/api v0.0.0-20181018013834-843ad2d9b9ae    k8s.io/apimachinery v0.0.0-20181015213631-60666be32c5d    k8s.io/client-go v9.0.0+incompatible)</code></pre><p><code>go.mod</code>显示正在使用client-go v9.0.0（最新的版本）（k8s.io/api和k8s.io/apimachinery尚未<em>SemVer’d</em>，因此使用了最新的HEAD版本）。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>Trouble will come when you want to adjust (upgrade/downgrade) client-go to a specific version. For instance, in the previous section, <code>go mod</code> selected v9.0.0 of client-go. But, let us say we want do downgrade to v7.0.0 because:</p><pre><code class="bash">$&gt; cd ./pvcwatch$&gt; go get k8s.io/client-go@v7.0.0</code></pre><p>This, unfortunately, will create a version mismatch between client-go and its dependent packages <code>k8s.io/api</code> and <code>k8s.io/apimachinery</code>:</p><pre><code class="bash">$&gt; go build .go: finding github.com/howeyc/gopass latestgo: finding k8s.io/client-go v7.0.0+incompatible#../../pkg/apis/clientauthentication/v1alpha1/zz_generated.conversion.go:39:15: scheme.AddGeneratedConversionFuncs undefined (type *runtime.Scheme has no field or method AddGeneratedConversionFuncs)</code></pre><p>现在，您必须手动确定您的依赖关系图。幸运的是，client-go附带了一个方便的兼容性矩阵，可以准确地告诉我们需要哪些版本（参见下图）。</p><p><img src="./compatible-matrix.png" alt="client-go兼容性矩阵"></p><p>根据矩阵可以看出，client-go v7.0.0与Kubernetes 1.10兼容。因此，让我们使用go get（其支持分支名称或非semver标记名称）将其他依赖组件降级到匹配版本。</p><p>降级 k8s.io/api to Kubernets-1.10:</p><pre><code class="bash"># Downgrade with a tag name$&gt; go get k8s.io/api@kubernetes-1.10.9go: finding k8s.io/api kubernetes-1.10.9go: downloading k8s.io/api v0.0.0-20180828232432-12444147eb11...# equivalent to using matching branch name$&gt; go get k8s.io/api@release-1.10go: finding k8s.io/api release-1.10</code></pre><p>降级 k8s.io/apimachinery to Kubernetes-1.10</p><pre><code>&gt; go get k8s.io/apimachinery@release-1.10go: finding k8s.io/apimachinery release-1.10go: downloading k8s.io/apimachinery v0.0.0-20180619225948-e386b2658ed2</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><code>Dep</code>将`Go包管理推向了极致的高度。现在，Go模块及其与Go命令行工具的深度集成，已经将Go依赖管理做的更上一层楼了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现如今kubernetes和golang的发展非常之快，Golang的依赖管理也不断的更新换代，从最初的&lt;code&gt;Go dep&lt;/code&gt; 到现在&lt;code&gt;go mod&lt;/code&gt;。本篇文章主要介绍如何使用&lt;code&gt;go mod&lt;/code&gt;来管理项目依赖。&lt;/p
      
    
    </summary>
    
      <category term="Go modules" scheme="https://readailib.com/categories/Go-modules/"/>
    
    
      <category term="Golang" scheme="https://readailib.com/tags/Golang/"/>
    
      <category term="Go modules" scheme="https://readailib.com/tags/Go-modules/"/>
    
      <category term="client-go" scheme="https://readailib.com/tags/client-go/"/>
    
  </entry>
  
  <entry>
    <title>在仪表盘上增加heapster指标</title>
    <link href="https://readailib.com/2019/01/15/kubernetes/add-heapster-to-dashboard/"/>
    <id>https://readailib.com/2019/01/15/kubernetes/add-heapster-to-dashboard/</id>
    <published>2019-01-15T08:41:31.000Z</published>
    <updated>2019-03-12T07:48:04.942Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547542105188485.png" alt="2019-01-15 16-47-57屏幕截图.png"></p><p>Prerequisites: You have a kubernetes cluster with the dashboard plugin installed, see the article from <a href="https://medium.com/@mrjensens" target="_blank" rel="noopener">Martin Jensen</a> entitled Kubernetes dashboard on ARM with RBAC for the instructions on how to do that.</p><a id="more"></a><pre><code class="bash">$ git clone $ cd heapster/</code></pre><p>and edit the heapster.yaml and influxdb.yaml files to change the image architecture from -amd64 to -arm. For example image: k8s.gcr.io/heapster-amd64:v1.4.2 should be changed to image: k8s.gcr.io/heapster-arm:v1.4.2</p><pre><code class="bash">$ kubectl create -f influxdb.yaml$ kubectl create -f heapster.yaml</code></pre><p>to deploy the heapster and influxeb deployment, service and serviceaccounts.</p><p>Now we need to add the roles from <a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">heapster</a>/<a href="https://github.com/kubernetes/heapster/tree/master/deploy" target="_blank" rel="noopener">deploy</a>/<a href="https://github.com/kubernetes/heapster/tree/master/deploy/kube-config" target="_blank" rel="noopener">kube-config</a>/rbac/.</p><pre><code class="bash">$ cd rbac/$ kubectl create -f heapster-rbac.yaml</code></pre><p>If you go back to the kubernetes dashboard, you will not see any metrics. They are being collected but will not appear until we restart the dashboard.</p><pre><code class="bash">$ kubectl delete -n kube-system kubernetes-dashboard-7fcc5cb979–85vt5</code></pre><p>should take care of that.</p><h2 id="Some-wrong"><a href="#Some-wrong" class="headerlink" title="Some wrong!!!"></a>Some wrong!!!</h2><hr><p>Bug solution: <a href="https://brookbach.com/2018/10/29/Heapster-on-Kubernetes-1.11.3.html" target="_blank" rel="noopener">https://brookbach.com/2018/10/29/Heapster-on-Kubernetes-1.11.3.html</a></p><blockquote class="colorquote info"><h1 id="Heapster"><a href="#Heapster" class="headerlink" title="Heapster"></a>Heapster</h1><h2 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h2><p>First, clone the Heapster repository.</p><pre><code class="bash">$ git <span class="built_in">clone</span> https://github.com/kubernetes/heapster/$ <span class="built_in">cd</span> heapster</code></pre><p>Then, set Grafana Service Type to NodePort and downgrade container version from 5.0.4 to 4.4.3 as follows. The reason why I downgrade the version is the dashboard on Grafana is not shown in 5.0.4.</p><pre><code class="bash">$ diff --git a/deploy/kube-config/influxdb/grafana.yaml b/deploy/kube-config/influxdb/grafana.yamlindex 216bd9a..266f47a 100644--- a/deploy/kube-config/influxdb/grafana.yaml+++ b/deploy/kube-config/influxdb/grafana.yaml@@ -13,7 +13,7 @@ spec:     spec:       containers:       - name: grafana-        image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4+        image: k8s.gcr.io/heapster-grafana-amd64:v4.4.3         ports:         - containerPort: 3000           protocol: TCP@@ -64,7 +64,7 @@ spec:   <span class="comment"># or through a public IP.</span>   <span class="comment"># type: LoadBalancer</span>   <span class="comment"># You could also use NodePort to expose the service at a randomly-generated port</span>-  <span class="comment"># type: NodePort</span>+  <span class="built_in">type</span>: NodePort   ports:   - port: 80     targetPort: 3000</code></pre><p>In this point, The pods are launched if I <code>kubectl apply</code> under <code>deploy/kube-config/rbac</code>and <code>deploy/kube-config/influxdb</code> , but the following error logs are generated and not worked correctly.</p><pre><code class="bash">E1028 07:39:05.011439       1 manager.go:101] Error <span class="keyword">in</span> scraping containers from Kubelet:XX.XX.XX.XX:10255: failed to get all container stats from Kubelet URL <span class="string">"http://XX.XX.XX.XX:10255/stats/container/"</span>: Post http://XX.XX.XX.XX:10255/stats/container/: dial tcp XX.XX.XX.XX:10255: getsockopt: connection refused</code></pre><h2 id="Apply-patch"><a href="#Apply-patch" class="headerlink" title="Apply patch"></a>Apply patch</h2><p>After googling the error message, I found <a href="https://github.com/kubernetes/heapster/issues/1936" target="_blank" rel="noopener">this issue</a>.</p><p>It looks the port to access are changed and I need to deal with HTTPS.</p><p>So I edit <code>deploy/kube-config/influxdb/heapster.yaml</code> and <code>deploy/kube-config/rbac/heapster-rbac.yaml</code> as below.</p><pre><code class="bash">diff --git a/deploy/kube-config/influxdb/heapster.yaml b/deploy/kube-config/influxdb/heapster.yamlindex e820ca5..195061a 100644--- a/deploy/kube-config/influxdb/heapster.yaml+++ b/deploy/kube-config/influxdb/heapster.yaml@@ -24,7 +24,7 @@ spec:         imagePullPolicy: IfNotPresent         <span class="built_in">command</span>:         - /heapster-        - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default+        - --<span class="built_in">source</span>=kubernetes.summary_api:<span class="string">''</span>?useServiceAccount=<span class="literal">true</span>&amp;kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250&amp;insecure=<span class="literal">true</span>         - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 --- apiVersion: v1diff --git a/deploy/kube-config/rbac/heapster-rbac.yaml b/deploy/kube-config/rbac/heapster-rbac.yamlindex 6e63803..1f982fb 100644--- a/deploy/kube-config/rbac/heapster-rbac.yaml+++ b/deploy/kube-config/rbac/heapster-rbac.yaml@@ -5,7 +5,7 @@ metadata: roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole-  name: system:heapster+  name: heapster subjects: - kind: ServiceAccount   name: heapster</code></pre><p>Moreover, I create <code>deploy/kube-config/rbac/heapster-role.yaml</code>.</p><pre><code class="yaml"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span><span class="attr">kind:</span> <span class="string">ClusterRole</span><span class="attr">metadata:</span><span class="attr">  name:</span> <span class="string">heapster</span><span class="attr">rules:</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">""</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">pods</span><span class="bullet">  -</span> <span class="string">nodes</span><span class="bullet">  -</span> <span class="string">namespaces</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span><span class="bullet">  -</span> <span class="string">list</span><span class="bullet">  -</span> <span class="string">watch</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">extensions</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">deployments</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span><span class="bullet">  -</span> <span class="string">list</span><span class="bullet">  -</span> <span class="string">update</span><span class="bullet">  -</span> <span class="string">watch</span><span class="attr">- apiGroups:</span><span class="bullet">  -</span> <span class="string">""</span><span class="attr">  resources:</span><span class="bullet">  -</span> <span class="string">nodes/stats</span><span class="attr">  verbs:</span><span class="bullet">  -</span> <span class="string">get</span></code></pre><h2 id="Create-pods"><a href="#Create-pods" class="headerlink" title="Create pods"></a>Create pods</h2><p>Finally, I apply these configurations.</p><pre><code class="bash">$ kubectl create -f ./deploy/kube-config/rbac/clusterrolebinding.rbac.authorization.k8s.io/heapster createdclusterrole.rbac.authorization.k8s.io/heapster created$ kubectl create -f ./deploy/kube-config/influxdb/deployment.extensions/monitoring-grafana createdservice/monitoring-grafana createdserviceaccount/heapster createddeployment.extensions/heapster createdservice/heapster createddeployment.extensions/monitoring-influxdb createdservice/monitoring-influxdb created</code></pre><p>After updating, I can configure launching Heapster, Grafana, and Influx DB by <code>kubectl get all -n kube-system</code>. And after waiting for minutes, I can see the metrics by <code>kubectl top node</code>.</p><h1 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h1><p>When I faced to some miss behavior on Kubernetes, I often find a clue of solution by seeing the log by <code>kubectl logs</code>.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://www.41sh.cn/zb_users/upload/2019/01/201901151547542105188485.png&quot; alt=&quot;2019-01-15 16-47-57屏幕截图.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Prerequisites: You have a kubernetes cluster with the dashboard plugin installed, see the article from &lt;a href=&quot;https://medium.com/@mrjensens&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Martin Jensen&lt;/a&gt; entitled Kubernetes dashboard on ARM with RBAC for the instructions on how to do that.&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes DashBoard" scheme="https://readailib.com/categories/Kubernetes-DashBoard/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dashboard" scheme="https://readailib.com/tags/Dashboard/"/>
    
      <category term="Heapster" scheme="https://readailib.com/tags/Heapster/"/>
    
  </entry>
  
  <entry>
    <title>树莓派kuernetes集群中部署dashboard</title>
    <link href="https://readailib.com/2019/01/15/kubernetes/kubernetes-dashboard/"/>
    <id>https://readailib.com/2019/01/15/kubernetes/kubernetes-dashboard/</id>
    <published>2019-01-15T08:32:03.000Z</published>
    <updated>2019-03-07T08:42:28.349Z</updated>
    
    <content type="html"><![CDATA[<p>在最近关于在树莓派集群上使用kubeadm配置安装Kubernetes 1.13.0的教程中，默认情况下启用了RBAC，本文章将介绍如何在启用RBAC的情况下运行Kubernetes仪表板。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul><li>部署和启动Kubernetes集群 (see <a href="https://www.41sh.cn/?id=16" target="_blank" rel="noopener">这篇文章</a>)</li><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a> v. 1.13.0</li></ul><h2 id="配置-kubeconfig"><a href="#配置-kubeconfig" class="headerlink" title="配置 kubeconfig"></a>配置 kubeconfig</h2><p>配置本地树莓派（并非master），以便使用kubectl与上一个教程中配置的集群进行通信。使用scp从master节点上下载配置文件</p><pre><code class="bash">$ scp pi@edge-master:/home/pi/.kube/config ./config</code></pre><p>将配置文件拷贝到本地机器上的 ~/.kube 目录下.</p><p>注意：如果你已经有一个集群的配置，它将会覆盖之前的配置，为了避免这种情况，可以在复制时添加  — kubeconfig.</p><pre><code class="b">$ cp config ~/.kube/config</code></pre><p>测试:</p><pre><code class="bash">$ kubectl get nodesNAME          STATUS   ROLES    AGE     VERSIONedge-master   Ready    master   7d23h   v1.13.1edge-node1    Ready    &lt;none&gt;   7d23h   v1.13.1edge-node2    Ready    &lt;none&gt;   7d23h   v1.13.1edge-node3    Ready    &lt;none&gt;   6d      v1.13.1</code></pre><h2 id="创建仪表盘"><a href="#创建仪表盘" class="headerlink" title="创建仪表盘"></a>创建仪表盘</h2><p><a href="https://github.com/kubernetes/dashboard/blob/master/src/deploy/recommended/kubernetes-dashboard-arm.yaml" target="_blank" rel="noopener">Kubernetes source</a> 在 kube-system 命名空间下将会创建以下几种资源: secret, service account, role, rolebinding, deployment, and service.</p><pre><code class="bash">$ kubectl create -f  https://raw.githubusercontent.com/kubernetes/dashboard/72832429656c74c4c568ad5b7163fa9716c3e0ec/src/deploy/recommended/kubernetes-dashboard-arm.yamlsecret &quot;kubernetes-dashboard-certs&quot; createdserviceaccount &quot;kubernetes-dashboard&quot; createdrole &quot;kubernetes-dashboard-minimal&quot; createdrolebinding &quot;kubernetes-dashboard-minimal&quot; createddeployment &quot;kubernetes-dashboard&quot; createdservice &quot;kubernetes-dashboard&quot; created</code></pre><blockquote class="colorquote warning"><p><strong>坑</strong>：这里会出现一个问题就是Pod总是重启，后来查资料发现dashboard的版本低，这时需要手动修改一下yaml或者在线修改pods的镜像即可。</p><p>这里介绍几个技巧：</p><p><em>查看日志</em>：</p><pre><code class="bash">$ kubectl logs -p POD_NAME</code></pre><p><em>编辑 Pod</em>:</p><pre><code class="bash">$ kubectl edit pods POD_NAME -n NAMESPACE</code></pre><p><em>导出pod或者deployment配置</em>：</p><pre><code class="bash">$ kubectl get pods POD_NAME -n NAMESPACE_NAME -o yaml</code></pre></blockquote><p>之后，您将能够从本地机器启动代理以访问刚刚创建的服务。</p><pre><code class="bash">$ nohup kubectl proxy --address 0.0.0.0 --accept-hosts &#39;.*&#39; &amp;</code></pre><p>Dashboard  可以通过  <a href="https://172.16.3.17:32351" target="_blank" rel="noopener">https://172.16.3.17:32351</a>  访问仪表盘。</p><blockquote class="colorquote warning"><p><strong>注意</strong>：</p><ul><li><p>这里要使用https协议</p></li><li><p>由于kubernetes-dashboard服务使用了NodePort的方式，因此这里的端口号为NodePort，这里的IP为NodeIP，即该POd所在的Node地址。   </p></li></ul></blockquote><p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547536342389405.png" alt="2019-01-15 15-12-04屏幕截图.png"></p><p>这里我们选择跳过，可以出现下面所示的资源管理界面。</p><p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901151547536405708187.png" alt="2019-01-15 15-13-14屏幕截图.png"></p><p>但是，这种打开方式是安全的，但是集群管理是不允许操作的，这就是说这种打开方式是匿名或者游客模式。</p><p>接下来需要创建一个 service account 在 default namespace 并且创建一个 clusterrolebinding 对象允许我们的service account 来使用 dashboard.</p><pre><code class="bash">$ kubectl create serviceaccount dashboard -n defaultserviceaccount “dashboard” created$ kubectl create clusterrolebinding dashboard-admin -n default \  --clusterrole=cluster-admin \  --serviceaccount=default:dashboardclusterrolebinding &quot;dashboard-admin&quot; created</code></pre><blockquote><p><em>如果想了解更多有关集群角色和更细的配置可以参考</em>:<br><a href="https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles" target="_blank" rel="noopener">官方文档</a>. </p></blockquote><p>以下命令可以得到我们创建的 服务帐号的 token（令牌），我们将打印出的token值复制到登录对话框中。</p><pre><code class="bash">$ kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=&quot;{.secrets[0].name}&quot;) -o jsonpath=&quot;{.data.token}&quot; | base64 --decodeeyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC10b2tlbi01bmo5ayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1OGEzNzlmMC0xODk1LTExZTktYjBlMi1iODI3ZWJiMWY4NTgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQifQ.X6evSc9wDPPa6T9CLy40gDj8aUTjVMg1PhA5bQIhSTnvVeB8mG763y9j8c0G3wOv4gCk1egziTIenpFx0w04P2zUTcqrHdse51vnbE5TNETQo8EiY2ELsqUJuaGd-O3Z6nmL8psBk4CmloPCMgYaBXPWiHPeS9dyOgTH-KxFoEEAuCX1i3BWPkYN_faN-sQe7zlrhu27lPJVUey8HGVjPu_6zGxMSWcZu2Wz3Euc1A-Rg6tekDhnxhxH_dcMRF38jSVY_z9r7mfvw6dJmxXTlH-KNzagruulH2l-Pg9obpz7HO7t14JB6c1F6p5Qa4zk2y9vOz4qCPk6IM7_ZfTwCw</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在最近关于在树莓派集群上使用kubeadm配置安装Kubernetes 1.13.0的教程中，默认情况下启用了RBAC，本文章将介绍如何在启用RBAC的情况下运行Kubernetes仪表板。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes DashBoard" scheme="https://readailib.com/categories/Kubernetes-DashBoard/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Dashboard" scheme="https://readailib.com/tags/Dashboard/"/>
    
  </entry>
  
  <entry>
    <title>在树莓派上建立kubernetes集群</title>
    <link href="https://readailib.com/2019/01/07/kubernetes/raspberrypi/build-a-kubernetes-cluster/"/>
    <id>https://readailib.com/2019/01/07/kubernetes/raspberrypi/build-a-kubernetes-cluster/</id>
    <published>2019-01-07T07:37:08.000Z</published>
    <updated>2019-03-07T08:42:31.713Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://www.41sh.cn/zb_users/upload/2019/01/201901161547625729333503.jpg" alt="IMG_0468.jpg"></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>最近学了Kubernetes一段时间后，突然想在自己的树莓派上玩玩，搭建一个集群出来，玩过树莓派的同学都知道，树莓派作为一个“卡片电脑式”的嵌入式电脑，它的性能是非常有限的，内存和CPU对于我们传统的机器智能是远远不可及的，因此我在这里PO出我为什么要做树莓派的kubernetes研究：</p><a id="more"></a><p><em>打造一个具有边缘智能的系统，可以在上面部署边缘级别的微服务组件，包括传感器的采集服务、控制和计算单元为核心的服务！</em></p><p>这里的实验环境为 1 个kubernetes master和 3 个 worker节点，共计 4 个树莓派节点。为了方便，这里的主机名以及IP地址如下：</p><ul><li>edge-master, IP:172.16.3.1</li><li>edge-node1, IP:172.16.3.17</li><li>edge-node2, IP:172.16.3.32</li><li>edge-node3, IP:172.16.3.3</li></ul><blockquote><p>这里我没有使用静态IP，因此IP地址是有可能变的，因此为了稳定，建议设置成静态IP。但为了教程的完整性，我也会说明如何设置静态IP。</p></blockquote><p>这里主要参考了下面的教程：</p><ul><li><a href="https://gist.github.com/aaronkjones/d996f1a441bc80875fd4929866ca65ad" target="_blank" rel="noopener">https://gist.github.com/aaronkjones/d996f1a441bc80875fd4929866ca65ad</a></li><li><a href="https://github.com/alexellis/k8s-on-raspbian/blob/master/GUIDE.md" target="_blank" rel="noopener">https://github.com/alexellis/k8s-on-raspbian/blob/master/GUIDE.md</a></li></ul><h2 id="系统准备-TL-DR"><a href="#系统准备-TL-DR" class="headerlink" title="系统准备(TL;DR)"></a>系统准备(TL;DR)</h2><p>这里的准备系统是要准备树莓派操作系统，一般来讲运行kubernetes这么大的系统，最好的系统选择方式为轻量级的操作系统，这里推荐两个操作系统：</p><ul><li>Raspbian Stretch Lite - <a href="https://www.raspberrypi.org/downloads/raspbian/" target="_blank" rel="noopener">https://www.raspberrypi.org/downloads/raspbian/</a></li><li>Hypriot OS - <a href="https://blog.hypriot.com/" target="_blank" rel="noopener">https://blog.hypriot.com/</a></li></ul><p>这里为了方便，我使用了 Raspbian Stretch Lite 系统作为我的边缘设备操作系统，下载地址： <a href="https://www.raspberrypi.org/downloads/raspbian/" target="_blank" rel="noopener">https://www.raspberrypi.org/downloads/raspbian/</a></p><p>树莓派系统安装都是一个样，安装教程可以参考：<a href="https://www.shenhengheng.xyz/files/respberry_doc.pdf" target="_blank" rel="noopener">https://www.shenhengheng.xyz/files/respberry_doc.pdf</a></p><p>安装完成之后，需要初始化下面的操作，包括更改主机名，密码，连接wifi，设置docker网络代理，还有设置静态IP等。</p><h3 id="初始化系统"><a href="#初始化系统" class="headerlink" title="初始化系统"></a>初始化系统</h3><ul><li>更改主机名，修改密码以及连接Wi-Fi等工作都可以通过 raspi-config 命令来完成。</li></ul><h2 id="Master节点设置"><a href="#Master节点设置" class="headerlink" title="Master节点设置"></a>Master节点设置</h2><h3 id="设置静态IP"><a href="#设置静态IP" class="headerlink" title="设置静态IP"></a>设置静态IP</h3><pre><code class="bash">$ cat &lt;&lt; EOF &gt;&gt; /etc/dhcpcd.confprofile static_eth0static ip_address=192.168.0.100/24static routers=192.168.0.1static domain_name_servers=8.8.8.8EOF</code></pre><blockquote><p>安装100，101，102，103 的格式设置其他的树莓派节点IP地址</p></blockquote><h3 id="设置网络代理"><a href="#设置网络代理" class="headerlink" title="设置网络代理"></a>设置网络代理</h3><p>因为后期会需要下载kubeadm,kubelete，docker等软件，因此需要设置网络代理，通过代理下载软件。</p><pre><code class="bash">$ export http_proxy=&quot;http://172.14.1.54:1080&quot;$ export https_proxy=&quot;http://172.14.1.54:1080&quot;</code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><p>这个步骤依赖于上面的网络代理设置的，如果设置成功了，那么这步才可能成功执行。</p><pre><code class="bash">$ curl -sSL get.docker.com | sh &amp;&amp; sudo usermod pi -aG docker$ newgrp docker</code></pre><h3 id="关闭swap"><a href="#关闭swap" class="headerlink" title="关闭swap"></a>关闭swap</h3><pre><code class="bash">$ sudo dphys-swapfile swapoff &amp;&amp; \  sudo dphys-swapfile uninstall &amp;&amp; \  sudo update-rc.d dphys-swapfile remove</code></pre><p>这个步骤是为了后面的 kubeadm init 成功执行！具体的缘由可以参考我之前的文章：<a href="https://www.41sh.cn/?id=8" target="_blank" rel="noopener">https://www.41sh.cn/?id=8</a></p><p>为了检测是否成功关闭，可以执行下面的命令，如果成功执行了，那么下面的命令将不会有任何的输出。</p><pre><code class="bash">$ sudo swapon --summary</code></pre><h3 id="编辑-boot-cmdline-txt"><a href="#编辑-boot-cmdline-txt" class="headerlink" title="编辑 /boot/cmdline.txt"></a>编辑 /boot/cmdline.txt</h3><p>打开 /boot/cmdline.txt，并在末尾添加如下指令。</p><pre><code class="bash">cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code></pre><blockquote><p>该步骤执行完成后，一定要重启！否则后面会有错误。</p></blockquote><h3 id="安装kubernetes"><a href="#安装kubernetes" class="headerlink" title="安装kubernetes"></a>安装kubernetes</h3><p>该步骤也同样依赖于设置网络代理那一步骤！</p><pre><code class="bash">$ sudo su # 切换到root用户$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &amp;&amp; \echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee /etc/apt/sources.list.d/kubernetes.list &amp;&amp; \apt-get update -q &amp;&amp; \apt-get install -qy kubeadm=1.10.2-00 kubectl=1.10.2-00 kubelet=1.10.2-00</code></pre><blockquote class="colorquote warning"><p>1). 这里有个<strong>小坑</strong>，切到root用户后，需要重新配置网络代理，然后就可以使用了。</p><pre><code class="bash">$ <span class="built_in">export</span> http_proxy=<span class="string">"http://172.14.1.54:1080"</span>$ <span class="built_in">export</span> https_proxy=<span class="string">"http://172.14.1.54:1080"</span></code></pre><p>2). 使用root时，不用加sudo前缀，这个已经踩了很多次了</p></blockquote><p>如果安装最新版本，不用指定版本。可以直接使用下面的命令：</p><pre><code class="bash">$ apt-get install -qy kubeadm kubectl kubelet</code></pre><h3 id="修改docker的网络代理"><a href="#修改docker的网络代理" class="headerlink" title="修改docker的网络代理"></a>修改docker的网络代理</h3><p>这一步非常重要！因为我们可能需要pull很多国内不能访问的镜像，因此需要设置docker网络代理，因为默认情况下，docker服务不是通过systemctl管理的，因此需要创建一个系统服务，具体的命令如下：</p><pre><code class="bash">$ mkdir -pv /etc/systemd/system/docker.service.d$ cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=&quot;HTTP_PROXY=http://172.16.3.12:1080/&quot; &quot;HTTPS_PROXY=http://172.16.3.12:1080/&quot;EOF $ systemctl daemon-reload$ systemctl restart docker</code></pre><blockquote><p><strong>参考：</strong><a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/</a></p></blockquote><h3 id="预先pull镜像"><a href="#预先pull镜像" class="headerlink" title="预先pull镜像"></a>预先pull镜像</h3><pre><code class="bash">$ kubeadm config images pull -v3</code></pre><h3 id="初始化master节点"><a href="#初始化master节点" class="headerlink" title="初始化master节点"></a>初始化master节点</h3><pre><code class="bash">$ kubeadm init --token-ttl=0 --pod-network-cidr=10.244.0.0/16</code></pre><p>这步如果成功的话，会打印下面的消息： </p><pre><code class="bash">Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:  kubeadm join 172.16.3.1:6443 --token xo78oj.02cia85vdh285aqj --discovery-token-ca-cert-hash sha256:9517c72036f8261ac912adaf8339b65583fdaa7dbb8dd60054c6e84e8880a3fd</code></pre><p> 然后根据输出的提示，执行下面的语句：</p><pre><code class="bash">$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><blockquote><p>上面的语句需要在pi用户下执行！否则不管用！</p></blockquote><p>这步完成之后，执行下面的命令，你会发现master节点NotReady的消息，这是因为我们的网络需要依赖于flannel网络组件！</p><blockquote class="colorquote info"><h2 id="Setup-networking-with-Weave-Net-or-Flannel"><a href="#Setup-networking-with-Weave-Net-or-Flannel" class="headerlink" title="Setup networking with Weave Net or Flannel"></a>Setup networking with Weave Net or Flannel</h2><p>Some users have reported stability issues with Weave Net on ARMHF. These issues do not appear to affect x86_64 (regular PCs/VMs). You may want to try Flannel instead of Weave Net for your RPi cluster.</p><h3 id="Weave-Net"><a href="#Weave-Net" class="headerlink" title="Weave Net"></a>Weave Net</h3><p>Install <a href="https://www.weave.works/oss/net/" target="_blank" rel="noopener">Weave Net</a> network driver</p><pre><code class="bash">$ kubectl apply -f <span class="string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="variable">$(kubectl version | base64 | tr -d '\n')</span>"</span></code></pre><p>If you run into any issues with Weaveworks’ networking then <a href="https://github.com/coreos/flannel" target="_blank" rel="noopener">flannel</a> is also a popular choice for the ARM platform.</p><h3 id="Flannel-alternative"><a href="#Flannel-alternative" class="headerlink" title="Flannel (alternative)"></a>Flannel (alternative)</h3><p>Apply the Flannel driver on the master:</p><pre><code class="bash">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/c5d10c8/Documentation/kube-flannel.yml</code></pre><p>On each node that joins including the master:</p><pre><code class="bash">$ sudo sysctl net.bridge.bridge-nf-call-iptables=1</code></pre></blockquote><h2 id="其他节点设置"><a href="#其他节点设置" class="headerlink" title="其他节点设置"></a>其他节点设置</h2><p>其他节点和master类似，最后需要执行下面的命令就可以了！</p><pre><code class="bash">$ kubeadm join 172.16.3.1:6443 --token xo78oj.02cia85vdh285aqj --discovery-token-ca-cert-hash sha256:9517c72036f8261ac912adaf8339b65583fdaa7dbb8dd60054c6e84e8880a3fd</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://www.41sh.cn/zb_users/upload/2019/01/201901161547625729333503.jpg&quot; alt=&quot;IMG_0468.jpg&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;最近学了Kubernetes一段时间后，突然想在自己的树莓派上玩玩，搭建一个集群出来，玩过树莓派的同学都知道，树莓派作为一个“卡片电脑式”的嵌入式电脑，它的性能是非常有限的，内存和CPU对于我们传统的机器智能是远远不可及的，因此我在这里PO出我为什么要做树莓派的kubernetes研究：&lt;/p&gt;
    
    </summary>
    
      <category term="RPI Kubernetes" scheme="https://readailib.com/categories/RPI-Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="RaspberryPi" scheme="https://readailib.com/tags/RaspberryPi/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes学习笔记|利用 Kubeflow 管理 Tensorflow 程序</title>
    <link href="https://readailib.com/2018/11/23/kubernetes/kubeflow-deployment/"/>
    <id>https://readailib.com/2018/11/23/kubernetes/kubeflow-deployment/</id>
    <published>2018-11-22T16:00:00.000Z</published>
    <updated>2019-03-12T07:51:47.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://kairen.github.io/2018/03/15/tensorflow/kubeflow/" target="_blank" rel="noopener">https://kairen.github.io/2018/03/15/tensorflow/kubeflow/</a></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>每个人是不是都想拥有一个属于自己的 “人工智能实验室”，他有哪些特点呢？主要包括以下几个特点：</p><ol><li>支持 tensorflow/torch/keras等等主流深度学习库</li><li>用户界面友好</li><li>支持分布式训练</li><li>支持 GPU 训练</li><li>支持快速产生人工智能产品原型</li></ol><p>Kubeflow 是 Google 开源的机器学习工具，目的是简化 Kubernetes 上运行机器学习的工程，使之更简单、可携带与可扩展。Kubeflow 目标不是在于重建其他服务，而是提供一个最佳的开发系统用于部署到各种基础设施架构中，另外由于使用kubernetes 来作为基础，因此只要有 Kubernetes 的地方，都能夠执行 Kubeflow。</p><p>该工具可以用来建立如下功能：</p><ol><li>用于建立与管理工具（IDE） Jupyter notebook 的 JupyterHub。</li><li>可以使用 CPU 或 GPU，并可以通过单一设定来调整单个数据集的 Tensorflow Training Controller。</li><li>用 TensorFlow Serving 容器來提供模型服务。</li></ol><p>Kubeflow 目标是通过 Kubernetes 的特性使机器学习更加简单和快捷：</p><ol><li>在不同 IaaS 上实现简单可重复的携带性部署(Laptop <-> ML rig <-> Training cluster <-> Production cluster)。</-></-></-></li><li>管理与部署松耦合的微服务。</li><li>可根据需要来自动扩缩容</li></ol><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>本次安装基于 1.12.1 版本的 kube*，我的安装比较简单粗暴！适合有运维经验的同学，如果有疑问可以联系我或者访问上面给出的链接?。</p><p>本次安装假定你的集群已经运行，且稳定！</p><blockquote><p>注意：这里只给出 CPU 版本</p></blockquote><p>安装 ksonnet 0.9.2，请参考以下：</p><pre><code class="bash">$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.9.2/ks_0.9.2_linux_amd64.tar.gz$ tar xvf ks_0.9.2_linux_amd64.tar.gz$ sudo cp ks_0.9.2_linux_amd64/ks /usr/local/bin/$ ks versionksonnet version: 0.9.2jsonnet version: v0.9.5client-go version: 1.8</code></pre><h2 id="部署-Kubeflow"><a href="#部署-Kubeflow" class="headerlink" title="部署 Kubeflow"></a>部署 Kubeflow</h2><p>本节將说明如歌利用 ksonnet 来部署 Kubeflow 到 Kubernetes 集群中。首先在 master 节点初始化 ksonnet 应用程序目录：</p><pre><code class="bash">$ ks init my-kubeflow</code></pre><blockquote class="colorquote warning"><p>如果遇到以下问题的話，可以自己建立 GitHub Token 来存取 GitHub API，请参考 <a href="https://ksonnet.io/docs/tutorial#troubleshooting-github-rate-limiting-errors" target="_blank" rel="noopener">Github rate limiting errors</a>。</p><pre><code class="bash">ERROR GET https://api.github.com/repos/ksonnet/parts/commits/master: 403 API rate <span class="built_in">limit</span> exceeded <span class="keyword">for</span> 122.146.93.152.</code></pre></blockquote><p>接着安装 Kubeflow 套件到上面创建的应用程序目录下：</p><pre><code class="bash">$ cd my-kubeflow$ ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow$ VERSION=v0.1.2$ ks pkg install kubeflow/core@${VERSION}$ ks pkg install kubeflow/tf-serving@${VERSION}$ ks pkg install kubeflow/tf-job@${VERSION}</code></pre><p>然后建立 Kubeflow 核心组件，该组件包含 JupyterHub 和 TensorFlow job controller：</p><pre><code class="bash">$ kubectl create namespace kubeflow$ kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator$ ks generate core kubeflow-core --name=kubeflow-core --namespace=kubeflow$ ks env add kubeflow$ ks env set kubeflow --namespace kubeflow$ ks apply kubeflow -c kubeflow-core$ PODNAME=`kubectl get pods --namespace=kubeflow --selector=&quot;app=tf-hub&quot; --output=template --template=&quot;{{with index .items 0}}{{.metadata.name}}{{end}}&quot;`</code></pre><p>这样就可以访问了！如果想要在集群外部访问，修改 <code>tf-hub-lb service</code> 的类型为 <code>NodePort</code> 或者 <code>LoadBalancer</code> 即可！</p><pre><code class="bash">$ kubectl get service tf-hub-lb  -nkubeflowNAME        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEtf-hub-lb   LoadBalancer   10.109.167.223   172.16.3.4    8000:30962/TCP   34m</code></pre><p><img src="https://shenheng.xyz/img/kubeflow.png" alt="img"></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p><img src="https://shenheng.xyz/img/kubeflow-test.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参考&quot;&gt;&lt;a href=&quot;#参考&quot; class=&quot;headerlink&quot; title=&quot;参考&quot;&gt;&lt;/a&gt;参考&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://kairen.github.io/2018/03/15/tensorflow/kubeflow
      
    
    </summary>
    
      <category term="kubeflow Installation" scheme="https://readailib.com/categories/kubeflow-Installation/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
      <category term="Deep Learning" scheme="https://readailib.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>K8s集群部署</title>
    <link href="https://readailib.com/2018/11/15/kubernetes/kubernetes-install-note/"/>
    <id>https://readailib.com/2018/11/15/kubernetes/kubernetes-install-note/</id>
    <published>2018-11-15T08:41:31.000Z</published>
    <updated>2019-03-13T10:59:08.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="重点参考"><a href="#重点参考" class="headerlink" title="重点参考"></a>重点参考</h2><ul><li><a href="http://blog.51cto.com/douya/1945382" target="_blank" rel="noopener">http://blog.51cto.com/douya/1945382</a></li><li><a href="http://www.cnblogs.com/burningTheStar/p/7865998.html" target="_blank" rel="noopener">http://www.cnblogs.com/burningTheStar/p/7865998.html</a></li></ul><h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>采用 CentOS7.4 minimual，docker 1.13，kubeadm 1.10.0，etcd 3.0， k8s 1.10.0 我们这里选用两个个节点搭建一个实验环境。</p><pre><code class="bash">192.168.59.133 k8smaster192.168.59.150 k8snode1</code></pre><p>下面开始准备环境。</p><ol><li><p>配置好各节点hosts文件</p><pre><code class="bash">$ cat&gt;&gt; /etc/hosts  &lt;&lt; EOF192.168.59.133 k8smaster192.168.59.150 k8snode1EOF</code></pre></li><li><p>关闭系统防火墙</p><pre><code class="bash">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld</code></pre></li><li><p>关闭SElinux</p><pre><code class="bash">$ setenforce 0$ sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config</code></pre><p>重启生效！</p></li><li><p>关闭swap</p><pre><code class="bash">$ swapoff -a</code></pre></li><li><p>配置系统内核参数</p><p>使流过网桥的流量也进入iptables/netfilter框架中，在/etc/sysctl.conf中添加以下配置：</p><pre><code class="bash">net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1</code></pre><p>然后执行下面的命令：</p><pre><code class="bash"> $ sysctl -p</code></pre></li></ol><h2 id="使用kubeadm安装"><a href="#使用kubeadm安装" class="headerlink" title="使用kubeadm安装"></a>使用kubeadm安装</h2><ol><li><p>首先配置阿里K8S YUM源</p><pre><code class="bash">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0EOF$ yum -y install epel-release$ yum clean all$ yum makecache</code></pre></li><li><p>安装kubeadm和相关工具包</p><pre><code class="bash">$ yum -y install docker kubelet kubeadm kubectl kubernetes-cni</code></pre></li><li><p>启动Docker与kubelet服务</p><pre><code class="bash">$ systemctl enable docker &amp;&amp; systemctl start docker$ systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre><blockquote><p>提示：此时kubelet的服务运行状态是异常的，因为缺少主配置文件kubelet.conf。但可以暂不处理，因为在完成Master节点的初始化后才会生成这个配置文件。</p></blockquote></li><li><p>下载K8S相关镜像(可以直接看5)</p><p>因为无法直接访问gcr.io下载镜像，所以需要配置一个国内的容器镜像加速器 配置一个阿里云的加速器：（可省略)</p><p>登录 <a href="https://cr.console.aliyun.com/" target="_blank" rel="noopener">https://cr.console.aliyun.com/</a></p><p>在页面中找到并点击镜像加速按钮，即可看到属于自己的专属加速链接，选择Centos版本后即可看到配置方法。</p><p>提示： 在阿里云上使用 Docker 并配置阿里云镜像加速器，可能会遇到 daemon.json 导致 docker daemon 无法启动的问题，可以通过以下方法解决。</p><p>你需要的是编辑 </p><pre><code class="bash">$ vim /etc/sysconfig/docker</code></pre><p>然后</p><pre><code class="bash">OPTIONS=&#39;--selinux-enabled --log-driver=journald --registry-mirror=http://xxxx.mirror.aliyuncs.com&#39;</code></pre><p><code>registry-mirror</code> 输入你的镜像地址 </p><p>最后 </p><pre><code class="bash">$ service docker restart</code></pre><p>重启 daemon. 然后 </p><pre><code class="bash">$ ps aux |grep docker</code></pre><p>然后你就会发现带有镜像的启动参数了。</p></li><li><p>下载K8S相关镜像</p><p>OK，解决完加速器的问题之后，开始下载k8s相关镜像，下载后将镜像名改为k8s.gcr.io/开头的名字，以便kubeadm识别使用。</p><pre><code class="bash">#!/bin/bashimages=(kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0etcd-amd64:3.1.12 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8k8s-dns-dnsmasq-nanny-amd64:1.14.8)for imageName in ${images[@]} ; do  docker pull keveon/$imageName  docker tag keveon/$imageName k8s.gcr.io/$imageName  docker rmi keveon/$imageNamedone</code></pre><p>上面的shell脚本主要做了3件事，下载各种需要用到的容器镜像、重新打标记为符合k8s命令规范的版本名称、清除旧的容器镜像。</p><blockquote><p>提示：镜像版本一定要和kubeadm安装的版本一致，否则会出现time out问题。</p></blockquote></li><li><p>初始化安装K8S Master</p><p>执行上述shell脚本，等待下载完成后，执行<code>kubeadm init</code>.</p><pre><code class="bash">$ kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version 1.10.0··············································································kubeadm join 192.168.59.133:6443 --token 102952.1a7dd4cc8d1f4cc5 --discovery-token-ca-cert-hash sha256:6f1a864c6f530351f9ec7a42f74404497fcbe91ad7bf726bffd8cb3e3c333a38</code></pre><p>最后会出现这个信息，在这个信息会非常有用</p><pre><code class="bash">$ kubeadm join 192.168.59.133:6443 --token 102952.1a7dd4cc8d1f4cc5 --discovery-token-ca-cert-hash sha256:6f1a864c6f530351f9ec7a42f74404497fcbe91ad7bf726bffd8cb3e3c333a38</code></pre><blockquote><p>提示：选项–kubernetes-version=v1.10.0是必须的，否则会因为访问google网站被墙而无法执行命令。这里使用v1.10.0版本，刚才前面也说到了下载的容器镜像版本必须与K8S版本一致否则会出现time out。</p></blockquote><p>上面的命令大约需要1分钟的过程，期间可以观察下tail -f /var/log/message日志文件的输出，掌握该配置过程和进度。上面最后一段的输出信息保存一份，后续添加工作节点还要用到。</p></li><li><p>配置kubectl认证信息 # 对于非root用户</p><pre><code class="bash">$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><p># 对于root用户</p><pre><code class="bash">$ export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre><p>也可以直接放到~/.bash_profile</p><pre><code class="bash">$ echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile</code></pre></li><li><p>安装flannel网络</p><pre><code class="bash">$ mkdir -p /etc/cni/net.d/$ cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf{“name”: “cbr0”,“type”: “flannel”,“delegate”: {“isDefaultGateway”: true}}EOF$ mkdir /usr/share/oci-umount/oci-umount.d -p$ mkdir /run/flannel/$ cat &lt;&lt;EOF&gt; /run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.1.0/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOFkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml</code></pre></li><li><p>让node1加入集群</p><p>在node1节点上分别执行kubeadm join命令，加入集群：</p><pre><code class="bash">$ kubeadm join 192.168.59.133:6443 --token 102952.1a7dd4cc8d1f4cc5 --discovery-token-ca-cert-hash sha256:6f1a864c6f530351f9ec7a42f74404497fcbe91ad7bf726bffd8cb3e3c333a38[preflight] Running pre-flight checks.    [WARNING Service-Kubelet]: kubelet service is not enabled, please run &#39;systemctl enable kubelet.service&#39;    [WARNING FileExisting-crictl]: crictl not found in system pathSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl[discovery] Trying to connect to API Server &quot;10.0.100.202:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://10.0.100.202:6443&quot;[discovery] Requesting info from &quot;https://10.0.100.202:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;10.0.100.202:6443&quot;[discovery] Successfully established connection with API Server &quot;10.0.100.202:6443&quot;This node has joined the cluster:* Certificate signing request was sent to master and a response  was received.* The Kubelet was informed of the new secure connection details.Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code></pre><blockquote><p>提示：细心的童鞋应该会发现，这段命令其实就是前面K8S Matser安装成功后我让你们保存的那段命令。</p></blockquote><p>默认情况下，Master节点不参与工作负载，但如果希望安装出一个All-In-One的k8s环境，则可以执行以下命令，让Master节点也成为一个Node节点：</p><pre><code class="bash">$ kubectl taint nodes --all node-role.kubernetes.io/master-</code></pre></li><li><p>验证K8S Master是否搭建成功</p><pre><code class="bash"># 查看节点状态kubectl get nodes# 查看pods状态kubectl get pods --all-namespaces# 查看K8S集群状态kubectl get cs</code></pre></li><li><p>安装 dashboard</p><p>具体参考：<a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard</a></p><pre><code class="bash">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</code></pre></li></ol><p><img src="https://shenheng.xyz/img/k8-1.png" alt="img"></p><h2 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h2><ol><li><p>如果出现NotReady，说明网络问题，需要创建一个网络服务，在这里需要先下载一个文件：</p></li><li><p>到 <a href="https://github.com/weaveworks/weave/releases" target="_blank" rel="noopener">https://github.com/weaveworks/weave/releases</a> 下载</p><pre><code class="bash">  $ wget https://github.com/weaveworks/weave/releases/download/v2.3.0/weave-daemonset-k8s-1.7.yaml</code></pre><p>  然后执行</p><pre><code class="bash">  $ kubectl create -f weave-daemonset-k8s-1.7.yaml</code></pre><p>  现在可以了！?</p><ul><li>或者使用flannel网络！</li></ul></li><li><p>安装时候最常见的就是time out，因为K8S镜像在国外，所以我们在前面就说到了提前把他下载下来，可以用一个国外机器采用habor搭建一个私有仓库把镜像都download下来。</p><pre><code>[root@k8smaster ~]# kubeadm init[init] Using Kubernetes version: v1.10.0[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks.    [WARNING Service-Kubelet]: kubelet service is not enabled, please run &#39;systemctl enable kubelet.service&#39;    [WARNING FileExisting-crictl]: crictl not found in system pathSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl[preflight] Starting the kubelet service[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8smaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.100.202][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated etcd/ca certificate and key.[certificates] Generated etcd/server certificate and key.[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1][certificates] Generated etcd/peer certificate and key.[certificates] etcd/peer serving cert is signed for DNS names [k8smaster] and IPs [10.0.100.202][certificates] Generated etcd/healthcheck-client certificate and key.[certificates] Generated apiserver-etcd-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;.[init] This might take a minute or longer if the control plane images have to be pulled.Unfortunately, an error has occurred:    timed out waiting for the conditionThis error is likely caused by:    - The kubelet is not running    - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)    - Either there is no internet connection, or imagePullPolicy is set to &quot;Never&quot;,      so the kubelet cannot pull or find the following control plane images:        - k8s.gcr.io/kube-apiserver-amd64:v1.10.0        - k8s.gcr.io/kube-controller-manager-amd64:v1.10.0        - k8s.gcr.io/kube-scheduler-amd64:v1.10.0        - k8s.gcr.io/etcd-amd64:3.1.12 (only if no external etcd endpoints are configured)If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:    - &#39;systemctl status kubelet&#39;    - &#39;journalctl -xeu kubelet&#39;couldn&#39;t initialize a Kubernetes cluster</code></pre><p>那出现这个问题大部分原因是因为安装的K8S版本和依赖的K8S相关镜像版本不符导致的，关于这部分排错可以查看/var/log/message 我们在文章开始安装的时候也提到了要多看日志。 还有些童鞋可能会说，那我安装失败了，怎么清理环境重新安装啊？下面教大家一条命令：</p><pre><code>kubeadm reset</code></pre></li><li><p>我们可能在使用kubeadm 安装完会出现下面的问题：</p><pre><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></pre><p>解决方案如下：</p><pre><code>$ sudo cp /etc/kubernetes/admin.conf $HOME/$ sudo chown $(id -u):$(id -g) $HOME/admin.conf$ export KUBECONFIG=$HOME/admin.conf</code></pre></li><li><p>访问 apiserver 或者 dashboard 出现 <code>&quot;&lt;h3&gt;Unauthorized&lt;/h3&gt;&quot;</code></p><p>可以查看当前的是否存在 <code>kubectl proxy</code> 进程</p><pre><code>$ ps -aux | grep &#39;kubectl proxy&#39;</code></pre><p>如果有，杀死该进程，执行下面的命令：</p><pre><code>$ kubectl proxy --address 0.0.0.0 --accept-hosts &#39;.*&#39;</code></pre></li><li><p>可能在初始化集群出现下面的错误</p><pre><code>[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1    [ERROR Swap]: running with swap on is not supported. Please disable swap[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</code></pre><p>处理方法：</p><pre><code>$ echo &quot;1&quot; &gt;/proc/sys/net/bridge/bridge-nf-call-iptables</code></pre></li><li><p>可能出现 swap 不支持错误</p><pre><code>[ERROR Swap]: running with swap on is not supported. Please disable swap[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</code></pre><p>关闭它：</p><pre><code>$ swapoff -a</code></pre></li><li><p>在初始化集群的时候，可能会出现下面的错误：</p><pre><code class="bash">[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</code></pre><p>很容易看出 <code>kubelet</code> 异常，<strong>centos</strong> 的解决方案如下：</p><blockquote class="colorquote info"><p>On CentOS I can add these options in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>:</p><pre><code class="bash"><span class="comment"># egrep KUBELET_CGROUP_ARGS= /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span>Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"</span></code></pre></blockquote><p>然后重启 <code>kubelet</code> 服务即可。</p><pre><code class="bash">$ systemctl daemon-reload$ systemctl restart kubelet</code></pre></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;重点参考&quot;&gt;&lt;a href=&quot;#重点参考&quot; class=&quot;headerlink&quot; title=&quot;重点参考&quot;&gt;&lt;/a&gt;重点参考&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.51cto.com/douya/1945382&quot; target=&quot;_b
      
    
    </summary>
    
      <category term="Kubernetes Installation" scheme="https://readailib.com/categories/Kubernetes-Installation/"/>
    
    
      <category term="Docker" scheme="https://readailib.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://readailib.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Spark如何工作的</title>
    <link href="https://readailib.com/2018/03/13/spark/how-spark-works/"/>
    <id>https://readailib.com/2018/03/13/spark/how-spark-works/</id>
    <published>2018-03-13T07:43:20.000Z</published>
    <updated>2019-03-13T08:09:47.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="快速认识-Spark"><a href="#快速认识-Spark" class="headerlink" title="快速认识 Spark"></a>快速认识 Spark</h2><ul><li>Spark 往往被看作是 Apache MapReduce 的替代版本，比传统的 MapReduce 范式要更加的方便和高效;</li><li>Spark 不需要非得和 Apache Hadoop 一起使用，虽然在实践中常常这样;<ul><li>因为Spark自身已经继承了Hadoop的一些API、设计和支持来自其他现有的计算框架的数据格式，比如 DryadLINQ;</li></ul></li><li>在处理故障方面，内部的实现以及工作原理和传统的系统是不一样的;</li><li>Spark 在内存计算方面中采用了延迟执行（lazy evaluation）;</li><li>Spark 可以看成快速处理和分析分布式数据的更高级的语言模型.</li></ul><h2 id="快速认识-Microsoft-Dryad"><a href="#快速认识-Microsoft-Dryad" class="headerlink" title="快速认识 Microsoft Dryad"></a>快速认识 Microsoft Dryad</h2><ul><li>Dryad是对MapReduce模型的一种扩展<ul><li>组成单元不仅是Map和Reduce，可以是多种节点</li><li>节点之间形成一个有向无环图DAG(Directed Acyclic Graph) ，以表达所需要的计算</li><li>节点之间的数据传输模式更加多样<ul><li>可以是类似Map/Reduce中的shuffle</li><li>也可以是直接1:1、1:多、多:1传输</li></ul></li><li>比MapReduce更加灵活，但也更复杂<ul><li>需要程序员规定计算的DAG</li></ul></li></ul></li></ul><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ul><li>Spark 的设计原则</li><li>Spark 程序的执行方式</li><li>Spark 的并行计算模型 </li><li>Spark 的调度和执行引擎（ Schedual 和 Executor ） </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;快速认识-Spark&quot;&gt;&lt;a href=&quot;#快速认识-Spark&quot; class=&quot;headerlink&quot; title=&quot;快速认识 Spark&quot;&gt;&lt;/a&gt;快速认识 Spark&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Spark 往往被看作是 Apache MapReduce 的替
      
    
    </summary>
    
    
  </entry>
  
</feed>
