<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/images/favicon.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          TensorFlow 基本使用與分散式概念 - ShenHengheng&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://readailib.com/2017/04/10/tensorflow/intro/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/beantech.min.css">

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('undefined')
            /*post*/
        
    }
    
</style>

<header class="intro-header">
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#TensorFlow" title="TensorFlow">TensorFlow</a>
                            
                              <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                            
                              <a class="tag" href="/tags/#Ubuntu" title="Ubuntu">Ubuntu</a>
                            
                        </div>
                        <h1>TensorFlow 基本使用與分散式概念</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by ShenHengheng on
                            2017-04-10
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>


    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">ShenH.&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About Me</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">tags</a>
                        </li>
                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。</p>
<a id="more"></a>
<p>TensorFlow 其實在意思上是要用兩個部分來解釋，Tensor 與 Flow：</p>
<ul>
<li><strong>Tensor</strong>：是中文翻譯是<code>張量</code>，其實就是一個<code>n</code>維度的陣列或列表。如一維 Tensor 就是向量，二維 Tensor 就是矩陣等等.</li>
<li><strong>Flow</strong>：是指 Graph 運算過程中的資料流.</li>
</ul>
<p><img src="/images/tf/tf-logo.png" alt></p>
<h2 id="Data-Flow-Graphs"><a href="#Data-Flow-Graphs" class="headerlink" title="Data Flow Graphs"></a>Data Flow Graphs</h2><p>資料流圖(Data Flow Graphs)是一種有向圖的節點(Node)與邊(Edge)來描述計算過程。圖中的節點表示數學操作，亦表示資料 I/O 端點; 而邊則表示節點之間的關析，用來傳遞操作之間互相使用的多維陣列(Tensors)，而 Tensor 是在圖中流動的資料表示。一旦節點相連的邊傳來資料流，這時節點就會被分配到運算裝置上異步(節點之間)或同步(節點之內)的執行。</p>
<p><img src="https://www.tensorflow.org/images/tensors_flowing.gif" alt></p>
<h2 id="TensorFlow-基本使用"><a href="#TensorFlow-基本使用" class="headerlink" title="TensorFlow 基本使用"></a>TensorFlow 基本使用</h2><p>在開始進行 TensorFlow 之前，需要了解幾個觀念：</p>
<ul>
<li>使用 <a href="https://www.tensorflow.org/api_docs/python/tf/Graph" target="_blank" rel="noopener">tf.Graph</a> 來表示計算任務.</li>
<li>採用<code>tensorflow::Session</code>的上下文(Context)來執行圖.</li>
<li>以 Tensor 來表示所有資料，可看成擁有靜態資料類型，但有動態大小的多維陣列與列表，如 Boolean 或 String 轉成數值類型.</li>
<li>透過<code>tf.Variable</code>來維護狀態.</li>
<li>透過 feed 與 fetch 來任意操作(Arbitrary operation)給予值或從中取得資料.</li>
</ul>
<p>TensorFlow 的圖中的節點被稱為 <a href="https://www.tensorflow.org/api_docs/python/tf/Operation" target="_blank" rel="noopener">op(operation)</a>。一個<code>op</code>會有 0 至多個 Tensor，而每個 Tensor 是一種類別化的多維陣列，例如把一個圖集合表示成四維浮點陣列，分別為<code>[batch, height, width, channels]</code>。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2630831-5da81623d4661886.jpg?imageMogr2/auto-orient/strip" alt></p>
<p>利用三種不同稱呼來描述 Tensor 的維度，Shape、Rank 與 Dimension。可參考 <a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="noopener">Rank, Shape, 和 Type</a>。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2630831-3625a021343b5da3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<p>一般只有 shape 能夠直接被 print，而 Tensor 則需要 Session 來提供，一般需要三個操作步驟：</p>
<ol>
<li>建立 Tensor.</li>
<li>新增 op.</li>
<li>建立 Session(包含一個 Graph)來執行運算.</li>
</ol>
<p>以下是一個簡單範例，說明如何建立運算：</p>
<pre><code class="py"># coding=utf-8
import tensorflow as tf

a = tf.constant(1)
b = tf.constant(2)
c = tf.constant(3)
d = tf.constant(4)
add1 = tf.add(a, b)
mul1 = tf.multiply(b, c)
add2 = tf.add(c, d)
output = tf.add(add1, mul1)

with tf.Session() as sess:
    print sess.run(output)
</code></pre>
<p>執行流程如下圖：<br><img src="https://github.com/lienhua34/notes/raw/master/tensorflow/asserts/graph_compute_flow.jpg?_=5998853" alt></p>
<p>以下是一個簡單範例，說明如何建立多個 Graph：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

logs_path = &#39;./basic_tmp&#39;

# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點
g1 = tf.Graph()
with g1.as_default():
    a = tf.constant([1.5, 6.0])
    b = tf.constant([1.5, 3.2])
    c = a * b

with tf.Graph().as_default() as g2:
    # 建立一個 1x2 矩陣與 2x1 矩陣 op
    m1 = tf.constant([[1., 0., 2.], [-1., 3., 1.]])
    m2 = tf.constant([[3., 1.], [2., 1.], [1., 0.]])
    m3 = tf.matmul(m1, m2) # 矩陣相乘

# 在 session 執行 graph，並進行資料數據操作 `c`。
# 然後指派給 cpu 做運算
with tf.Session(graph=g1) as sess_cpu:
  with tf.device(&quot;/cpu:0&quot;):
      writer = tf.summary.FileWriter(logs_path, graph=g1)
      print(sess_cpu.run(c))

with tf.Session(graph=g2) as sess_gpu:
  with tf.device(&quot;/gpu:0&quot;):
      result = sess_gpu.run(m3)
      print(result)

# 使用 tf.InteractiveSession 方式來印出內容(不會實際執行)
it_sess = tf.InteractiveSession()
x = tf.Variable([1.0, 2.0])
a = tf.constant([3.0, 3.0])

# 使用初始器 initializer op 的 run() 方法初始化 &#39;x&#39;
x.initializer.run()
sub = tf.subtract(x, a)

print sub.eval()
it_sess.close()

</code></pre>
<blockquote>
<ul>
<li>範例來至 <a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage" target="_blank" rel="noopener">Basic Usage</a>。</li>
<li>指定 Device 可以看這邊 <a href="https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/" target="_blank" rel="noopener">Using GPU</a>.</li>
</ul>
</blockquote>
<p>上面範例可以看到建立了一個 Graph 的計算過程<code>c</code>，而當直接執行到<code>c</code>時，並不會真的執行運算，而是在<code>sess</code>會話建立後，並透過<code>sess</code>執行分配給 CPU 或 GPU 之類設備進行運算後，才會回傳一個節點的 Tensor，在 Python 中 Tensor 是一個 Numpy 的 ndarry 物件。</p>
<p>TensorFlow 也可以透過變數來維護 Graph 的執行過程狀態，這邊提供一個簡單的累加器：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 建立一個變數 counter，並初始化為 0
state = tf.Variable(0, name=&quot;counter&quot;)

# 建立一個常數 op 為 1，並用來累加 state
one = tf.constant(1)
new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# 啟動 Graph 前，變數必須先被初始化(init) op
init_op = tf.global_variables_initializer()

# 啟動 Graph 來執行 op
with tf.Session() as sess:
  sess.run(init_op)
  print sess.run(state)
  # 執行 op 並更新 state
  for _ in range(3):
    sess.run(update)
    print sess.run(state)
</code></pre>
<blockquote>
<p>更多細節可以查看 <a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="noopener">Variables</a>。</p>
</blockquote>
<p>另外可以利用 Fetch 方式來一次取得多個節點的 Tensor，範例如下：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.multiply(input1, intermed)

with tf.Session() as sess:
  # 一次取得多個 Tensor
  result = sess.run([mul, intermed])
  print result
</code></pre>
<p>而當我們想要在執行 Session 時，臨時替換 Tensor 內容的話，就可以利用 TensorFlow 內建的 Feed 方法來解決：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.multiply(input1, input2)

with tf.Session() as sess:
  # 透過 feed 來更改 op 內容，這只會在執行時有效
  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})
  print sess.run([output])
</code></pre>
<h2 id="TensorFlow-分散式運算"><a href="#TensorFlow-分散式運算" class="headerlink" title="TensorFlow 分散式運算"></a>TensorFlow 分散式運算</h2><p>本節將以 TensorFlow 分散式深度學習為例。</p>
<h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><p>gRPC(google Remote Procedure Call) 是 Google 開發的基於 HTTP/2 和 Protocol Buffer 3 的 RPC 框架，該框架有各種常見語言的實作，如 C、Java 與 Go 等語言，提供輕鬆跨語言的呼叫。</p>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>說明客戶端(Client)、叢集(Cluster)、工作(Job)、任務(Task)、TensorFlow 伺服器、Master 與 Worker 服務。</p>
<p><img src="http://www.pittnuts.com/wp-content/uploads/2016/08/TFramework.png" alt></p>
<p>如圖所示，幾個流程說明如下：</p>
<ul>
<li>整個系统映射到 TensorFlow 叢集.</li>
<li>參數伺服器映射到一個 Job.</li>
<li>每個模型(Model)副本映射到一個 Job.</li>
<li>每台實體運算節點映射到其 Job 中的 Task.</li>
<li>每個 Task 都有一個 TF Server，並利用 Master 服務來進行溝通與協調工作，而 Worker 服務則透過本地裝置(CPU 或 GPU)進行 TF graph 運算.</li>
</ul>
<p>TensorFlow 叢集裡包含了一個或多個工作(Job)，每個工作又可以拆分成一個或多個任務(Task)，簡單說 Cluster 是 Job 的集合，而 Job 是 Task 的集合。叢集概念主要用在一個特定層次對象，如訓練神經網路、平行操作多台機器等，一個叢集物件可以透過<code>tf.train.ClusterSpec</code>來定義。</p>
<p>如上所述，TensorFlow 的叢集就是一組工作任務，每個任務是一個服務，而服務又分成<code>Master</code>與<code>Worker</code>這兩種，並提供給<code>Client</code>進行操作。</p>
<ul>
<li><strong>Client</strong>：是用於建立 TensorFlow 計算 Graph，並建立與叢集進行互動的<code>tensorflow::Session</code>行程，一般由 Python 或 C++ 實作，單一客戶端可以同時連接多個 TF 伺服器連接，同時也能被多個 TF 伺服器連接.</li>
<li><strong>Master Service</strong>：是一個 RPC 服務行程，用來遠端連線一系列分散式裝置，主要提供<code>tensorflow::Session</code>介面，並負責透過 Worker Service 與工作的任務進行溝通.</li>
<li><strong>Worker Service</strong>：是一個可以使用本地裝置(CPU 或 GPU)對部分 Graph 進行運算的 RPC 邏輯，透過<code>worker_service.proto</code>介面來實作，所有 TensorFlow 伺服器均包含了 Worker Service 邏輯.</li>
</ul>
<blockquote>
<p><strong>TensorFlow 伺服器</strong>是運行<code>tf.train.Server</code>實例的行程，其為叢集一員，並有 Master 與 Worker 之分。</p>
</blockquote>
<p>而 TensorFlow 的工作(Job)可拆成多個相同功能的任務(Task)，這些工作又分成<code>Parameter server</code>與<code>Worker</code>，兩者功能說明如下：</p>
<p><img src="https://img.tipelse.com/uploads/B/6A/B6A07C1923.jpeg" alt></p>
<ul>
<li><strong>Parameter server(ps)</strong>:是分散式系統縮放至工業大小機器學習的問題，它提供工作節點與伺服器節點之間的非同步與零拷貝 key-value 的溝通，並支援資料的一致性模型的分散式儲存。在 TensorFlow 中主要根據梯度更新變數，並儲存於<code>tf.Variable</code>，可理解成只儲存 TF Model 的變數，並存放 Variable 副本.</li>
</ul>
<p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_0.png" alt></p>
<ul>
<li><strong>Worker</strong>:通常稱為計算節點，一般管理無狀態(Stateless)，且執行密集型的 Graph 運算資源，並根據變數運算梯度。存放 Graph 副本.</li>
</ul>
<p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_1.png" alt></p>
<blockquote>
<ul>
<li><a href="http://blog.csdn.net/cyh_24/article/details/50545780" target="_blank" rel="noopener">Parameter Server 詳解</a></li>
</ul>
</blockquote>
<p>一般對於<code>小型規模訓練</code>，這種資料與參數量不多時，可以用一個 CPU 來同時執行兩種任務。而<code>中型規模訓練</code>，資料量較大，但參數量不多時，計算梯度的工作負載較高，而參數更新負載較低，所以計算梯度交給若干個 CPU 或 GPU 去執行，而更新參數則交給一個 CPU 即可。對於<code>大型規模訓練</code>，資料與參數量多時，不僅計算梯度需要部署多個 CPU 或 GPU，連更新參數也要不說到多個 CPU 中。</p>
<p>然而單一節點能夠裝載的 CPU 與 GPU 是有限的，所以在大量訓練時就需要多台機器來提供運算能力的擴展。</p>
<h3 id="分散式變數伺服器-Parameter-Server"><a href="#分散式變數伺服器-Parameter-Server" class="headerlink" title="分散式變數伺服器(Parameter Server)"></a>分散式變數伺服器(Parameter Server)</h3><p>當在較大規模的訓練時，隨著模型的變數越來越多，很可能造成單一節點因為效能問題，而無法負荷模型變數儲存與更新時，這時候就需要將變數分開到不同機器來做儲存與更新。而 TensorFlow 提供了變數伺服器的邏輯實現，並可以用多台機器來組成叢集，類似分散式儲存結構，主要用來解決變數的儲存與更新效能問題。</p>
<h3 id="撰寫分散式程式注意概念"><a href="#撰寫分散式程式注意概念" class="headerlink" title="撰寫分散式程式注意概念"></a>撰寫分散式程式注意概念</h3><p>當我們在寫分散式程式時，需要知道使用的副本與訓練模式。</p>
<p><img src="https://camo.githubusercontent.com/0b7a1232bd3f8861dfbccab568a30591588384dc/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74656e736f72666c6f775f666967757265372e706e67" alt></p>
<h4 id="In-graph-與-Between-graph-副本模式"><a href="#In-graph-與-Between-graph-副本模式" class="headerlink" title="In-graph 與 Between-graph 副本模式"></a>In-graph 與 Between-graph 副本模式</h4><p>下圖顯示兩者差異，而這邊也在進行描述。</p>
<ul>
<li><strong>In-graph</strong>：只有一個 Clinet(主要呼叫<code>tf::Session</code>行程)，並將裡面變數與 op 指定給對應的 Job 完成，因此資料分發只由一個 Client 完成。這種方式設定簡單，其他節點只需要 join 操作，並提供一個 gRPC 位址來等待任務。但是訓練資料只在單一節點，因此要把資料分發到不同機器時，會影響平行訓練效能。可理解成所有 op 都在同一個 Graph 中，伺服器只需要做<code>join()</code>功能.</li>
<li><strong>Between-graph</strong>：多個獨立 Client 建立相同 Graph(包含變數)，並透過<code>tf.train.replica_device_setter</code>將這些參數映射到 ps 上，即訓練的變數儲存在 Parameter Server，而資料不用分發，資料分片(Shards)會存在個計算節點，因此個節點自己算自己的，算完後，把要更新變數告知 Parameter Server 進行更新。適合在 TB 級別的資料量使用，節省大量資料傳輸時間，也是深度學習推薦模式。</li>
</ul>
<h4 id="同步-Synchronous-訓練與非同步-Asynchronous-訓鍊"><a href="#同步-Synchronous-訓練與非同步-Asynchronous-訓鍊" class="headerlink" title="同步(Synchronous)訓練與非同步(Asynchronous)訓鍊"></a>同步(Synchronous)訓練與非同步(Asynchronous)訓鍊</h4><p>TensorFlow 的副本擁有 in-graph 和 between-graph 模式，這兩者都支援了同步與非同步更新。本節將說明同步與非同步兩者的差異為何。</p>
<ul>
<li><strong>Synchronous</strong>：每個 Graph 的副本讀取相同 Parameter 的值，然後平行計算梯度(gradients)，將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的分片計算時間。</li>
</ul>
<blockquote>
<p>可以利用<code>tf.train.SyncReplicasOptimizer</code>來解決這個問題(在 Between-graph 情況下)，而在 In-graph 則將所有梯度平均即可。</p>
</blockquote>
<ul>
<li><strong>Asynchronous</strong>：自己計算完梯度後，就去更新 paramenter，不同副本之前不會進行協調進度，因此計算資源被充分的利用。缺點是 loss 的下降不穩定。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20161114005141032" alt></p>
<p>一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。</p>
<h3 id="簡單分散式訓練程式"><a href="#簡單分散式訓練程式" class="headerlink" title="簡單分散式訓練程式"></a>簡單分散式訓練程式</h3><p>TensorFlow 提供建立 Server 函式來進行測試使用，以下是建立一個分散式訓練 Server 程式<code>server.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 定義 Cluster
cluster = tf.train.ClusterSpec({&quot;worker&quot;: [&quot;localhost:2222&quot;]})

# 建立 Worker server
server = tf.train.Server(cluster,job_name=&quot;worker&quot;,task_index=0)
server.join()
</code></pre>
<blockquote>
<p>也可以透過<code>tf.train.Server.create_local_server()</code> 來建立 Local Server</p>
</blockquote>
<p>當確認程式沒有任何問題後，就可以透過以下方式啟動：</p>
<pre><code class="bash">$ python server.py
2017-04-10 18:19:41.953448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0)
2017-04-10 18:19:41.983913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -&gt; {0 -&gt; localhost:2222}
2017-04-10 18:19:41.984946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222
</code></pre>
<p>接著我們要撰寫 Client 端來進行定義 Graph 運算的程式<code>client.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 執行目標 Session
server_target = &quot;grpc://localhost:2222&quot;
logs_path = &#39;./basic_tmp&#39;

# 指定 worker task 0 使用 CPU 運算
with tf.device(&quot;/job:worker/task:0&quot;):
    with tf.device(&quot;/cpu:0&quot;):
        a = tf.constant([1.5, 6.0], name=&#39;a&#39;)
        b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)
        c = (a * b) + (a / b)
        d = c * a
        y = tf.assign(b, d)

# 啟動 Session
with tf.Session(server_target) as sess:
    sess.run(tf.global_variables_initializer())
    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())
    print(sess.run(y))
</code></pre>
<p>完成後即可透過以下指令測試：</p>
<pre><code class="python=">$ python client.py
[   4.875       126.45000458]
</code></pre>
<h3 id="線性迴歸訓練程式"><a href="#線性迴歸訓練程式" class="headerlink" title="線性迴歸訓練程式"></a>線性迴歸訓練程式</h3><p>上面範例提供了很簡單的 Client 與 Server 運算操作。而這邊建立一個 Between-graph 執行程式<code>bg_dist.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf
import numpy as np

parameter_servers = [&quot;localhost:2222&quot;]
workers = [&quot;localhost:2223&quot;, &quot;localhost:2224&quot;]

tf.app.flags.DEFINE_string(&quot;job_name&quot;, &quot;&quot;, &quot;輸入 &#39;ps&#39; 或是 &#39;worker&#39;&quot;)
tf.app.flags.DEFINE_integer(&quot;task_index&quot;, 0, &quot;Job 的任務 index&quot;)
FLAGS = tf.app.flags.FLAGS


def main(_):

    cluster = tf.train.ClusterSpec({&quot;ps&quot;: parameter_servers, &quot;worker&quot;: workers})
    server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index)

    if FLAGS.job_name == &quot;ps&quot;:
        server.join()
    elif FLAGS.job_name == &quot;worker&quot;:

        train_X = np.linspace(-1.0, 1.0, 100)
        train_Y = 2.0 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10.0

        X = tf.placeholder(&quot;float&quot;)
        Y = tf.placeholder(&quot;float&quot;)

        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
                worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,
                cluster=cluster)):

            w = tf.Variable(0.0, name=&quot;weight&quot;)
            b = tf.Variable(0.0, name=&quot;bias&quot;)
            # 損失函式，用於描述模型預測值與真實值的差距大小，常見為`均方差(Mean Squared Error)`
            loss = tf.square(Y - tf.multiply(X, w) - b)

            global_step = tf.Variable(0)

            train_op = tf.train.AdagradOptimizer(0.01).minimize(
                loss, global_step=global_step)

            saver = tf.train.Saver()
            summary_op = tf.summary.merge_all()
            init_op = tf.global_variables_initializer()

        # 建立 &quot;Supervisor&quot; 來負責監督訓練過程
        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir=&quot;/tmp/train_logs&quot;,
                                 init_op=init_op,
                                 summary_op=summary_op,
                                 saver=saver,
                                 global_step=global_step,
                                 save_model_secs=600)

        with sv.managed_session(server.target) as sess:
            loss_value = 100
            while not sv.should_stop() and loss_value &gt; 70.0:
                # 執行一個非同步 training 步驟.
                # 若要執行同步可利用`tf.train.SyncReplicasOptimizer` 來進行
                for (x, y) in zip(train_X, train_Y):
                    _, step = sess.run([train_op, global_step],
                                       feed_dict={X: x, Y: y})

                loss_value = sess.run(loss, feed_dict={X: x, Y: y})
                print(&quot;步驟: {}, loss: {}&quot;.format(step, loss_value))

        sv.stop()


if __name__ == &quot;__main__&quot;:
    tf.app.run()
</code></pre>
<blockquote>
<p>若想指定 Device 可以用以下方式：</p>
<pre><code class="python">tf.train.replica_device_setter(ps_tasks=0, ps_device=&#39;/job:ps&#39;, worker_device=&#39;/job:worker&#39;, merge_devices=True, cluster=None, ps_ops=None)
</code></pre>
</blockquote>
<p>撰寫完成後，透過以下指令來進行測試：</p>
<pre><code class="bash">$ python liner_dist.py --job_name=ps --task_index=0
$ python liner_dist.py --job_name=worker --task_index=0
$ python liner_dist.py --job_name=worker --task_index=1
</code></pre>
<h2 id="Tensorboard-視覺化工具"><a href="#Tensorboard-視覺化工具" class="headerlink" title="Tensorboard 視覺化工具"></a>Tensorboard 視覺化工具</h2><p>Tensorboard 是 TensorFlow 內建的視覺化工具，我們可以透過讀取事件紀錄結構化的資料，來顯示以下幾個項目來提供視覺化：</p>
<ul>
<li><strong>Event</strong>：訓練過程中統計資料(平均值等)變化狀態.</li>
<li><strong>Image</strong>：訓練過程中紀錄的 Graph.</li>
<li><strong>Audio</strong>：訓練過程中紀錄的 Audio.</li>
<li><strong>Histogram</strong>：順練過程中紀錄的資料分散圖</li>
</ul>
<p>一個範例程式如下所示：</p>
<pre><code class="python"># coding=utf-8
import tensorflow as tf

logs_path = &#39;./tmp/1&#39;

# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點
g1 = tf.Graph()
with g1.as_default():
    a = tf.constant([1.5, 6.0], name=&#39;a&#39;)
    b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)
    c = (a * b) + (a / b)
    d = c * a
    y = tf.assign(b, d)

# 在 session 執行 graph，並進行資料數據操作 `c`。
# 然後指派給 cpu 做運算
with tf.Session(graph=g1) as sess_cpu:
  with tf.device(&quot;/cpu:0&quot;):
      sess_cpu.run(tf.global_variables_initializer())
      writer = tf.summary.FileWriter(logs_path, graph=g1)
      print(sess_cpu.run(y))

</code></pre>
<p>執行後會看到當前目錄產生<code>tmp_mnist</code> logs 檔案，這時候就可以透過 thensorboard 來視覺化訓練結果：</p>
<pre><code class="bash">$ tensorboard --logdir=run1:./tmp/1 --port=6006
</code></pre>
<blockquote>
<p>run1 是當有多次 log 被載入時做為區別用。</p>
</blockquote>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2017/04/23/container/linuxkit/" data-toggle="tooltip" data-placement="top" title="品嚐 Moby LinuxKit 的 Linux 作業系統">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2017/03/25/kubernetes/helm-quickstart/" data-toggle="tooltip" data-placement="top" title="Kuberentes Helm 介紹">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                    <div class="comment">
                        <div id="disqus_thread" class="disqus-thread"></div>
                    </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Data-Flow-Graphs"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Data Flow Graphs</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#TensorFlow-基本使用"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">TensorFlow 基本使用</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#TensorFlow-分散式運算"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">TensorFlow 分散式運算</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#gRPC"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">gRPC</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#概念"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">概念</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#分散式變數伺服器-Parameter-Server"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">分散式變數伺服器(Parameter Server)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#撰寫分散式程式注意概念"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">撰寫分散式程式注意概念</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#In-graph-與-Between-graph-副本模式"><span class="toc-nav-number">3.4.1.</span> <span class="toc-nav-text">In-graph 與 Between-graph 副本模式</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#同步-Synchronous-訓練與非同步-Asynchronous-訓鍊"><span class="toc-nav-number">3.4.2.</span> <span class="toc-nav-text">同步(Synchronous)訓練與非同步(Asynchronous)訓鍊</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#簡單分散式訓練程式"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">簡單分散式訓練程式</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#線性迴歸訓練程式"><span class="toc-nav-number">3.6.</span> <span class="toc-nav-text">線性迴歸訓練程式</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Tensorboard-視覺化工具"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Tensorboard 視覺化工具</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#TensorFlow" title="TensorFlow">TensorFlow</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                          <a class="tag" href="/tags/#Ubuntu" title="Ubuntu">Ubuntu</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://hwchiu.com" target="_blank">威猛邱牛的部落格</a></li>
                    
                        <li><a href="http://www.evanlin.com" target="_blank">吃草爸爸的部落格</a></li>
                    
                        <li><a href="https://ellis-wu.github.io" target="_blank">跟我一樣可悲的同事部落格</a></li>
                    
                        <li><a href="https://blog.pichuang.com.tw" target="_blank">小飛機的部落格</a></li>
                    
                        <li><a href="https://bestsamina.github.io/" target="_blank">超猛姍蓉的部落格</a></li>
                    
                        <li><a href="https://igene.tw" target="_blank">郭大俠的部落格</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "shenhengheng";
    var disqus_identifier = "https://readailib.com/2017/04/10/tensorflow/intro/";
    var disqus_url = "https://readailib.com/2017/04/10/tensorflow/intro/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                    <li>
                        <a href="/atom.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank" href="https://twitter.com/shenhengheng">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                

                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/shenhengheng">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://github.com/rh01">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/heng960509">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; ShenHengheng 2019
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a>
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span>
                    re-Ported by <a href="http://beantech.org">BeanTech</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=YenYuHsuan&repo=hexo-theme-beantech&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://readailib.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->



<!-- Highlight.js -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://readailib.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
