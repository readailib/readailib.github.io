<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/images/favicon.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          Kubernetes v1.11.x HA 全手動苦工安裝教學(TL;DR) - ShenHengheng&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://readailib.com/2018/07/09/kubernetes/deploy/manual-v1.11/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/beantech.min.css">

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/images/kube/bg.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header">
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Docker" title="Docker">Docker</a>
                            
                              <a class="tag" href="/tags/#Kubernetes" title="Kubernetes">Kubernetes</a>
                            
                              <a class="tag" href="/tags/#Calico" title="Calico">Calico</a>
                            
                        </div>
                        <h1>Kubernetes v1.11.x HA 全手動苦工安裝教學(TL;DR)</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by ShenHengheng on
                            2018-07-09
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>


    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">ShenH.&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About Me</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">tags</a>
                        </li>
                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                        
                    

                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>本篇延續過往<code>手動安裝方式</code>來部署 Kubernetes v1.11.x 版本的 High Availability 叢集，而此次教學將直接透過裸機進行部署 Kubernetes 叢集。以手動安裝的目標是學習 Kubernetes 各元件關析、流程、設定與部署方式。若不想這麼累的話，可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a> 來選擇自己最喜歡的方式。</p>
<p><img src="/images/kube/kubernetes-aa-ha.png" alt></p>
<a id="more"></a>
<h2 id="Kubernetes-部署資訊"><a href="#Kubernetes-部署資訊" class="headerlink" title="Kubernetes 部署資訊"></a>Kubernetes 部署資訊</h2><p>Kubernetes 部署的版本資訊：</p>
<ul>
<li>Kubernetes: v1.11.0</li>
<li>CNI: v0.7.1</li>
<li>Etcd: v3.3.8</li>
<li>Docker: v18.05.0-ce</li>
<li>Calico: v3.1</li>
</ul>
<p>Kubernetes 部署的網路資訊：</p>
<ul>
<li><strong>Cluster IP CIDR</strong>: 10.244.0.0/16</li>
<li><strong>Service Cluster IP CIDR</strong>: 10.96.0.0/12</li>
<li><strong>Service DNS IP</strong>: 10.96.0.10</li>
<li><strong>DNS DN</strong>: cluster.local</li>
<li><strong>Kubernetes API VIP</strong>: 172.22.132.9</li>
<li><strong>Kubernetes Ingress VIP</strong>: 172.22.132.8</li>
</ul>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學採用以下節點數與機器規格進行部署裸機(Bare-metal)，作業系統採用<code>Ubuntu 16+</code>(理論上 CentOS 7+ 也行)進行測試：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Hostname</th>
<th>CPU</th>
<th>Memory</th>
<th>Extra Device</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.10</td>
<td>k8s-m1</td>
<td>4</td>
<td>16G</td>
<td>None</td>
</tr>
<tr>
<td>172.22.132.11</td>
<td>k8s-m2</td>
<td>4</td>
<td>16G</td>
<td>None</td>
</tr>
<tr>
<td>172.22.132.12</td>
<td>k8s-m3</td>
<td>4</td>
<td>16G</td>
<td>None</td>
</tr>
<tr>
<td>172.22.132.13</td>
<td>k8s-g1</td>
<td>4</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
<tr>
<td>172.22.132.14</td>
<td>k8s-g2</td>
<td>4</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
</tbody>
</table>
<p>另外由所有 master 節點提供一組 VIP <code>172.22.132.9</code>。</p>
<blockquote>
<ul>
<li>這邊<code>m</code>為 K8s Master 節點，<code>g</code>為 K8s Node 節點。</li>
<li>所有操作全部用<code>root</code>使用者進行，主要方便部署用。</li>
</ul>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始部署叢集前需先確保以下條件已達成：</p>
<ul>
<li><code>所有節點</code>彼此網路互通，並且<code>k8s-m1</code> SSH 登入其他節點為 passwdless，由於過程中很多會在某台節點(<code>k8s-m1</code>)上以 SSH 複製與操作其他節點。</li>
<li>確認所有防火牆與 SELinux 已關閉。如 CentOS：</li>
</ul>
<pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
$ vim /etc/selinux/config
SELINUX=disabled
</code></pre>
<blockquote>
<p>關閉是為了方便安裝使用，若有需要防火牆可以參考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubeadm/#check-required-ports" target="_blank" rel="noopener">Required ports</a> 來設定。</p>
</blockquote>
<ul>
<li><code>所有節點</code>需要設定<code>/etc/hosts</code>解析到所有叢集主機。</li>
</ul>
<pre><code>...
172.22.132.10 k8s-m1
172.22.132.11 k8s-m2
172.22.132.12 k8s-m3
172.22.132.13 k8s-g1
172.22.132.14 k8s-g2
</code></pre><ul>
<li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li>
</ul>
<pre><code class="sh">$ curl -fsSL https://get.docker.com/ | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<ul>
<li><code>所有節點</code>需要設定以下系統參數。</li>
</ul>
<pre><code class="sh">$ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

$ sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>
<blockquote>
<p>關於<code>bridge-nf-call-iptables</code>的啟用取決於是否將容器連接到<code>Linux bridge</code>或使用其他一些機制(如 SDN vSwitch)。</p>
</blockquote>
<ul>
<li>Kubernetes v1.8+ 要求關閉系統 Swap，請在<code>所有節點</code>利用以下指令關閉：</li>
</ul>
<pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0

# 不同機器會有差異
$ sed &#39;/swap.img/d&#39; -i /etc/fstab
</code></pre>
<blockquote>
<p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p>
</blockquote>
<ul>
<li>在<code>所有節點</code>下載 Kubernetes 二進制執行檔：</li>
</ul>
<pre><code class="sh">$ export KUBE_URL=https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/amd64
$ wget ${KUBE_URL}/kubelet -O /usr/local/bin/kubelet
$ chmod +x /usr/local/bin/kubelet

# Node 可忽略下載 kubectl
$ wget ${KUBE_URL}/kubectl -O /usr/local/bin/kubectl
$ chmod +x /usr/local/bin/kubectl
</code></pre>
<ul>
<li>在<code>所有節點</code>下載 Kubernetes CNI 二進制執行檔：</li>
</ul>
<pre><code class="sh">$ export CNI_URL=https://github.com/containernetworking/plugins/releases/download
$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin
$ wget -qO- --show-progress &quot;${CNI_URL}/v0.7.1/cni-plugins-amd64-v0.7.1.tgz&quot; | tar -zx
</code></pre>
<ul>
<li>在<code>k8s-m1</code>節點安裝<code>cfssl</code>工具，這將會用來建立 CA ，並產生 TLS 憑證。</li>
</ul>
<pre><code class="sh">$ export CFSSL_URL=https://pkg.cfssl.org/R1.2
$ wget ${CFSSL_URL}/cfssl_linux-amd64 -O /usr/local/bin/cfssl
$ wget ${CFSSL_URL}/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
</code></pre>
<h2 id="建立-CA-與產生-TLS-憑證"><a href="#建立-CA-與產生-TLS-憑證" class="headerlink" title="建立 CA 與產生 TLS 憑證"></a>建立 CA 與產生 TLS 憑證</h2><p>本節將會透過 CFSSL 工具來產生不同元件的憑證，如 Etcd、Kubernetes API Server 等等，其中各元件都會有一個根數位憑證認證機構(Root Certificate Authority)被用在元件之間的認證。</p>
<blockquote>
<p>要注意 CA JSON 檔中的<code>CN(Common Name)</code>與<code>O(Organization)</code>等內容是會影響 Kubernetes 元件認證的。</p>
</blockquote>
<p>首先在<code>k8s-m1</code>透過 Git 取得部署用檔案：</p>
<pre><code class="sh">$ git clone https://github.com/kairen/k8s-manual-files.git ~/k8s-manual-files
$ cd ~/k8s-manual-files/pki
</code></pre>
<h3 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h3><p>在<code>k8s-m1</code>建立<code>/etc/etcd/ssl</code>資料夾，並產生 Etcd CA：</p>
<pre><code class="sh">$ export DIR=/etc/etcd/ssl
$ mkdir -p ${DIR}
$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare ${DIR}/etcd-ca
</code></pre>
<p>接著產生 Etcd 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${DIR}/etcd-ca.pem \
  -ca-key=${DIR}/etcd-ca-key.pem \
  -config=ca-config.json \
  -hostname=127.0.0.1,172.22.132.10,172.22.132.11,172.22.132.12 \
  -profile=kubernetes \
  etcd-csr.json | cfssljson -bare ${DIR}/etcd
</code></pre>
<blockquote>
<p><code>-hostname</code>需修改成所有 masters 節點。</p>
</blockquote>
<p>刪除不必要的檔案，並檢查<code>/etc/etcd/ssl</code>目錄是否成功建立以下檔案：</p>
<pre><code class="sh">$ rm -rf ${DIR}/*.csr
$ ls /etc/etcd/ssl
etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem
</code></pre>
<p>複製檔案至其他 Etcd 節點，這邊為所有<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot; mkdir -p /etc/etcd/ssl&quot;
    for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}
    done
  done
</code></pre>
<h3 id="Kubernetes-元件"><a href="#Kubernetes-元件" class="headerlink" title="Kubernetes 元件"></a>Kubernetes 元件</h3><p>在<code>k8s-m1</code>建立<code>/etc/kubernetes/pki</code>資料夾，並依據下面指令來產生 CA：</p>
<pre><code class="sh">$ export K8S_DIR=/etc/kubernetes
$ export PKI_DIR=${K8S_DIR}/pki
$ export KUBE_APISERVER=https://172.22.132.9:6443
$ mkdir -p ${PKI_DIR}
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ${PKI_DIR}/ca
$ ls ${PKI_DIR}/ca*.pem
/etc/kubernetes/pki/ca-key.pem  /etc/kubernetes/pki/ca.pem
</code></pre>
<blockquote>
<p><code>KUBE_APISERVER</code>這邊設定為 VIP 位址。</p>
</blockquote>
<p>接著依照以下小節來建立各元件的 TLS 憑證。</p>
<h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>此憑證將被用於 API Server 與 Kubelet Client 溝通使用。首先透過以下指令產生 Kubernetes API Server 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${PKI_DIR}/ca.pem \
  -ca-key=${PKI_DIR}/ca-key.pem \
  -config=ca-config.json \
  -hostname=10.96.0.1,172.22.132.9,127.0.0.1,kubernetes.default \
  -profile=kubernetes \
  apiserver-csr.json | cfssljson -bare ${PKI_DIR}/apiserver

$ ls ${PKI_DIR}/apiserver*.pem
/etc/kubernetes/pki/apiserver-key.pem  /etc/kubernetes/pki/apiserver.pem
</code></pre>
<blockquote>
<p>這邊<code>-hostname</code>的<code>10.96.0.1</code>是 Cluster IP 的 Kubernetes 端點; <code>172.22.132.9</code>為 VIP 位址; <code>kubernetes.default</code>為 Kubernetes 系統在 default namespace 自動建立的 API service domain name。</p>
</blockquote>
<h4 id="Front-Proxy-Client"><a href="#Front-Proxy-Client" class="headerlink" title="Front Proxy Client"></a>Front Proxy Client</h4><p>此憑證將被用於 Authenticating Proxy 的功能上，而該功能主要是提供 API Aggregation 的認證。首先透過以下指令產生 CA：</p>
<pre><code class="sh">$ cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-ca
$ ls ${PKI_DIR}/front-proxy-ca*.pem
/etc/kubernetes/pki/front-proxy-ca-key.pem  /etc/kubernetes/pki/front-proxy-ca.pem
</code></pre>
<p>接著產生 Front proxy client 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${PKI_DIR}/front-proxy-ca.pem \
  -ca-key=${PKI_DIR}/front-proxy-ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  front-proxy-client-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-client

$ ls ${PKI_DIR}/front-proxy-client*.pem
/etc/kubernetes/pki/front-proxy-client-key.pem  /etc/kubernetes/pki/front-proxy-client.pem
</code></pre>
<h4 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h4><p>憑證會建立<code>system:kube-controller-manager</code>的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的<code>system:kube-controller-manager</code>來讓 Controller Manager 元件能夠存取需要的 API object。這邊透過以下指令產生 Controller Manager 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${PKI_DIR}/ca.pem \
  -ca-key=${PKI_DIR}/ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  manager-csr.json | cfssljson -bare ${PKI_DIR}/controller-manager

$ ls ${PKI_DIR}/controller-manager*.pem
/etc/kubernetes/pki/controller-manager-key.pem  /etc/kubernetes/pki/controller-manager.pem
</code></pre>
<p>接著利用 kubectl 來產生 Controller Manager 的 kubeconfig 檔：</p>
<pre><code class="sh">$ kubectl config set-cluster kubernetes \
    --certificate-authority=${PKI_DIR}/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=${K8S_DIR}/controller-manager.conf

$ kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=${PKI_DIR}/controller-manager.pem \
    --client-key=${PKI_DIR}/controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=${K8S_DIR}/controller-manager.conf

$ kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=${K8S_DIR}/controller-manager.conf

$ kubectl config use-context system:kube-controller-manager@kubernetes \
    --kubeconfig=${K8S_DIR}/controller-manager.conf
</code></pre>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>憑證會建立<code>system:kube-scheduler</code>的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的<code>system:kube-scheduler</code>來讓 Scheduler 元件能夠存取需要的 API object。這邊透過以下指令產生 Scheduler 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${PKI_DIR}/ca.pem \
  -ca-key=${PKI_DIR}/ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  scheduler-csr.json | cfssljson -bare ${PKI_DIR}/scheduler

$ ls ${PKI_DIR}/scheduler*.pem
/etc/kubernetes/pki/scheduler-key.pem  /etc/kubernetes/pki/scheduler.pem
</code></pre>
<p>接著利用 kubectl 來產生 Scheduler 的 kubeconfig 檔：</p>
<pre><code class="sh">$ kubectl config set-cluster kubernetes \
    --certificate-authority=${PKI_DIR}/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=${K8S_DIR}/scheduler.conf

$ kubectl config set-credentials system:kube-scheduler \
    --client-certificate=${PKI_DIR}/scheduler.pem \
    --client-key=${PKI_DIR}/scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=${K8S_DIR}/scheduler.conf

$ kubectl config set-context system:kube-scheduler@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=${K8S_DIR}/scheduler.conf

$ kubectl config use-context system:kube-scheduler@kubernetes \
    --kubeconfig=${K8S_DIR}/scheduler.conf
</code></pre>
<h4 id="Admin"><a href="#Admin" class="headerlink" title="Admin"></a>Admin</h4><p>Admin 被用來綁定 RBAC Cluster Role 中 cluster-admin，當想要操作所有 Kubernetes 叢集功能時，就必須利用這邊產生的 kubeconfig 檔案。這邊透過以下指令產生 Kubernetes Admin 憑證：</p>
<pre><code class="sh">$ cfssl gencert \
  -ca=${PKI_DIR}/ca.pem \
  -ca-key=${PKI_DIR}/ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare ${PKI_DIR}/admin

$ ls ${PKI_DIR}/admin*.pem
/etc/kubernetes/pki/admin-key.pem  /etc/kubernetes/pki/admin.pem
</code></pre>
<p>接著利用 kubectl 來產生 Admin 的 kubeconfig 檔：</p>
<pre><code class="sh">$ kubectl config set-cluster kubernetes \
    --certificate-authority=${PKI_DIR}/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=${K8S_DIR}/admin.conf

$ kubectl config set-credentials kubernetes-admin \
    --client-certificate=${PKI_DIR}/admin.pem \
    --client-key=${PKI_DIR}/admin-key.pem \
    --embed-certs=true \
    --kubeconfig=${K8S_DIR}/admin.conf

$ kubectl config set-context kubernetes-admin@kubernetes \
    --cluster=kubernetes \
    --user=kubernetes-admin \
    --kubeconfig=${K8S_DIR}/admin.conf

$ kubectl config use-context kubernetes-admin@kubernetes \
    --kubeconfig=${K8S_DIR}/admin.conf
</code></pre>
<h4 id="Masters-Kubelet"><a href="#Masters-Kubelet" class="headerlink" title="Masters Kubelet"></a>Masters Kubelet</h4><p>這邊使用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/" target="_blank" rel="noopener">Node authorizer</a> 來讓節點的 kubelet 能夠存取如 services、endpoints 等 API，而使用 Node authorizer 需定義 <code>system:nodes</code> 群組(憑證的 Organization)，並且包含<code>system:node:&lt;nodeName&gt;</code>的使用者名稱(憑證的 Common Name)。</p>
<p>首先在<code>k8s-m1</code>節點產生所有 master 節點的 kubelet 憑證，這邊透過下面腳本來產生：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    cp kubelet-csr.json kubelet-$NODE-csr.json;
    sed -i &quot;s/\$NODE/$NODE/g&quot; kubelet-$NODE-csr.json;
    cfssl gencert \
      -ca=${PKI_DIR}/ca.pem \
      -ca-key=${PKI_DIR}/ca-key.pem \
      -config=ca-config.json \
      -hostname=$NODE \
      -profile=kubernetes \
      kubelet-$NODE-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-$NODE;
    rm kubelet-$NODE-csr.json
  done

$ ls ${PKI_DIR}/kubelet*.pem
/etc/kubernetes/pki/kubelet-k8s-m1-key.pem  /etc/kubernetes/pki/kubelet-k8s-m2.pem
/etc/kubernetes/pki/kubelet-k8s-m1.pem      /etc/kubernetes/pki/kubelet-k8s-m3-key.pem
/etc/kubernetes/pki/kubelet-k8s-m2-key.pem  /etc/kubernetes/pki/kubelet-k8s-m3.pem
</code></pre>
<p>產生完成後，將 kubelet 憑證複製到所有<code>master</code>節點上：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p ${PKI_DIR}&quot;
    scp ${PKI_DIR}/ca.pem ${NODE}:${PKI_DIR}/ca.pem
    scp ${PKI_DIR}/kubelet-$NODE-key.pem ${NODE}:${PKI_DIR}/kubelet-key.pem
    scp ${PKI_DIR}/kubelet-$NODE.pem ${NODE}:${PKI_DIR}/kubelet.pem
    rm ${PKI_DIR}/kubelet-$NODE-key.pem ${PKI_DIR}/kubelet-$NODE.pem
  done
</code></pre>
<p>接著利用 kubectl 來產生 kubelet 的 kubeconfig 檔，這邊透過腳本來產生所有<code>master</code>節點的檔案：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;cd ${PKI_DIR} &amp;&amp; \
      kubectl config set-cluster kubernetes \
        --certificate-authority=${PKI_DIR}/ca.pem \
        --embed-certs=true \
        --server=${KUBE_APISERVER} \
        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \
      kubectl config set-credentials system:node:${NODE} \
        --client-certificate=${PKI_DIR}/kubelet.pem \
        --client-key=${PKI_DIR}/kubelet-key.pem \
        --embed-certs=true \
        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \
      kubectl config set-context system:node:${NODE}@kubernetes \
        --cluster=kubernetes \
        --user=system:node:${NODE} \
        --kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp; \
      kubectl config use-context system:node:${NODE}@kubernetes \
        --kubeconfig=${K8S_DIR}/kubelet.conf&quot;
  done
</code></pre>
<h4 id="Service-Account-Key"><a href="#Service-Account-Key" class="headerlink" title="Service Account Key"></a>Service Account Key</h4><p>Kubernetes Controller Manager 利用 Key pair 來產生與簽署 Service Account 的 tokens，而這邊不透過 CA 做認證，而是建立一組公私鑰來讓 API Server 與 Controller Manager 使用：</p>
<pre><code class="sh">$ openssl genrsa -out ${PKI_DIR}/sa.key 2048
$ openssl rsa -in ${PKI_DIR}/sa.key -pubout -out ${PKI_DIR}/sa.pub
$ ls ${PKI_DIR}/sa.*
/etc/kubernetes/pki/sa.key  /etc/kubernetes/pki/sa.pub
</code></pre>
<h4 id="刪除不必要檔案"><a href="#刪除不必要檔案" class="headerlink" title="刪除不必要檔案"></a>刪除不必要檔案</h4><p>當所有檔案建立與產生完成後，將一些不必要檔案刪除：</p>
<pre><code class="sh">$ rm -rf ${PKI_DIR}/*.csr \
    ${PKI_DIR}/scheduler*.pem \
    ${PKI_DIR}/controller-manager*.pem \
    ${PKI_DIR}/admin*.pem \
    ${PKI_DIR}/kubelet*.pem
</code></pre>
<h4 id="複製檔案至其他節點"><a href="#複製檔案至其他節點" class="headerlink" title="複製檔案至其他節點"></a>複製檔案至其他節點</h4><p>將憑證複製到其他<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    for FILE in $(ls ${PKI_DIR}); do
      scp ${PKI_DIR}/${FILE} ${NODE}:${PKI_DIR}/${FILE}
    done
  done
</code></pre>
<p>複製各元件 kubeconfig 檔案至其他<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    for FILE in admin.conf controller-manager.conf scheduler.conf; do
      scp ${K8S_DIR}/${FILE} ${NODE}:${K8S_DIR}/${FILE}
    done
  done
</code></pre>
<h2 id="Kubernetes-Masters"><a href="#Kubernetes-Masters" class="headerlink" title="Kubernetes Masters"></a>Kubernetes Masters</h2><p>本節將說明如何部署與設定 Kubernetes Master 角色中的各元件，在開始前先簡單了解一下各元件功能：</p>
<ul>
<li><strong>kubelet</strong>：負責管理容器的生命週期，定期從 API Server 取得節點上的預期狀態(如網路、儲存等等配置)資源，並呼叫對應的容器介面(CRI、CNI 等)來達成這個狀態。任何 Kubernetes 節點都會擁有該元件。</li>
<li><strong>kube-apiserver</strong>：以 REST APIs 提供 Kubernetes 資源的 CRUD，如授權、認證、存取控制與 API 註冊等機制。</li>
<li><strong>kube-controller-manager</strong>：透過核心控制循環(Core Control Loop)監聽 Kubernetes API 的資源來維護叢集的狀態，這些資源會被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而這些控制器會處理著自動擴展、滾動更新等等功能。</li>
<li><strong>kube-scheduler</strong>：負責將一個(或多個)容器依據排程策略分配到對應節點上讓容器引擎(如 Docker)執行。而排程受到 QoS 要求、軟硬體約束、親和性(Affinity)等等規範影響。</li>
<li><strong>Etcd</strong>：用來保存叢集所有狀態的 Key/Value 儲存系統，所有 Kubernetes 元件會透過 API Server 來跟 Etcd 進行溝通來保存或取得資源狀態。</li>
<li><strong>HAProxy</strong>：提供多個 API Server 的負載平衡(Load Balance)。</li>
<li><strong>Keepalived</strong>：建立一個虛擬 IP(VIP) 來作為 API Server 統一存取端點。</li>
</ul>
<p>而上述元件除了 kubelet 外，其他將透過 kubelet 以 <a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/" target="_blank" rel="noopener">Static Pod</a> 方式進行部署，這種方式可以減少管理 Systemd 的服務，並且能透過 kubectl 來觀察啟動的容器狀況。</p>
<h3 id="部署與設定"><a href="#部署與設定" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>首先在<code>k8s-m1</code>節點進入<code>k8s-manual-files</code>目錄，並依序執行下述指令來完成部署：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
</code></pre>
<p>首先利用<code>./hack/gen-configs.sh</code>腳本在每台<code>master</code>節點產生組態檔：</p>
<pre><code class="sh">$ export NODES=&quot;k8s-m1 k8s-m2 k8s-m3&quot;
$ ./hack/gen-configs.sh
k8s-m1 config generated...
k8s-m2 config generated...
k8s-m3 config generated...
</code></pre>
<p>完成後記得檢查<code>/etc/etcd/config.yml</code>與<code>/etc/haproxy/haproxy.cfg</code>是否設定正確。</p>
<blockquote>
<p>這邊主要確認檔案中的<code>${xxx}</code>字串是否有被更改，並且符合環境。詳細內容可以查看<code>k8s-manual-files</code>。</p>
</blockquote>
<p>接著利用<code>./hack/gen-manifests.sh</code>腳本在每台<code>master</code>節點產生 Static pod YAML 檔案，以及其他相關設定檔(如 EncryptionConfig)：</p>
<pre><code class="sh">$ export NODES=&quot;k8s-m1 k8s-m2 k8s-m3&quot;
$ ./hack/gen-manifests.sh
k8s-m1 manifests generated...
k8s-m2 manifests generated...
k8s-m3 manifests generated...
</code></pre>
<p>完成後記得檢查<code>/etc/kubernetes/manifests</code>、<code>/etc/kubernetes/encryption</code>與<code>/etc/kubernetes/audit</code>目錄中的檔案是否是定正確。</p>
<blockquote>
<p>這邊主要確認檔案中的<code>${xxx}</code>字串是否有被更改，並且符合環境需求。詳細內容可以查看<code>k8s-manual-files</code>。</p>
</blockquote>
<p>確認上述兩個產生檔案步驟完成後，即可設定所有<code>master</code>節點的 kubelet systemd 來啟動 Kubernetes 元件。首先複製下列檔案到指定路徑：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d&quot;
    scp master/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml
    scp master/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service
    scp master/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf
  done
</code></pre>
<p>接著在<code>k8s-m1</code>透過 SSH 啟動所有<code>master</code>節點的 kubelet：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    ssh ${NODE} &quot;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&quot;
  done
</code></pre>
<p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p>
<pre><code class="sh">$ watch netstat -ntlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      9407/kube-scheduler
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      9338/kube-controlle
tcp        0      0 127.0.0.1:38420         0.0.0.0:*               LISTEN      8676/kubelet
tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      9602/haproxy
tcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      9602/haproxy
tcp6       0      0 :::10250                :::*                    LISTEN      8676/kubelet
tcp6       0      0 :::2379                 :::*                    LISTEN      9487/etcd
tcp6       0      0 :::6443                 :::*                    LISTEN      9133/kube-apiserver
tcp6       0      0 :::2380                 :::*                    LISTEN      9487/etcd
...
</code></pre>
<blockquote>
<p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker</code>指令來查看。</p>
</blockquote>
<p>接下來將建立 TLS Bootstrapping 來讓 Node 簽證並授權註冊到叢集。</p>
<h3 id="建立-TLS-Bootstrapping"><a href="#建立-TLS-Bootstrapping" class="headerlink" title="建立 TLS Bootstrapping"></a>建立 TLS Bootstrapping</h3><p>由於本教學採用 TLS 認證來確保 Kubernetes 叢集的安全性，因此每個節點的 kubelet 都需要透過 API Server 的 CA 進行身份驗證後，才能與 API Server 進行溝通，而這過程過去都是採用手動方式針對每台節點(<code>master</code>與<code>node</code>)單獨簽署憑證，再設定給 kubelet 使用，然而這種方式是一件繁瑣的事情，因為當節點擴展到一定程度時，將會非常費時，甚至延伸初管理不易問題。</p>
<p>而由於上述問題，Kubernetes 實現了 TLS Bootstrapping 來解決此問題，這種做法是先讓 kubelet 以一個低權限使用者(一個能存取 CSR API 的 Token)存取 API Server，接著對 API Server 提出申請憑證簽署請求，並在受理後由 API Server 動態簽署 kubelet 憑證提供給對應的<code>node</code>節點使用。具體作法請參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a> 與 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank" rel="noopener">Authenticating with Bootstrap Tokens</a>。</p>
<p>在<code>k8s-m1</code>建立 bootstrap 使用者的 kubeconfig 檔：</p>
<pre><code class="sh">$ export TOKEN_ID=$(openssl rand 3 -hex)
$ export TOKEN_SECRET=$(openssl rand 8 -hex)
$ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET}
$ export KUBE_APISERVER=&quot;https://172.22.132.9:6443&quot;

$ kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/pki/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf

$ kubectl config set-credentials tls-bootstrap-token-user \
    --token=${BOOTSTRAP_TOKEN} \
    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf

$ kubectl config set-context tls-bootstrap-token-user@kubernetes \
    --cluster=kubernetes \
    --user=tls-bootstrap-token-user \
    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf

$ kubectl config use-context tls-bootstrap-token-user@kubernetes \
    --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
</code></pre>
<blockquote>
<p><code>KUBE_APISERVER</code>這邊設定為 VIP 位址。若想要用手動簽署憑證來進行授權的話，可以參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/" target="_blank" rel="noopener">Certificate</a>。</p>
</blockquote>
<p>接著在<code>k8s-m1</code>建立 TLS Bootstrap Secret 來提供自動簽證使用：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-${TOKEN_ID}
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  token-id: &quot;${TOKEN_ID}&quot;
  token-secret: &quot;${TOKEN_SECRET}&quot;
  usage-bootstrap-authentication: &quot;true&quot;
  usage-bootstrap-signing: &quot;true&quot;
  auth-extra-groups: system:bootstrappers:default-node-token
EOF

secret &quot;bootstrap-token-65a3a9&quot; created
</code></pre>
<p>然後建立 TLS Bootstrap Autoapprove RBAC 來提供自動受理 CSR：</p>
<pre><code class="sh">$ kubectl apply -f master/resources/kubelet-bootstrap-rbac.yml
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
</code></pre>
<h3 id="驗證-Master-節點"><a href="#驗證-Master-節點" class="headerlink" title="驗證 Master 節點"></a>驗證 Master 節點</h3><p>完成後，在任意一台<code>master</code>節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證：</p>
<pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config
$ kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {&quot;health&quot;:&quot;true&quot;}
etcd-1               Healthy   {&quot;health&quot;:&quot;true&quot;}
etcd-2               Healthy   {&quot;health&quot;:&quot;true&quot;}

$ kubectl -n kube-system get po
NAME                             READY     STATUS    RESTARTS   AGE
etcd-k8s-m1                      1/1       Running   0          1h
etcd-k8s-m2                      1/1       Running   0          1h
etcd-k8s-m3                      1/1       Running   0          1h
kube-apiserver-k8s-m1            1/1       Running   0          1h
kube-apiserver-k8s-m2            1/1       Running   0          1h
kube-apiserver-k8s-m3            1/1       Running   0          1h
...

$ kubectl get node
NAME      STATUS     ROLES     AGE       VERSION
k8s-m1    NotReady   master    38s       v1.11.0
k8s-m2    NotReady   master    37s       v1.11.0
k8s-m3    NotReady   master    36s       v1.11.0
</code></pre>
<blockquote>
<p>在這階段狀態處於<code>NotReady</code>是正常，往下進行就會了解為何。</p>
</blockquote>
<p>透過 kubectl logs 來查看容器的日誌：</p>
<pre><code class="sh">$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1
Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-k8s-m1)
</code></pre>
<blockquote>
<p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源存取權限，屬於正常。</p>
</blockquote>
<p>為了方便管理叢集，因此需要透過 kubectl logs 來查看，但由於 API 權限問題，故需要建立一個  RBAC Role 來獲取存取權限，這邊在<code>k8s-m1</code>節點執行以下指令建立：</p>
<pre><code class="sh">$ kubectl apply -f master/resources/apiserver-to-kubelet-rbac.yml
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
</code></pre>
<p>完成後，再次透過 kubectl logs 查看 Pod：</p>
<pre><code class="sh">$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1
I0708 15:22:33.906269       1 get.go:245] Starting watch for /api/v1/services, rv=2494 labels= fields= timeout=8m29s
I0708 15:22:40.919638       1 get.go:245] Starting watch for /apis/certificates.k8s.io/v1beta1/certificatesigningrequests, rv=11084 labels= fields= timeout=7m29s
...
</code></pre>
<p>接著設定 <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">Taints and Tolerations</a> 來讓一些特定 Pod 能夠排程到所有<code>master</code>節點上：</p>
<pre><code class="sh">$ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --all
node &quot;k8s-m1&quot; tainted
node &quot;k8s-m2&quot; tainted
node &quot;k8s-m3&quot; tainted
</code></pre>
<p>截至這邊已完成<code>master</code>節點部署，接下來將針對<code>node</code>的部署進行說明。</p>
<h2 id="Kubernetes-Nodes"><a href="#Kubernetes-Nodes" class="headerlink" title="Kubernetes Nodes"></a>Kubernetes Nodes</h2><p>本節將說明如何建立與設定 Kubernetes Node 節點，Node 是主要執行容器實例(Pod)的工作節點。這過程只需要將 PKI、Bootstrap conf 等檔案複製到機器上，再用 kubelet 啟動即可。</p>
<p>在開始部署前，在<code>k8-m1</code>將需要用到的檔案複製到所有<code>node</code>節點上：</p>
<pre><code class="sh">$ for NODE in k8s-g1 k8s-g2; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;
    for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done
</code></pre>
<h3 id="部署與設定-1"><a href="#部署與設定-1" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>確認檔案都複製後，即可設定所有<code>node</code>節點的 kubelet systemd 來啟動 Kubernetes 元件。首先在<code>k8s-m1</code>複製下列檔案到指定路徑：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
$ for NODE in k8s-g1 k8s-g2; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests&quot;
    scp node/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml
    scp node/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service
    scp node/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf
  done
</code></pre>
<p>接著在<code>k8s-m1</code>透過 SSH 啟動所有<code>node</code>節點的 kubelet：</p>
<pre><code class="sh">$ for NODE in k8s-g1 k8s-g2; do
    ssh ${NODE} &quot;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&quot;
  done
</code></pre>
<h3 id="驗證-Node-節點"><a href="#驗證-Node-節點" class="headerlink" title="驗證 Node 節點"></a>驗證 Node 節點</h3><p>完成後，在任意一台<code>master</code>節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證：</p>
<pre><code class="sh">$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-99n76                                              1h        system:node:k8s-m2        Approved,Issued
csr-9n88h                                              1h        system:node:k8s-m1        Approved,Issued
csr-vdtqr                                              1h        system:node:k8s-m3        Approved,Issued
node-csr-5VkCjWvb8tGVtO-d2gXiQrnst-G1xe_iA0AtQuYNEMI   2m        system:bootstrap:872255   Approved,Issued
node-csr-Uwpss9OhJrAgOB18P4OIEH02VHJwpFrSoMOWkkrK-lo   2m        system:bootstrap:872255   Approved,Issued

$ kubectl get nodes
NAME      STATUS     ROLES     AGE       VERSION
k8s-g1    NotReady   &lt;none&gt;    8m        v1.11.0
k8s-g2    NotReady   &lt;none&gt;    8m        v1.11.0
k8s-m1    NotReady   master    20m       v1.11.0
k8s-m2    NotReady   master    20m       v1.11.0
k8s-m3    NotReady   master    20m       v1.11.0
</code></pre>
<blockquote>
<p>在這階段狀態處於<code>NotReady</code>是正常，往下進行就會了解為何。</p>
</blockquote>
<p>到這邊就表示<code>node</code>節點部署已完成了，接下來章節將針對 Kubernetes Addons 安裝進行說明。</p>
<h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成<code>master</code>與<code>node</code>節點的部署，並組合成一個可運作叢集後，就可以開始透過 kubectl 部署 Addons，Kubernetes 官方提供了多種 Addons 來加強 Kubernetes 的各種功能，如叢集 DNS 解析的<code>kube-dns(or CoreDNS)</code>、外部存取服務的<code>kube-proxy</code>與 Web-based 管理介面的<code>dashboard</code>等等。而其中有些 Addons 是被 Kubernetes 認定為必要的，因此本節將說明如何部署這些 Addons。</p>
<p>首先在<code>k8s-m1</code>節點進入<code>k8s-manual-files</code>目錄，並依序執行下述指令來完成部署：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
</code></pre>
<h3 id="Kubernetes-Proxy"><a href="#Kubernetes-Proxy" class="headerlink" title="Kubernetes Proxy"></a>Kubernetes Proxy</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">kube-proxy</a> 是實現 Kubernetes Service 資源功能的關鍵元件，這個元件會透過 DaemonSet 在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的事件，並依據資源預期狀態透過 iptables 或 ipvs 來實現網路轉發，而本次安裝採用 ipvs。</p>
<p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立，並檢查是否部署成功：</p>
<pre><code class="sh">$ export KUBE_APISERVER=https://172.22.132.9:6443
$ sed -i &quot;s/\${KUBE_APISERVER}/${KUBE_APISERVER}/g&quot; addons/kube-proxy/kube-proxy-cm.yml
$ kubectl -f addons/kube-proxy/

$ kubectl -n kube-system get po -l k8s-app=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE
kube-proxy-dd2m7   1/1       Running   0          8m
kube-proxy-fwgx8   1/1       Running   0          8m
kube-proxy-kjn57   1/1       Running   0          8m
kube-proxy-vp47w   1/1       Running   0          8m
kube-proxy-xsncw   1/1       Running   0          8m

# 檢查 log 是否使用 ipvs
$ kubectl -n kube-system logs -f kube-proxy-fwgx8
I0709 08:41:48.220815       1 feature_gate.go:230] feature gates: &amp;{map[SupportIPVSProxyMode:true]}
I0709 08:41:48.231009       1 server_others.go:183] Using ipvs Proxier.
...
</code></pre>
<p>若有安裝 ipvsadm 的話，可以透過以下指令查看 proxy 規則：</p>
<pre><code class="sh">$ ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -&gt; 172.22.132.9:5443            Masq    1      0          0
</code></pre>
<h3 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h3><p>本節將透過 CoreDNS 取代 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 作為叢集服務發現元件，由於 Kubernetes 需要讓 Pod 與 Pod 之間能夠互相溝通，然而要能夠溝通需要知道彼此的 IP 才行，而這種做法通常是透過 Kubernetes API 來取得達到，但是 Pod IP 會因為生命週期變化而改變，因此這種做法無法彈性使用，且還會增加 API Server 負擔，基於此問題 Kubernetes 提供了 DNS 服務來作為查詢，讓 Pod 能夠以 Service 名稱作為域名來查詢 IP 位址，因此使用者就再不需要關切實際 Pod IP，而 DNS 也會根據 Pod 變化更新資源紀錄(Record resources)。</p>
<p><a href="https://github.com/coredns/coredns" target="_blank" rel="noopener">CoreDNS</a> 是由 CNCF 維護的開源 DNS 專案，該專案前身是 SkyDNS，其採用了 Caddy 的一部分來開發伺服器框架，使其能夠建構一套快速靈活的 DNS，而 CoreDNS 每個功能都可以被實作成一個插件的中介軟體，如 Log、Cache、Kubernetes 等功能，甚至能夠將源紀錄儲存至 Redis、Etcd 中。</p>
<p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立，並檢查是否部署成功：</p>
<pre><code class="sh">$ kubectl create -f addons/coredns/

$ kubectl -n kube-system get po -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
coredns-589dd74cb6-5mv5c   0/1       Pending   0          3m
coredns-589dd74cb6-d42ft   0/1       Pending   0          3m
</code></pre>
<p>這邊會發現 Pod 處於<code>Pending</code>狀態，這是由於 Kubernetes 的叢集網路沒有建立，因此所有節點會處於<code>NotReady</code>狀態，而這也導致 Kubernetes Scheduler 無法替 Pod 找到適合節點而處於<code>Pending</code>，為了解決這個問題，下節將說明與建立 Kubernetes 叢集網路。</p>
<blockquote>
<p>若 Pod 是被 DaemonSet 管理，且設定使用<code>hostNetwork</code>的話，則不會處於<code>Pending</code>狀態。</p>
</blockquote>
<h2 id="Kubernetes-叢集網路"><a href="#Kubernetes-叢集網路" class="headerlink" title="Kubernetes 叢集網路"></a>Kubernetes 叢集網路</h2><p>Kubernetes 在預設情況下與 Docker 的網路有所不同。在 Kubernetes 中有四個問題是需要被解決的，分別為：</p>
<ul>
<li><strong>高耦合的容器到容器溝通</strong>：透過 Pods 與 Localhost 的溝通來解決。</li>
<li><strong>Pod 到 Pod 的溝通</strong>：透過實現網路模型來解決。</li>
<li><strong>Pod 到 Service 溝通</strong>：由 Service objects 結合 kube-proxy 解決。</li>
<li><strong>外部到 Service 溝通</strong>：一樣由 Service objects 結合 kube-proxy 解決。</li>
</ul>
<p>而 Kubernetes 對於任何網路的實現都需要滿足以下基本要求(除非是有意調整的網路分段策略)：</p>
<ul>
<li>所有容器能夠在沒有 NAT 的情況下與其他容器溝通。</li>
<li>所有節點能夠在沒有 NAT 情況下與所有容器溝通(反之亦然)。</li>
<li>容器看到的 IP 與其他人看到的 IP 是一樣的。</li>
</ul>
<p>慶幸的是 Kubernetes 已經有非常多種的<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model" target="_blank" rel="noopener">網路模型</a>以<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" rel="noopener">網路插件(Network Plugins)</a>方式被實現，因此可以選用滿足自己需求的網路功能來使用。另外 Kubernetes 中的網路插件有以下兩種形式：</p>
<ul>
<li><strong>CNI plugins</strong>：以 appc/CNI 標準規範所實現的網路，詳細可以閱讀 <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="noopener">CNI Specification</a>。</li>
<li><strong>Kubenet plugin</strong>：使用 CNI plugins 的 bridge 與 host-local 來實現基本的 cbr0。這通常被用在公有雲服務上的 Kubernetes 叢集網路。</li>
</ul>
<blockquote>
<p>如果想了解如何選擇可以閱讀 Chris Love 的 <a href="https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/" target="_blank" rel="noopener">Choosing a CNI Network Provider for Kubernetes</a> 文章。</p>
</blockquote>
<h3 id="網路部署與設定"><a href="#網路部署與設定" class="headerlink" title="網路部署與設定"></a>網路部署與設定</h3><p>從上述了解 Kubernetes 有多種網路能夠選擇，而本教學選擇了 <a href="https://www.projectcalico.org/" target="_blank" rel="noopener">Calico</a> 作為叢集網路的使用。Calico 是一款純 Layer 3 的網路，其好處是它整合了各種雲原生平台(Docker、Mesos 與 OpenStack 等)，且 Calico 不採用 vSwitch，而是在每個 Kubernetes 節點使用 vRouter 功能，並透過 Linux Kernel 既有的 L3 forwarding 功能，而當資料中心複雜度增加時，Calico 也可以利用 BGP route reflector 來達成。</p>
<blockquote>
<p>想了解 Calico 與傳統 overlay networks 的差異，可以閱讀 <a href="https://www.projectcalico.org/learn/" target="_blank" rel="noopener">Difficulties with traditional overlay networks</a> 文章。</p>
</blockquote>
<p>由於 Calico 提供了 Kubernetes resources YAML 檔來快速以容器方式部署網路插件至所有節點上，因此只需要在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
$ sed -i &#39;s/192.168.0.0\/16/10.244.0.0\/16/g&#39; cni/calico/v3.1/calico.yaml
$ kubectl -f cni/calico/v3.1/
</code></pre>
<blockquote>
<ul>
<li>這邊要記得將<code>CALICO_IPV4POOL_CIDR</code>的網路修改 Cluster IP CIDR。</li>
<li>另外當節點超過 50 台，可以使用 Calico 的 <a href="https://github.com/projectcalico/typha" target="_blank" rel="noopener">Typha</a> 模式來減少透過 Kubernetes datastore 造成 API Server 的負擔。</li>
</ul>
</blockquote>
<p>部署後透過 kubectl 檢查是否有啟動：</p>
<pre><code class="sh">$ kubectl -n kube-system get po -l k8s-app=calico-node
NAME                READY     STATUS    RESTARTS   AGE
calico-node-27jwl   2/2       Running   0          59s
calico-node-4fgv6   2/2       Running   0          59s
calico-node-mvrt7   2/2       Running   0          59s
calico-node-p2q9g   2/2       Running   0          59s
calico-node-zchsz   2/2       Running   0          59s
</code></pre>
<p>確認 calico-node 都正常運作後，透過 kubectl exec 進入 calicoctl pod 來檢查功能是否正常：</p>
<pre><code class="sh">$ kubectl exec -ti -n kube-system calicoctl -- calicoctl get profiles -o wide
NAME              LABELS
kns.default       map[]
kns.kube-public   map[]
kns.kube-system   map[]

$ kubectl exec -ti -n kube-system calicoctl -- calicoctl get node -o wide
NAME     ASN         IPV4               IPV6
k8s-g1   (unknown)   172.22.132.13/24
k8s-g2   (unknown)   172.22.132.14/24
k8s-m1   (unknown)   172.22.132.10/24
k8s-m2   (unknown)   172.22.132.11/24
k8s-m3   (unknown)   172.22.132.12/24
</code></pre>
<blockquote>
<p>若沒問題，就可以將 kube-system 下的 calicoctl pod 刪除。</p>
</blockquote>
<p>完成後，透過檢查節點是否不再是<code>NotReady</code>，以及 Pod 是否不再處於<code>Pending</code>：</p>
<pre><code class="sh">$ kubectl get no
NAME      STATUS    ROLES     AGE       VERSION
k8s-g1    Ready     &lt;none&gt;    35m       v1.11.0
k8s-g2    Ready     &lt;none&gt;    35m       v1.11.0
k8s-m1    Ready     master    35m       v1.11.0
k8s-m2    Ready     master    35m       v1.11.0
k8s-m3    Ready     master    35m       v1.11.0

$ kubectl -n kube-system get po -l k8s-app=kube-dns -o wide
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
coredns-589dd74cb6-5mv5c   1/1       Running   0          10m       10.244.4.2   k8s-g2
coredns-589dd74cb6-d42ft   1/1       Running   0          10m       10.244.3.2   k8s-g1
</code></pre>
<p>當成功到這邊時，一個能運作的 Kubernetes 叢集基本上就完成了，接下來將介紹一些好用的 Addons 來幫助使用與管理 Kubernetes。</p>
<h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的額外 Addons，如 Dashboard、Metrics Server 與 Ingress Controller 等等。</p>
<p>所有 Addons 部署檔案均存已放至<code>k8s-manual-files</code>中，因此在<code>k8s-m1</code>進入該目錄，並依序下小節建立：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
</code></pre>
<h3 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h3><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">Ingress</a> 是 Kubernetes 中的一個抽象資源，其功能是透過 Web Server 的 Virtual Host 概念以域名(Domain Name)方式轉發到內部 Service，這避免了使用 Service 中的 NodePort 與 LoadBalancer 類型所帶來的限制(如 Port 數量上限)，而實現 Ingress 功能則是透過 <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers" target="_blank" rel="noopener">Ingress Controller</a> 來達成，它會負責監聽 Kubernetes API 中的 Ingress 與 Service 資源物件，並在發生資源變化時，依據資源預期的結果來設定 Web Server。另外 Ingress Controller 有許多實現可以選擇：</p>
<ul>
<li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">Ingress NGINX</a>: Kubernetes 官方維護的專案，也是本次安裝使用的 Controller。</li>
<li><a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.5/" target="_blank" rel="noopener">F5 BIG-IP Controller</a>: F5 所開發的 Controller，它能夠讓管理員透過 CLI 或 API 從 Kubernetes 與 OpenShift 管理 F5 BIG-IP 設備。</li>
<li><a href="https://konghq.com/blog/kubernetes-ingress-controller-for-kong/" target="_blank" rel="noopener">Ingress Kong</a>: 著名的開源 API Gateway 專案所維護的 Kubernetes Ingress Controller。</li>
<li><a href="https://github.com/containous/traefik" target="_blank" rel="noopener">Træfik</a>: 是一套開源的 HTTP 反向代理與負載平衡器，而它也支援了 Ingress。</li>
<li><a href="https://github.com/appscode/voyager" target="_blank" rel="noopener">Voyager</a>: 一套以 HAProxy 為底的 Ingress Controller。</li>
</ul>
<blockquote>
<p>而 Ingress Controller 的實現不只這些專案，還有很多可以在網路上找到，未來自己也會寫一篇 Ingress Controller 的實作方式文章。</p>
</blockquote>
<p>首先在<code>k8s-m1</code>執行下述指令來建立 Ingress Controller，並檢查是否部署正常：</p>
<pre><code class="sh">$ export INGRESS_VIP=172.22.132.8
$ sed -i &quot;s/\${INGRESS_VIP}/${INGRESS_VIP}/g&quot; addons/ingress-controller/ingress-controller-svc.yml
$ kubectl create ns ingress-nginx
$ kubectl apply -f addons/ingress-controller
$ kubectl -n ingress-nginx get po,svc
NAME                                           READY     STATUS    RESTARTS   AGE
pod/default-http-backend-846b65fb5f-l5hrc      1/1       Running   0          2m
pod/nginx-ingress-controller-5db8d65fb-z2lf9   1/1       Running   0          2m

NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
service/default-http-backend   ClusterIP      10.99.105.112   &lt;none&gt;         80/TCP         2m
service/ingress-nginx          LoadBalancer   10.106.18.106   172.22.132.8   80:31197/TCP   2m
</code></pre>
<p>完成後透過瀏覽器存取 <a href="http://172.22.132.8:80" target="_blank" rel="noopener">http://172.22.132.8:80</a> 來查看是否能連線，若可以會如下圖結果。</p>
<p><img src="https://i.imgur.com/CfbLwOP.png" alt></p>
<p>當確認上面步驟都沒問題後，就可以透過 kubeclt 建立簡單 NGINX 來測試功能：</p>
<pre><code class="sh">$ kubectl apply -f apps/nginx/
deployment.extensions/nginx created
ingress.extensions/nginx-ingress created
service/nginx created

$ kubectl get po,svc,ing
NAME                        READY     STATUS    RESTARTS   AGE
pod/nginx-966857787-78kth   1/1       Running   0          32s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   2d
service/nginx        ClusterIP   10.104.180.119   &lt;none&gt;        80/TCP    32s

NAME                               HOSTS             ADDRESS        PORTS     AGE
ingress.extensions/nginx-ingress   nginx.k8s.local   172.22.132.8   80        33s
</code></pre>
<blockquote>
<p>P.S. Ingress 規則也支援不同 Path 的服務轉發，可以參考上面提供的官方文件來設定。</p>
</blockquote>
<p>完成後透過 cURL 工具來測試功能是否正常：</p>
<pre><code class="sh">$ curl 172.22.132.8 -H &#39;Host: nginx.k8s.local&#39;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...

# 測試其他 domain name 是否會回傳 404
$ curl 172.22.132.8 -H &#39;Host: nginx1.k8s.local&#39;
default backend - 404
</code></pre>
<p>雖然 Ingress 能夠讓我們透過域名方式存取 Kubernetes 內部服務，但是若域名於法被測試機器解析的話，將會顯示<code>default backend - 404</code>結果，而這經常發生在內部自建環境上，雖然可以透過修改主機<code>/etc/hosts</code>來描述，但並不彈性，因此下節將說明如何建立一個 External DNS 與 DNS 伺服器來提供自動解析 Ingress 域名。</p>
<h3 id="External-DNS"><a href="#External-DNS" class="headerlink" title="External DNS"></a>External DNS</h3><p><a href="https://github.com/kubernetes-incubator/external-dns" target="_blank" rel="noopener">External DNS</a> 是 Kubernetes 社區的孵化專案，被用於定期同步 Kubernetes Service 與 Ingress 資源，並依據資源內容來自動設定公有雲 DNS 服務的資源紀錄(Record resources)。而由於部署不是公有雲環境，因此需要透過 CoreDNS 提供一個內部 DNS 伺服器，再由 ExternalDNS 與這個 CoreDNS 做串接。</p>
<p>首先在<code>k8s-m1</code>執行下述指令來建立 CoreDNS Server，並檢查是否部署正常：</p>
<pre><code class="sh">$ export DNS_VIP=172.22.132.8
$ sed -i &quot;s/\${DNS_VIP}/${DNS_VIP}/g&quot; addons/external-dns/coredns/coredns-svc-tcp.yml
$ sed -i &quot;s/\${DNS_VIP}/${DNS_VIP}/g&quot; addons/external-dns/coredns/coredns-svc-udp.yml
$ kubectl create -f addons/external-dns/coredns/
$ kubectl -n external-dns get po,svc
NAME                                READY     STATUS    RESTARTS   AGE
pod/coredns-54bcfcbd5b-5grb5        1/1       Running   0          2m
pod/coredns-etcd-6c9c68fd76-n8rhj   1/1       Running   0          2m

NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                       AGE
service/coredns-etcd   ClusterIP      10.110.186.83    &lt;none&gt;         2379/TCP,2380/TCP             2m
service/coredns-tcp    LoadBalancer   10.109.105.166   172.22.132.8   53:32169/TCP,9153:32150/TCP   2m
service/coredns-udp    LoadBalancer   10.110.242.185   172.22.132.8   53:31210/UDP
</code></pre>
<blockquote>
<p>這邊域名為<code>k8s.local</code>，可以修改檔案中的<code>coredns-cm.yml</code>來改變。</p>
</blockquote>
<p>完成後，透過 dig 工具來檢查是否 DNS 是否正常：</p>
<pre><code>$ dig @172.22.132.8 SOA nginx.k8s.local +noall +answer +time=2 +tries=1
...
; (1 server found)
;; global options: +cmd
k8s.local.        300    IN    SOA    ns.dns.k8s.local. hostmaster.k8s.local. 1531299150 7200 1800 86400 30
</code></pre><p>接著部署 ExternalDNS 來與 CoreDNS 同步資源紀錄：</p>
<pre><code class="sh">$ kubectl apply -f addons/external-dns/external-dns/
$ kubectl -n external-dns get po -l k8s-app=external-dns
NAME                            READY     STATUS    RESTARTS   AGE
external-dns-86f67f6df8-ljnhj   1/1       Running   0          1m
</code></pre>
<p>完成後，透過 dig 與 nslookup 工具檢查上節測試 Ingress 的 NGINX 服務：</p>
<pre><code>$ dig @172.22.132.8 A nginx.k8s.local +noall +answer +time=2 +tries=1
...
; (1 server found)
;; global options: +cmd
nginx.k8s.local.    300    IN    A    172.22.132.8

$ nslookup nginx.k8s.local
Server:        172.22.132.8
Address:    172.22.132.8#53

** server can&#39;t find nginx.k8s.local: NXDOMAIN
</code></pre><p>這時會無法透過 nslookup 解析域名，這是因為測試機器並沒有使用這個 DNS 伺服器，可以透過修改<code>/etc/resolv.conf</code>來加入，或者類似下圖方式(不同 OS 有差異，不過都在網路設定中改)。</p>
<p><img src="https://i.imgur.com/MVDhXKi.png" alt></p>
<p>再次透過 nslookup 檢查，會發現可以解析了，這時也就能透過 cURL 來測試結果：</p>
<pre><code class="sh">$ nslookup nginx.k8s.local
Server:        172.22.132.8
Address:    172.22.132.8#53

Name:    nginx.k8s.local
Address: 172.22.132.8

$ curl nginx.k8s.local
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre>
<h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 官方開發的 Web-based 儀表板，目的是提升管理 Kubernetes 叢集資源便利性，並以資源視覺化方式，來讓人更直覺的看到整個叢集資源狀態，</p>
<p>在<code>k8s-m1</code>透過 kubeclt 執行下面指令來建立 Dashboard 至 Kubernetes，並檢查是否正確部署：</p>
<pre><code class="sh">$ cd ~/k8s-manual-files
$ kubectl apply -f addons/dashboard/
$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard
NAME                                       READY     STATUS    RESTARTS   AGE
pod/kubernetes-dashboard-6948bdb78-w26qc   1/1       Running   0          2m

NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/kubernetes-dashboard   ClusterIP   10.109.31.80   &lt;none&gt;        443/TCP   2m
</code></pre>
<p>在這邊會額外建立名稱為<code>anonymous-dashboard-proxy</code>的 Cluster Role(Binding) 來讓<code>system:anonymous</code>這個匿名使用者能夠透過 API Server 來 proxy 到 Kubernetes Dashboard，而這個 RBAC 規則僅能夠存取<code>services/proxy</code>資源，以及<code>https:kubernetes-dashboard:</code>資源名稱。</p>
<p>因此我們能夠在完成後，透過以下連結來進入 Kubernetes Dashboard：</p>
<ul>
<li><a href="https://YOUR_VIP:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">https://{YOUR_VIP}:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></li>
</ul>
<p>由於 Kubernetes Dashboard v1.7 版本以後不再提供 Admin 權限，因此需要透過 kubeconfig 或者 Service Account 來進行登入才能取得資源來呈現，這邊建立一個 Service Account 來綁定<code>cluster-admin</code> 以測試功能：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa dashboard
$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard
$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;)
$ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39;
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw
</code></pre>
<blockquote>
<p>複製<code>token</code>然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。</p>
</blockquote>
<p><img src="/images/kube/kubernetes-dashboard.png" alt></p>
<h3 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h3><p>由於 <a href="https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md" target="_blank" rel="noopener">Heapster</a> 將要被移棄，因此這邊選用 <a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a> 作為第三方的叢集監控方案。而本次安裝採用 CoreOS 開發的 <a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">Prometheus Operator</a> 用於管理在 Kubernetes 上的 Prometheus 叢集與資源，更多關於 Prometheus Operator 的資訊可以參考小弟的 <a href="https://kairen.github.io/2018/06/23/devops/prometheus-operator/" target="_blank" rel="noopener">Prometheus Operator 介紹與安裝</a> 文章。</p>
<p>首先在<code>k8s-m1</code>執行下述指令來部署所有 Prometheus 需要的元件：</p>
<pre><code class="sh">$ kubectl apply -f addons/prometheus/
$ kubectl apply -f addons/prometheus/operator/

# 這邊要等 operator 起來並建立好 CRDs 才能進行
$ kubectl apply -f addons/prometheus/alertmanater/
$ kubectl apply -f addons/prometheus/node-exporter/
$ kubectl apply -f addons/prometheus/kube-state-metrics/
$ kubectl apply -f addons/prometheus/grafana/
$ kubectl apply -f addons/prometheus/kube-service-discovery/
$ kubectl apply -f addons/prometheus/prometheus/
$ kubectl apply -f addons/prometheus/servicemonitor/
</code></pre>
<p>完成後，透過 kubectl 檢查服務是否正常運行：</p>
<pre><code class="sh">$ kubectl -n monitoring get po,svc,ing
NAME                                      READY     STATUS    RESTARTS   AGE
pod/alertmanager-main-0                   1/2       Running   0          1m
pod/grafana-6d495c46d5-jpf6r              1/1       Running   0          43s
pod/kube-state-metrics-b84cfb86-4b8qg     4/4       Running   0          37s
pod/node-exporter-2f4lh                   2/2       Running   0          59s
pod/node-exporter-7cz5s                   2/2       Running   0          59s
pod/node-exporter-djdtk                   2/2       Running   0          59s
pod/node-exporter-kfpzt                   2/2       Running   0          59s
pod/node-exporter-qp2jf                   2/2       Running   0          59s
pod/prometheus-k8s-0                      3/3       Running   0          28s
pod/prometheus-k8s-1                      3/3       Running   0          15s
pod/prometheus-operator-9ffd6bdd9-rvqsz   1/1       Running   0          1m

NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/alertmanager-main       ClusterIP   10.110.188.2     &lt;none&gt;        9093/TCP            1m
service/alertmanager-operated   ClusterIP   None             &lt;none&gt;        9093/TCP,6783/TCP   1m
service/grafana                 ClusterIP   10.104.147.154   &lt;none&gt;        3000/TCP            43s
service/kube-state-metrics      ClusterIP   None             &lt;none&gt;        8443/TCP,9443/TCP   51s
service/node-exporter           ClusterIP   None             &lt;none&gt;        9100/TCP            1m
service/prometheus-k8s          ClusterIP   10.96.78.58      &lt;none&gt;        9090/TCP            28s
service/prometheus-operated     ClusterIP   None             &lt;none&gt;        9090/TCP            33s
service/prometheus-operator     ClusterIP   10.99.251.16     &lt;none&gt;        8080/TCP            1m

NAME                                HOSTS                             ADDRESS        PORTS     AGE
ingress.extensions/grafana-ing      grafana.monitoring.k8s.local      172.22.132.8   80        45s
ingress.extensions/prometheus-ing   prometheus.monitoring.k8s.local   172.22.132.8   80        34s
</code></pre>
<p>確認沒問題後，透過瀏覽器查看 <a href="http://prometheus.monitoring.k8s.local" target="_blank" rel="noopener">prometheus.monitoring.k8s.local</a> 與 <a href="http://grafana.monitoring.k8s.local" target="_blank" rel="noopener">grafana.monitoring.k8s.local</a> 是否正常，若沒問題就可以看到如下圖所示結果。</p>
<p><img src="https://i.imgur.com/XFTZ4eF.png" alt></p>
<p><img src="https://i.imgur.com/YB5KAPe.png" alt></p>
<blockquote>
<p>另外這邊也推薦用 <a href="https://github.com/weaveworks/scope" target="_blank" rel="noopener">Weave Scope</a> 來監控容器的網路 Flow 拓樸圖。</p>
</blockquote>
<h3 id="Metrics-Server"><a href="#Metrics-Server" class="headerlink" title="Metrics Server"></a>Metrics Server</h3><p><a href="https://github.com/kubernetes-incubator/metrics-server" target="_blank" rel="noopener">Metrics Server</a> 是實現了 Metrics API 的元件，其目標是取代 Heapster 作為 Pod 與 Node 提供資源的 Usage metrics，該元件會從每個 Kubernetes 節點上的 Kubelet 所公開的 Summary API 中收集 Metrics。</p>
<p>首先在<code>k8s-m1</code>測試一下 kubectl top 指令：</p>
<pre><code class="sh">$ kubectl top node
error: metrics not available yet
</code></pre>
<p>發現 top 指令無法取得 Metrics，這表示 Kubernetes 叢集沒有安裝 Heapster 或是 Metrics Server 來提供 Metrics API 給 top 指令取得資源使用量。</p>
<p>由於上述問題，我們要在<code>k8s-m1</code>節點透過 kubectl 部署 Metrics Server 元件來解決：</p>
<pre><code class="sh">$ kubectl create -f addons/metric-server/
$ kubectl -n kube-system get po -l k8s-app=metrics-server
NAME                                  READY     STATUS    RESTARTS   AGE
pod/metrics-server-86bd9d7667-5hbn6   1/1       Running   0          1m
</code></pre>
<p>完成後，等待一點時間(約 30s - 1m)收集 Metrics，再次執行 kubectl top 指令查看：</p>
<pre><code class="sh">$ kubectl top node
NAME      CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
k8s-g1    106m         2%        1037Mi          6%
k8s-g2    212m         5%        1043Mi          8%
k8s-m1    386m         9%        2125Mi          13%
k8s-m2    320m         8%        1834Mi          11%
k8s-m3    457m         11%       1818Mi          11%
</code></pre>
<p>而這時若有使用 <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener">HPA</a> 的話，就能夠正確抓到 Pod 的 CPU 與 Memory 使用量了。</p>
<blockquote>
<p>若想讓 HPA 使用 Prometheus 的 Metrics 的話，可以閱讀 <a href="https://github.com/stefanprodan/k8s-prom-hpa#setting-up-a-custom-metrics-server" target="_blank" rel="noopener">Custom Metrics Server</a> 來了解。</p>
</blockquote>
<h3 id="Helm-Tiller-Server"><a href="#Helm-Tiller-Server" class="headerlink" title="Helm Tiller Server"></a>Helm Tiller Server</h3><p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源。其中<code>Tiller Server</code>主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</p>
<p>首先在<code>k8s-m1</code>安裝 Helm tool：</p>
<pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz | tar -zx
$ sudo mv linux-amd64/helm /usr/local/bin/
</code></pre>
<p>另外在所有<code>node</code>節點安裝 socat：</p>
<pre><code class="sh">$ sudo apt-get install -y socat
</code></pre>
<p>接著初始化 Helm(這邊會安裝 Tiller Server)：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
...
Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!

$ kubectl -n kube-system get po -l app=helm
NAME                            READY     STATUS    RESTARTS   AGE
tiller-deploy-759cb9df9-rfhqw   1/1       Running   0          19s

$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h4 id="測試-Helm-功能"><a href="#測試-Helm-功能" class="headerlink" title="測試 Helm 功能"></a>測試 Helm 功能</h4><p>這邊部署簡單 Jenkins 來進行功能測試：</p>
<pre><code class="sh">$ helm install --name demo --set Persistence.Enabled=false stable/jenkins
$ kubectl get po,svc  -l app=demo-jenkins
NAME                           READY     STATUS    RESTARTS   AGE
demo-jenkins-7bf4bfcff-q74nt   1/1       Running   0          2m

NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
demo-jenkins         LoadBalancer   10.103.15.129    &lt;pending&gt;     8080:31161/TCP   2m
demo-jenkins-agent   ClusterIP      10.103.160.126   &lt;none&gt;        50000/TCP        2m

# 取得 admin 帳號的密碼
$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo
r6y9FMuF2u
</code></pre>
<p>當服務都正常運作時，就可以透過瀏覽器查看 <a href="http://node_ip:31161" target="_blank" rel="noopener">http://node_ip:31161</a> 頁面。</p>
<p><img src="/images/kube/helm-jenkins-v1.10.png" alt></p>
<p>測試完成後，就可以透過以下指令來刪除 Release：</p>
<pre><code class="sh">$ helm ls
NAME    REVISION    UPDATED                     STATUS      CHART             NAMESPACE
demo    1           Tue Apr 10 07:29:51 2018    DEPLOYED    jenkins-0.14.4    default

$ helm delete demo --purge
release &quot;demo&quot; deleted
</code></pre>
<p>想要了解更多 Helm Apps 的話，可以到 <a href="https://hub.kubeapps.com/" target="_blank" rel="noopener">Kubeapps Hub</a> 網站尋找。</p>
<h2 id="測試叢集-HA-功能"><a href="#測試叢集-HA-功能" class="headerlink" title="測試叢集 HA 功能"></a>測試叢集 HA 功能</h2><p>首先進入<code>k8s-m1</code>節點，然後關閉該節點：</p>
<pre><code class="sh">$ sudo poweroff
</code></pre>
<p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p>
<pre><code># 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷
$ kubectl get cs
NAME                 STATUS      MESSAGE                                                                                                                                          ERROR
scheduler            Healthy     ok
controller-manager   Healthy     ok
etcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}
etcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}
etcd-0               Unhealthy   Get https://172.22.132.10:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

# 測試是否可以建立 Pod
$ kubectl run nginx --image nginx --restart=Never --port 80
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          22s
</code></pre>
                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2018/07/17/kubernetes/deploy/kubeadm-v1.11-ha/" data-toggle="tooltip" data-placement="top" title="利用 kubeadm 部署 Kubernetes v1.11.x HA 叢集">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2018/07/01/devops/prometheus-ha/" data-toggle="tooltip" data-placement="top" title="Prometheus 高可靠實現方式">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                    <div class="comment">
                        <div id="disqus_thread" class="disqus-thread"></div>
                    </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-部署資訊"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Kubernetes 部署資訊</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#節點資訊"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">節點資訊</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#事前準備"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">事前準備</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#建立-CA-與產生-TLS-憑證"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">建立 CA 與產生 TLS 憑證</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Etcd"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Etcd</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Kubernetes-元件"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Kubernetes 元件</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#API-Server"><span class="toc-nav-number">4.2.1.</span> <span class="toc-nav-text">API Server</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Front-Proxy-Client"><span class="toc-nav-number">4.2.2.</span> <span class="toc-nav-text">Front Proxy Client</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Controller-Manager"><span class="toc-nav-number">4.2.3.</span> <span class="toc-nav-text">Controller Manager</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Scheduler"><span class="toc-nav-number">4.2.4.</span> <span class="toc-nav-text">Scheduler</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Admin"><span class="toc-nav-number">4.2.5.</span> <span class="toc-nav-text">Admin</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Masters-Kubelet"><span class="toc-nav-number">4.2.6.</span> <span class="toc-nav-text">Masters Kubelet</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Service-Account-Key"><span class="toc-nav-number">4.2.7.</span> <span class="toc-nav-text">Service Account Key</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#刪除不必要檔案"><span class="toc-nav-number">4.2.8.</span> <span class="toc-nav-text">刪除不必要檔案</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#複製檔案至其他節點"><span class="toc-nav-number">4.2.9.</span> <span class="toc-nav-text">複製檔案至其他節點</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-Masters"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Kubernetes Masters</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#部署與設定"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">部署與設定</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#建立-TLS-Bootstrapping"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">建立 TLS Bootstrapping</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#驗證-Master-節點"><span class="toc-nav-number">5.3.</span> <span class="toc-nav-text">驗證 Master 節點</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-Nodes"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Kubernetes Nodes</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#部署與設定-1"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">部署與設定</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#驗證-Node-節點"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">驗證 Node 節點</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-Core-Addons-部署"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">Kubernetes Core Addons 部署</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Kubernetes-Proxy"><span class="toc-nav-number">7.1.</span> <span class="toc-nav-text">Kubernetes Proxy</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#CoreDNS"><span class="toc-nav-number">7.2.</span> <span class="toc-nav-text">CoreDNS</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-叢集網路"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">Kubernetes 叢集網路</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#網路部署與設定"><span class="toc-nav-number">8.1.</span> <span class="toc-nav-text">網路部署與設定</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Kubernetes-Extra-Addons-部署"><span class="toc-nav-number">9.</span> <span class="toc-nav-text">Kubernetes Extra Addons 部署</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Ingress-Controller"><span class="toc-nav-number">9.1.</span> <span class="toc-nav-text">Ingress Controller</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#External-DNS"><span class="toc-nav-number">9.2.</span> <span class="toc-nav-text">External DNS</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Dashboard"><span class="toc-nav-number">9.3.</span> <span class="toc-nav-text">Dashboard</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Prometheus"><span class="toc-nav-number">9.4.</span> <span class="toc-nav-text">Prometheus</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Metrics-Server"><span class="toc-nav-number">9.5.</span> <span class="toc-nav-text">Metrics Server</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Helm-Tiller-Server"><span class="toc-nav-number">9.6.</span> <span class="toc-nav-text">Helm Tiller Server</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#測試-Helm-功能"><span class="toc-nav-number">9.6.1.</span> <span class="toc-nav-text">測試 Helm 功能</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#測試叢集-HA-功能"><span class="toc-nav-number">10.</span> <span class="toc-nav-text">測試叢集 HA 功能</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Docker" title="Docker">Docker</a>
                        
                          <a class="tag" href="/tags/#Kubernetes" title="Kubernetes">Kubernetes</a>
                        
                          <a class="tag" href="/tags/#Calico" title="Calico">Calico</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://hwchiu.com" target="_blank">威猛邱牛的部落格</a></li>
                    
                        <li><a href="http://www.evanlin.com" target="_blank">吃草爸爸的部落格</a></li>
                    
                        <li><a href="https://ellis-wu.github.io" target="_blank">跟我一樣可悲的同事部落格</a></li>
                    
                        <li><a href="https://blog.pichuang.com.tw" target="_blank">小飛機的部落格</a></li>
                    
                        <li><a href="https://bestsamina.github.io/" target="_blank">超猛姍蓉的部落格</a></li>
                    
                        <li><a href="https://igene.tw" target="_blank">郭大俠的部落格</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "shenhengheng";
    var disqus_identifier = "https://readailib.com/2018/07/09/kubernetes/deploy/manual-v1.11/";
    var disqus_url = "https://readailib.com/2018/07/09/kubernetes/deploy/manual-v1.11/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                    <li>
                        <a href="/atom.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank" href="https://twitter.com/shenhengheng">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                

                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/shenhengheng">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://github.com/rh01">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/heng960509">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; ShenHengheng 2019
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a>
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span>
                    re-Ported by <a href="http://beantech.org">BeanTech</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=YenYuHsuan&repo=hexo-theme-beantech&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://readailib.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->



<!-- Highlight.js -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://readailib.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
